{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 导入包\n",
    "# 用于加载波士顿房价数据集\n",
    "from sklearn.datasets import load_boston \n",
    "# pandas工具包 对于pandas陌生的同学可以参考官方10分钟上手教程：https://pandas.pydata.org/pandas-docs/stable/10min.html\n",
    "# panel data set\n",
    "import pandas as pd \n",
    "# seaborn 用于画图\n",
    "import seaborn as sns\n",
    "import numpy as np # numpy\n",
    "# 显示画图\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## step1:load data & data analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = load_boston() # 加载数据集"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['DESCR', 'data', 'feature_names', 'filename', 'target']"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dir(dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['CRIM', 'ZN', 'INDUS', 'CHAS', 'NOX', 'RM', 'AGE', 'DIS', 'RAD',\n",
       "       'TAX', 'PTRATIO', 'B', 'LSTAT'], dtype='<U7')"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset['feature_names'] # 特征名称"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ".. _boston_dataset:\n",
      "\n",
      "Boston house prices dataset\n",
      "---------------------------\n",
      "\n",
      "**Data Set Characteristics:**  \n",
      "\n",
      "    :Number of Instances: 506 \n",
      "\n",
      "    :Number of Attributes: 13 numeric/categorical predictive. Median Value (attribute 14) is usually the target.\n",
      "\n",
      "    :Attribute Information (in order):\n",
      "        - CRIM     per capita crime rate by town\n",
      "        - ZN       proportion of residential land zoned for lots over 25,000 sq.ft.\n",
      "        - INDUS    proportion of non-retail business acres per town\n",
      "        - CHAS     Charles River dummy variable (= 1 if tract bounds river; 0 otherwise)\n",
      "        - NOX      nitric oxides concentration (parts per 10 million)\n",
      "        - RM       average number of rooms per dwelling\n",
      "        - AGE      proportion of owner-occupied units built prior to 1940\n",
      "        - DIS      weighted distances to five Boston employment centres\n",
      "        - RAD      index of accessibility to radial highways\n",
      "        - TAX      full-value property-tax rate per $10,000\n",
      "        - PTRATIO  pupil-teacher ratio by town\n",
      "        - B        1000(Bk - 0.63)^2 where Bk is the proportion of blacks by town\n",
      "        - LSTAT    % lower status of the population\n",
      "        - MEDV     Median value of owner-occupied homes in $1000's\n",
      "\n",
      "    :Missing Attribute Values: None\n",
      "\n",
      "    :Creator: Harrison, D. and Rubinfeld, D.L.\n",
      "\n",
      "This is a copy of UCI ML housing dataset.\n",
      "https://archive.ics.uci.edu/ml/machine-learning-databases/housing/\n",
      "\n",
      "\n",
      "This dataset was taken from the StatLib library which is maintained at Carnegie Mellon University.\n",
      "\n",
      "The Boston house-price data of Harrison, D. and Rubinfeld, D.L. 'Hedonic\n",
      "prices and the demand for clean air', J. Environ. Economics & Management,\n",
      "vol.5, 81-102, 1978.   Used in Belsley, Kuh & Welsch, 'Regression diagnostics\n",
      "...', Wiley, 1980.   N.B. Various transformations are used in the table on\n",
      "pages 244-261 of the latter.\n",
      "\n",
      "The Boston house-price data has been used in many machine learning papers that address regression\n",
      "problems.   \n",
      "     \n",
      ".. topic:: References\n",
      "\n",
      "   - Belsley, Kuh & Welsch, 'Regression diagnostics: Identifying Influential Data and Sources of Collinearity', Wiley, 1980. 244-261.\n",
      "   - Quinlan,R. (1993). Combining Instance-Based and Model-Based Learning. In Proceedings on the Tenth International Conference of Machine Learning, 236-243, University of Massachusetts, Amherst. Morgan Kaufmann.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(dataset['DESCR'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([6.32000e-03, 2.73100e-02, 2.72900e-02, 3.23700e-02, 6.90500e-02,\n",
       "       2.98500e-02, 8.82900e-02, 1.44550e-01, 2.11240e-01, 1.70040e-01,\n",
       "       2.24890e-01, 1.17470e-01, 9.37800e-02, 6.29760e-01, 6.37960e-01,\n",
       "       6.27390e-01, 1.05393e+00, 7.84200e-01, 8.02710e-01, 7.25800e-01,\n",
       "       1.25179e+00, 8.52040e-01, 1.23247e+00, 9.88430e-01, 7.50260e-01,\n",
       "       8.40540e-01, 6.71910e-01, 9.55770e-01, 7.72990e-01, 1.00245e+00,\n",
       "       1.13081e+00, 1.35472e+00, 1.38799e+00, 1.15172e+00, 1.61282e+00,\n",
       "       6.41700e-02, 9.74400e-02, 8.01400e-02, 1.75050e-01, 2.76300e-02,\n",
       "       3.35900e-02, 1.27440e-01, 1.41500e-01, 1.59360e-01, 1.22690e-01,\n",
       "       1.71420e-01, 1.88360e-01, 2.29270e-01, 2.53870e-01, 2.19770e-01,\n",
       "       8.87300e-02, 4.33700e-02, 5.36000e-02, 4.98100e-02, 1.36000e-02,\n",
       "       1.31100e-02, 2.05500e-02, 1.43200e-02, 1.54450e-01, 1.03280e-01,\n",
       "       1.49320e-01, 1.71710e-01, 1.10270e-01, 1.26500e-01, 1.95100e-02,\n",
       "       3.58400e-02, 4.37900e-02, 5.78900e-02, 1.35540e-01, 1.28160e-01,\n",
       "       8.82600e-02, 1.58760e-01, 9.16400e-02, 1.95390e-01, 7.89600e-02,\n",
       "       9.51200e-02, 1.01530e-01, 8.70700e-02, 5.64600e-02, 8.38700e-02,\n",
       "       4.11300e-02, 4.46200e-02, 3.65900e-02, 3.55100e-02, 5.05900e-02,\n",
       "       5.73500e-02, 5.18800e-02, 7.15100e-02, 5.66000e-02, 5.30200e-02,\n",
       "       4.68400e-02, 3.93200e-02, 4.20300e-02, 2.87500e-02, 4.29400e-02,\n",
       "       1.22040e-01, 1.15040e-01, 1.20830e-01, 8.18700e-02, 6.86000e-02,\n",
       "       1.48660e-01, 1.14320e-01, 2.28760e-01, 2.11610e-01, 1.39600e-01,\n",
       "       1.32620e-01, 1.71200e-01, 1.31170e-01, 1.28020e-01, 2.63630e-01,\n",
       "       1.07930e-01, 1.00840e-01, 1.23290e-01, 2.22120e-01, 1.42310e-01,\n",
       "       1.71340e-01, 1.31580e-01, 1.50980e-01, 1.30580e-01, 1.44760e-01,\n",
       "       6.89900e-02, 7.16500e-02, 9.29900e-02, 1.50380e-01, 9.84900e-02,\n",
       "       1.69020e-01, 3.87350e-01, 2.59150e-01, 3.25430e-01, 8.81250e-01,\n",
       "       3.40060e-01, 1.19294e+00, 5.90050e-01, 3.29820e-01, 9.76170e-01,\n",
       "       5.57780e-01, 3.22640e-01, 3.52330e-01, 2.49800e-01, 5.44520e-01,\n",
       "       2.90900e-01, 1.62864e+00, 3.32105e+00, 4.09740e+00, 2.77974e+00,\n",
       "       2.37934e+00, 2.15505e+00, 2.36862e+00, 2.33099e+00, 2.73397e+00,\n",
       "       1.65660e+00, 1.49632e+00, 1.12658e+00, 2.14918e+00, 1.41385e+00,\n",
       "       3.53501e+00, 2.44668e+00, 1.22358e+00, 1.34284e+00, 1.42502e+00,\n",
       "       1.27346e+00, 1.46336e+00, 1.83377e+00, 1.51902e+00, 2.24236e+00,\n",
       "       2.92400e+00, 2.01019e+00, 1.80028e+00, 2.30040e+00, 2.44953e+00,\n",
       "       1.20742e+00, 2.31390e+00, 1.39140e-01, 9.17800e-02, 8.44700e-02,\n",
       "       6.66400e-02, 7.02200e-02, 5.42500e-02, 6.64200e-02, 5.78000e-02,\n",
       "       6.58800e-02, 6.88800e-02, 9.10300e-02, 1.00080e-01, 8.30800e-02,\n",
       "       6.04700e-02, 5.60200e-02, 7.87500e-02, 1.25790e-01, 8.37000e-02,\n",
       "       9.06800e-02, 6.91100e-02, 8.66400e-02, 2.18700e-02, 1.43900e-02,\n",
       "       1.38100e-02, 4.01100e-02, 4.66600e-02, 3.76800e-02, 3.15000e-02,\n",
       "       1.77800e-02, 3.44500e-02, 2.17700e-02, 3.51000e-02, 2.00900e-02,\n",
       "       1.36420e-01, 2.29690e-01, 2.51990e-01, 1.35870e-01, 4.35710e-01,\n",
       "       1.74460e-01, 3.75780e-01, 2.17190e-01, 1.40520e-01, 2.89550e-01,\n",
       "       1.98020e-01, 4.56000e-02, 7.01300e-02, 1.10690e-01, 1.14250e-01,\n",
       "       3.58090e-01, 4.07710e-01, 6.23560e-01, 6.14700e-01, 3.15330e-01,\n",
       "       5.26930e-01, 3.82140e-01, 4.12380e-01, 2.98190e-01, 4.41780e-01,\n",
       "       5.37000e-01, 4.62960e-01, 5.75290e-01, 3.31470e-01, 4.47910e-01,\n",
       "       3.30450e-01, 5.20580e-01, 5.11830e-01, 8.24400e-02, 9.25200e-02,\n",
       "       1.13290e-01, 1.06120e-01, 1.02900e-01, 1.27570e-01, 2.06080e-01,\n",
       "       1.91330e-01, 3.39830e-01, 1.96570e-01, 1.64390e-01, 1.90730e-01,\n",
       "       1.40300e-01, 2.14090e-01, 8.22100e-02, 3.68940e-01, 4.81900e-02,\n",
       "       3.54800e-02, 1.53800e-02, 6.11540e-01, 6.63510e-01, 6.56650e-01,\n",
       "       5.40110e-01, 5.34120e-01, 5.20140e-01, 8.25260e-01, 5.50070e-01,\n",
       "       7.61620e-01, 7.85700e-01, 5.78340e-01, 5.40500e-01, 9.06500e-02,\n",
       "       2.99160e-01, 1.62110e-01, 1.14600e-01, 2.21880e-01, 5.64400e-02,\n",
       "       9.60400e-02, 1.04690e-01, 6.12700e-02, 7.97800e-02, 2.10380e-01,\n",
       "       3.57800e-02, 3.70500e-02, 6.12900e-02, 1.50100e-02, 9.06000e-03,\n",
       "       1.09600e-02, 1.96500e-02, 3.87100e-02, 4.59000e-02, 4.29700e-02,\n",
       "       3.50200e-02, 7.88600e-02, 3.61500e-02, 8.26500e-02, 8.19900e-02,\n",
       "       1.29320e-01, 5.37200e-02, 1.41030e-01, 6.46600e-02, 5.56100e-02,\n",
       "       4.41700e-02, 3.53700e-02, 9.26600e-02, 1.00000e-01, 5.51500e-02,\n",
       "       5.47900e-02, 7.50300e-02, 4.93200e-02, 4.92980e-01, 3.49400e-01,\n",
       "       2.63548e+00, 7.90410e-01, 2.61690e-01, 2.69380e-01, 3.69200e-01,\n",
       "       2.53560e-01, 3.18270e-01, 2.45220e-01, 4.02020e-01, 4.75470e-01,\n",
       "       1.67600e-01, 1.81590e-01, 3.51140e-01, 2.83920e-01, 3.41090e-01,\n",
       "       1.91860e-01, 3.03470e-01, 2.41030e-01, 6.61700e-02, 6.72400e-02,\n",
       "       4.54400e-02, 5.02300e-02, 3.46600e-02, 5.08300e-02, 3.73800e-02,\n",
       "       3.96100e-02, 3.42700e-02, 3.04100e-02, 3.30600e-02, 5.49700e-02,\n",
       "       6.15100e-02, 1.30100e-02, 2.49800e-02, 2.54300e-02, 3.04900e-02,\n",
       "       3.11300e-02, 6.16200e-02, 1.87000e-02, 1.50100e-02, 2.89900e-02,\n",
       "       6.21100e-02, 7.95000e-02, 7.24400e-02, 1.70900e-02, 4.30100e-02,\n",
       "       1.06590e-01, 8.98296e+00, 3.84970e+00, 5.20177e+00, 4.26131e+00,\n",
       "       4.54192e+00, 3.83684e+00, 3.67822e+00, 4.22239e+00, 3.47428e+00,\n",
       "       4.55587e+00, 3.69695e+00, 1.35222e+01, 4.89822e+00, 5.66998e+00,\n",
       "       6.53876e+00, 9.23230e+00, 8.26725e+00, 1.11081e+01, 1.84982e+01,\n",
       "       1.96091e+01, 1.52880e+01, 9.82349e+00, 2.36482e+01, 1.78667e+01,\n",
       "       8.89762e+01, 1.58744e+01, 9.18702e+00, 7.99248e+00, 2.00849e+01,\n",
       "       1.68118e+01, 2.43938e+01, 2.25971e+01, 1.43337e+01, 8.15174e+00,\n",
       "       6.96215e+00, 5.29305e+00, 1.15779e+01, 8.64476e+00, 1.33598e+01,\n",
       "       8.71675e+00, 5.87205e+00, 7.67202e+00, 3.83518e+01, 9.91655e+00,\n",
       "       2.50461e+01, 1.42362e+01, 9.59571e+00, 2.48017e+01, 4.15292e+01,\n",
       "       6.79208e+01, 2.07162e+01, 1.19511e+01, 7.40389e+00, 1.44383e+01,\n",
       "       5.11358e+01, 1.40507e+01, 1.88110e+01, 2.86558e+01, 4.57461e+01,\n",
       "       1.80846e+01, 1.08342e+01, 2.59406e+01, 7.35341e+01, 1.18123e+01,\n",
       "       1.10874e+01, 7.02259e+00, 1.20482e+01, 7.05042e+00, 8.79212e+00,\n",
       "       1.58603e+01, 1.22472e+01, 3.76619e+01, 7.36711e+00, 9.33889e+00,\n",
       "       8.49213e+00, 1.00623e+01, 6.44405e+00, 5.58107e+00, 1.39134e+01,\n",
       "       1.11604e+01, 1.44208e+01, 1.51772e+01, 1.36781e+01, 9.39063e+00,\n",
       "       2.20511e+01, 9.72418e+00, 5.66637e+00, 9.96654e+00, 1.28023e+01,\n",
       "       1.06718e+01, 6.28807e+00, 9.92485e+00, 9.32909e+00, 7.52601e+00,\n",
       "       6.71772e+00, 5.44114e+00, 5.09017e+00, 8.24809e+00, 9.51363e+00,\n",
       "       4.75237e+00, 4.66883e+00, 8.20058e+00, 7.75223e+00, 6.80117e+00,\n",
       "       4.81213e+00, 3.69311e+00, 6.65492e+00, 5.82115e+00, 7.83932e+00,\n",
       "       3.16360e+00, 3.77498e+00, 4.42228e+00, 1.55757e+01, 1.30751e+01,\n",
       "       4.34879e+00, 4.03841e+00, 3.56868e+00, 4.64689e+00, 8.05579e+00,\n",
       "       6.39312e+00, 4.87141e+00, 1.50234e+01, 1.02330e+01, 1.43337e+01,\n",
       "       5.82401e+00, 5.70818e+00, 5.73116e+00, 2.81838e+00, 2.37857e+00,\n",
       "       3.67367e+00, 5.69175e+00, 4.83567e+00, 1.50860e-01, 1.83370e-01,\n",
       "       2.07460e-01, 1.05740e-01, 1.11320e-01, 1.73310e-01, 2.79570e-01,\n",
       "       1.78990e-01, 2.89600e-01, 2.68380e-01, 2.39120e-01, 1.77830e-01,\n",
       "       2.24380e-01, 6.26300e-02, 4.52700e-02, 6.07600e-02, 1.09590e-01,\n",
       "       4.74100e-02])"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset['data'][:,0] #犯罪率"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define the problem\n",
    "\n",
    "### Assuming you were a real state salesperson in Boston 假设你是一个真正的房地产销售\n",
    "### Given some description data about a real state => It's price 来一个买家或者卖家，你要知道这个房子大概多少钱"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "      <th>10</th>\n",
       "      <th>11</th>\n",
       "      <th>12</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.00632</td>\n",
       "      <td>18.0</td>\n",
       "      <td>2.31</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.538</td>\n",
       "      <td>6.575</td>\n",
       "      <td>65.2</td>\n",
       "      <td>4.0900</td>\n",
       "      <td>1.0</td>\n",
       "      <td>296.0</td>\n",
       "      <td>15.3</td>\n",
       "      <td>396.90</td>\n",
       "      <td>4.98</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.02731</td>\n",
       "      <td>0.0</td>\n",
       "      <td>7.07</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.469</td>\n",
       "      <td>6.421</td>\n",
       "      <td>78.9</td>\n",
       "      <td>4.9671</td>\n",
       "      <td>2.0</td>\n",
       "      <td>242.0</td>\n",
       "      <td>17.8</td>\n",
       "      <td>396.90</td>\n",
       "      <td>9.14</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.02729</td>\n",
       "      <td>0.0</td>\n",
       "      <td>7.07</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.469</td>\n",
       "      <td>7.185</td>\n",
       "      <td>61.1</td>\n",
       "      <td>4.9671</td>\n",
       "      <td>2.0</td>\n",
       "      <td>242.0</td>\n",
       "      <td>17.8</td>\n",
       "      <td>392.83</td>\n",
       "      <td>4.03</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.03237</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2.18</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.458</td>\n",
       "      <td>6.998</td>\n",
       "      <td>45.8</td>\n",
       "      <td>6.0622</td>\n",
       "      <td>3.0</td>\n",
       "      <td>222.0</td>\n",
       "      <td>18.7</td>\n",
       "      <td>394.63</td>\n",
       "      <td>2.94</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.06905</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2.18</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.458</td>\n",
       "      <td>7.147</td>\n",
       "      <td>54.2</td>\n",
       "      <td>6.0622</td>\n",
       "      <td>3.0</td>\n",
       "      <td>222.0</td>\n",
       "      <td>18.7</td>\n",
       "      <td>396.90</td>\n",
       "      <td>5.33</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         0     1     2    3      4      5     6       7    8      9    10  \\\n",
       "0  0.00632  18.0  2.31  0.0  0.538  6.575  65.2  4.0900  1.0  296.0  15.3   \n",
       "1  0.02731   0.0  7.07  0.0  0.469  6.421  78.9  4.9671  2.0  242.0  17.8   \n",
       "2  0.02729   0.0  7.07  0.0  0.469  7.185  61.1  4.9671  2.0  242.0  17.8   \n",
       "3  0.03237   0.0  2.18  0.0  0.458  6.998  45.8  6.0622  3.0  222.0  18.7   \n",
       "4  0.06905   0.0  2.18  0.0  0.458  7.147  54.2  6.0622  3.0  222.0  18.7   \n",
       "\n",
       "       11    12  \n",
       "0  396.90  4.98  \n",
       "1  396.90  9.14  \n",
       "2  392.83  4.03  \n",
       "3  394.63  2.94  \n",
       "4  396.90  5.33  "
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataframe = pd.DataFrame(dataset['data'])\n",
    "dataframe.head(5) # 查看dataframe的前5行，我们可以看到列名是数字"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>CRIM</th>\n",
       "      <th>ZN</th>\n",
       "      <th>INDUS</th>\n",
       "      <th>CHAS</th>\n",
       "      <th>NOX</th>\n",
       "      <th>RM</th>\n",
       "      <th>AGE</th>\n",
       "      <th>DIS</th>\n",
       "      <th>RAD</th>\n",
       "      <th>TAX</th>\n",
       "      <th>PTRATIO</th>\n",
       "      <th>B</th>\n",
       "      <th>LSTAT</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.00632</td>\n",
       "      <td>18.0</td>\n",
       "      <td>2.31</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.538</td>\n",
       "      <td>6.575</td>\n",
       "      <td>65.2</td>\n",
       "      <td>4.0900</td>\n",
       "      <td>1.0</td>\n",
       "      <td>296.0</td>\n",
       "      <td>15.3</td>\n",
       "      <td>396.90</td>\n",
       "      <td>4.98</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.02731</td>\n",
       "      <td>0.0</td>\n",
       "      <td>7.07</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.469</td>\n",
       "      <td>6.421</td>\n",
       "      <td>78.9</td>\n",
       "      <td>4.9671</td>\n",
       "      <td>2.0</td>\n",
       "      <td>242.0</td>\n",
       "      <td>17.8</td>\n",
       "      <td>396.90</td>\n",
       "      <td>9.14</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.02729</td>\n",
       "      <td>0.0</td>\n",
       "      <td>7.07</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.469</td>\n",
       "      <td>7.185</td>\n",
       "      <td>61.1</td>\n",
       "      <td>4.9671</td>\n",
       "      <td>2.0</td>\n",
       "      <td>242.0</td>\n",
       "      <td>17.8</td>\n",
       "      <td>392.83</td>\n",
       "      <td>4.03</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.03237</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2.18</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.458</td>\n",
       "      <td>6.998</td>\n",
       "      <td>45.8</td>\n",
       "      <td>6.0622</td>\n",
       "      <td>3.0</td>\n",
       "      <td>222.0</td>\n",
       "      <td>18.7</td>\n",
       "      <td>394.63</td>\n",
       "      <td>2.94</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.06905</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2.18</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.458</td>\n",
       "      <td>7.147</td>\n",
       "      <td>54.2</td>\n",
       "      <td>6.0622</td>\n",
       "      <td>3.0</td>\n",
       "      <td>222.0</td>\n",
       "      <td>18.7</td>\n",
       "      <td>396.90</td>\n",
       "      <td>5.33</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      CRIM    ZN  INDUS  CHAS    NOX     RM   AGE     DIS  RAD    TAX  \\\n",
       "0  0.00632  18.0   2.31   0.0  0.538  6.575  65.2  4.0900  1.0  296.0   \n",
       "1  0.02731   0.0   7.07   0.0  0.469  6.421  78.9  4.9671  2.0  242.0   \n",
       "2  0.02729   0.0   7.07   0.0  0.469  7.185  61.1  4.9671  2.0  242.0   \n",
       "3  0.03237   0.0   2.18   0.0  0.458  6.998  45.8  6.0622  3.0  222.0   \n",
       "4  0.06905   0.0   2.18   0.0  0.458  7.147  54.2  6.0622  3.0  222.0   \n",
       "\n",
       "   PTRATIO       B  LSTAT  \n",
       "0     15.3  396.90   4.98  \n",
       "1     17.8  396.90   9.14  \n",
       "2     17.8  392.83   4.03  \n",
       "3     18.7  394.63   2.94  \n",
       "4     18.7  396.90   5.33  "
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataframe.columns = dataset['feature_names'] # 将数字列名替换为特征的名字\n",
    "dataframe.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>CRIM</th>\n",
       "      <th>ZN</th>\n",
       "      <th>INDUS</th>\n",
       "      <th>CHAS</th>\n",
       "      <th>NOX</th>\n",
       "      <th>RM</th>\n",
       "      <th>AGE</th>\n",
       "      <th>DIS</th>\n",
       "      <th>RAD</th>\n",
       "      <th>TAX</th>\n",
       "      <th>PTRATIO</th>\n",
       "      <th>B</th>\n",
       "      <th>LSTAT</th>\n",
       "      <th>price</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.00632</td>\n",
       "      <td>18.0</td>\n",
       "      <td>2.31</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.538</td>\n",
       "      <td>6.575</td>\n",
       "      <td>65.2</td>\n",
       "      <td>4.0900</td>\n",
       "      <td>1.0</td>\n",
       "      <td>296.0</td>\n",
       "      <td>15.3</td>\n",
       "      <td>396.9</td>\n",
       "      <td>4.98</td>\n",
       "      <td>24.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.02731</td>\n",
       "      <td>0.0</td>\n",
       "      <td>7.07</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.469</td>\n",
       "      <td>6.421</td>\n",
       "      <td>78.9</td>\n",
       "      <td>4.9671</td>\n",
       "      <td>2.0</td>\n",
       "      <td>242.0</td>\n",
       "      <td>17.8</td>\n",
       "      <td>396.9</td>\n",
       "      <td>9.14</td>\n",
       "      <td>21.6</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      CRIM    ZN  INDUS  CHAS    NOX     RM   AGE     DIS  RAD    TAX  \\\n",
       "0  0.00632  18.0   2.31   0.0  0.538  6.575  65.2  4.0900  1.0  296.0   \n",
       "1  0.02731   0.0   7.07   0.0  0.469  6.421  78.9  4.9671  2.0  242.0   \n",
       "\n",
       "   PTRATIO      B  LSTAT  price  \n",
       "0     15.3  396.9   4.98   24.0  \n",
       "1     17.8  396.9   9.14   21.6  "
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataframe['price'] = dataset['target'] # target为房价，也是我们的目标值，我们将目标值赋值给dataframe\n",
    "dataframe.head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>CRIM</th>\n",
       "      <th>ZN</th>\n",
       "      <th>INDUS</th>\n",
       "      <th>CHAS</th>\n",
       "      <th>NOX</th>\n",
       "      <th>RM</th>\n",
       "      <th>AGE</th>\n",
       "      <th>DIS</th>\n",
       "      <th>RAD</th>\n",
       "      <th>TAX</th>\n",
       "      <th>PTRATIO</th>\n",
       "      <th>B</th>\n",
       "      <th>LSTAT</th>\n",
       "      <th>price</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.00632</td>\n",
       "      <td>18.0</td>\n",
       "      <td>2.31</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.538</td>\n",
       "      <td>6.575</td>\n",
       "      <td>65.2</td>\n",
       "      <td>4.0900</td>\n",
       "      <td>1.0</td>\n",
       "      <td>296.0</td>\n",
       "      <td>15.3</td>\n",
       "      <td>396.90</td>\n",
       "      <td>4.98</td>\n",
       "      <td>24.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.02731</td>\n",
       "      <td>0.0</td>\n",
       "      <td>7.07</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.469</td>\n",
       "      <td>6.421</td>\n",
       "      <td>78.9</td>\n",
       "      <td>4.9671</td>\n",
       "      <td>2.0</td>\n",
       "      <td>242.0</td>\n",
       "      <td>17.8</td>\n",
       "      <td>396.90</td>\n",
       "      <td>9.14</td>\n",
       "      <td>21.6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.02729</td>\n",
       "      <td>0.0</td>\n",
       "      <td>7.07</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.469</td>\n",
       "      <td>7.185</td>\n",
       "      <td>61.1</td>\n",
       "      <td>4.9671</td>\n",
       "      <td>2.0</td>\n",
       "      <td>242.0</td>\n",
       "      <td>17.8</td>\n",
       "      <td>392.83</td>\n",
       "      <td>4.03</td>\n",
       "      <td>34.7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.03237</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2.18</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.458</td>\n",
       "      <td>6.998</td>\n",
       "      <td>45.8</td>\n",
       "      <td>6.0622</td>\n",
       "      <td>3.0</td>\n",
       "      <td>222.0</td>\n",
       "      <td>18.7</td>\n",
       "      <td>394.63</td>\n",
       "      <td>2.94</td>\n",
       "      <td>33.4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.06905</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2.18</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.458</td>\n",
       "      <td>7.147</td>\n",
       "      <td>54.2</td>\n",
       "      <td>6.0622</td>\n",
       "      <td>3.0</td>\n",
       "      <td>222.0</td>\n",
       "      <td>18.7</td>\n",
       "      <td>396.90</td>\n",
       "      <td>5.33</td>\n",
       "      <td>36.2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>501</th>\n",
       "      <td>0.06263</td>\n",
       "      <td>0.0</td>\n",
       "      <td>11.93</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.573</td>\n",
       "      <td>6.593</td>\n",
       "      <td>69.1</td>\n",
       "      <td>2.4786</td>\n",
       "      <td>1.0</td>\n",
       "      <td>273.0</td>\n",
       "      <td>21.0</td>\n",
       "      <td>391.99</td>\n",
       "      <td>9.67</td>\n",
       "      <td>22.4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>502</th>\n",
       "      <td>0.04527</td>\n",
       "      <td>0.0</td>\n",
       "      <td>11.93</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.573</td>\n",
       "      <td>6.120</td>\n",
       "      <td>76.7</td>\n",
       "      <td>2.2875</td>\n",
       "      <td>1.0</td>\n",
       "      <td>273.0</td>\n",
       "      <td>21.0</td>\n",
       "      <td>396.90</td>\n",
       "      <td>9.08</td>\n",
       "      <td>20.6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>503</th>\n",
       "      <td>0.06076</td>\n",
       "      <td>0.0</td>\n",
       "      <td>11.93</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.573</td>\n",
       "      <td>6.976</td>\n",
       "      <td>91.0</td>\n",
       "      <td>2.1675</td>\n",
       "      <td>1.0</td>\n",
       "      <td>273.0</td>\n",
       "      <td>21.0</td>\n",
       "      <td>396.90</td>\n",
       "      <td>5.64</td>\n",
       "      <td>23.9</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>504</th>\n",
       "      <td>0.10959</td>\n",
       "      <td>0.0</td>\n",
       "      <td>11.93</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.573</td>\n",
       "      <td>6.794</td>\n",
       "      <td>89.3</td>\n",
       "      <td>2.3889</td>\n",
       "      <td>1.0</td>\n",
       "      <td>273.0</td>\n",
       "      <td>21.0</td>\n",
       "      <td>393.45</td>\n",
       "      <td>6.48</td>\n",
       "      <td>22.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>505</th>\n",
       "      <td>0.04741</td>\n",
       "      <td>0.0</td>\n",
       "      <td>11.93</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.573</td>\n",
       "      <td>6.030</td>\n",
       "      <td>80.8</td>\n",
       "      <td>2.5050</td>\n",
       "      <td>1.0</td>\n",
       "      <td>273.0</td>\n",
       "      <td>21.0</td>\n",
       "      <td>396.90</td>\n",
       "      <td>7.88</td>\n",
       "      <td>11.9</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>506 rows × 14 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "        CRIM    ZN  INDUS  CHAS    NOX     RM   AGE     DIS  RAD    TAX  \\\n",
       "0    0.00632  18.0   2.31   0.0  0.538  6.575  65.2  4.0900  1.0  296.0   \n",
       "1    0.02731   0.0   7.07   0.0  0.469  6.421  78.9  4.9671  2.0  242.0   \n",
       "2    0.02729   0.0   7.07   0.0  0.469  7.185  61.1  4.9671  2.0  242.0   \n",
       "3    0.03237   0.0   2.18   0.0  0.458  6.998  45.8  6.0622  3.0  222.0   \n",
       "4    0.06905   0.0   2.18   0.0  0.458  7.147  54.2  6.0622  3.0  222.0   \n",
       "..       ...   ...    ...   ...    ...    ...   ...     ...  ...    ...   \n",
       "501  0.06263   0.0  11.93   0.0  0.573  6.593  69.1  2.4786  1.0  273.0   \n",
       "502  0.04527   0.0  11.93   0.0  0.573  6.120  76.7  2.2875  1.0  273.0   \n",
       "503  0.06076   0.0  11.93   0.0  0.573  6.976  91.0  2.1675  1.0  273.0   \n",
       "504  0.10959   0.0  11.93   0.0  0.573  6.794  89.3  2.3889  1.0  273.0   \n",
       "505  0.04741   0.0  11.93   0.0  0.573  6.030  80.8  2.5050  1.0  273.0   \n",
       "\n",
       "     PTRATIO       B  LSTAT  price  \n",
       "0       15.3  396.90   4.98   24.0  \n",
       "1       17.8  396.90   9.14   21.6  \n",
       "2       17.8  392.83   4.03   34.7  \n",
       "3       18.7  394.63   2.94   33.4  \n",
       "4       18.7  396.90   5.33   36.2  \n",
       "..       ...     ...    ...    ...  \n",
       "501     21.0  391.99   9.67   22.4  \n",
       "502     21.0  396.90   9.08   20.6  \n",
       "503     21.0  396.90   5.64   23.9  \n",
       "504     21.0  393.45   6.48   22.0  \n",
       "505     21.0  396.90   7.88   11.9  \n",
       "\n",
       "[506 rows x 14 columns]"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataframe"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Question:What's the most significant(salient)显著特征 feature in the huose price?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>CRIM</th>\n",
       "      <th>ZN</th>\n",
       "      <th>INDUS</th>\n",
       "      <th>CHAS</th>\n",
       "      <th>NOX</th>\n",
       "      <th>RM</th>\n",
       "      <th>AGE</th>\n",
       "      <th>DIS</th>\n",
       "      <th>RAD</th>\n",
       "      <th>TAX</th>\n",
       "      <th>PTRATIO</th>\n",
       "      <th>B</th>\n",
       "      <th>LSTAT</th>\n",
       "      <th>price</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>CRIM</th>\n",
       "      <td>1.000000</td>\n",
       "      <td>-0.200469</td>\n",
       "      <td>0.406583</td>\n",
       "      <td>-0.055892</td>\n",
       "      <td>0.420972</td>\n",
       "      <td>-0.219247</td>\n",
       "      <td>0.352734</td>\n",
       "      <td>-0.379670</td>\n",
       "      <td>0.625505</td>\n",
       "      <td>0.582764</td>\n",
       "      <td>0.289946</td>\n",
       "      <td>-0.385064</td>\n",
       "      <td>0.455621</td>\n",
       "      <td>-0.388305</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ZN</th>\n",
       "      <td>-0.200469</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>-0.533828</td>\n",
       "      <td>-0.042697</td>\n",
       "      <td>-0.516604</td>\n",
       "      <td>0.311991</td>\n",
       "      <td>-0.569537</td>\n",
       "      <td>0.664408</td>\n",
       "      <td>-0.311948</td>\n",
       "      <td>-0.314563</td>\n",
       "      <td>-0.391679</td>\n",
       "      <td>0.175520</td>\n",
       "      <td>-0.412995</td>\n",
       "      <td>0.360445</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>INDUS</th>\n",
       "      <td>0.406583</td>\n",
       "      <td>-0.533828</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.062938</td>\n",
       "      <td>0.763651</td>\n",
       "      <td>-0.391676</td>\n",
       "      <td>0.644779</td>\n",
       "      <td>-0.708027</td>\n",
       "      <td>0.595129</td>\n",
       "      <td>0.720760</td>\n",
       "      <td>0.383248</td>\n",
       "      <td>-0.356977</td>\n",
       "      <td>0.603800</td>\n",
       "      <td>-0.483725</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>CHAS</th>\n",
       "      <td>-0.055892</td>\n",
       "      <td>-0.042697</td>\n",
       "      <td>0.062938</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.091203</td>\n",
       "      <td>0.091251</td>\n",
       "      <td>0.086518</td>\n",
       "      <td>-0.099176</td>\n",
       "      <td>-0.007368</td>\n",
       "      <td>-0.035587</td>\n",
       "      <td>-0.121515</td>\n",
       "      <td>0.048788</td>\n",
       "      <td>-0.053929</td>\n",
       "      <td>0.175260</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>NOX</th>\n",
       "      <td>0.420972</td>\n",
       "      <td>-0.516604</td>\n",
       "      <td>0.763651</td>\n",
       "      <td>0.091203</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>-0.302188</td>\n",
       "      <td>0.731470</td>\n",
       "      <td>-0.769230</td>\n",
       "      <td>0.611441</td>\n",
       "      <td>0.668023</td>\n",
       "      <td>0.188933</td>\n",
       "      <td>-0.380051</td>\n",
       "      <td>0.590879</td>\n",
       "      <td>-0.427321</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>RM</th>\n",
       "      <td>-0.219247</td>\n",
       "      <td>0.311991</td>\n",
       "      <td>-0.391676</td>\n",
       "      <td>0.091251</td>\n",
       "      <td>-0.302188</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>-0.240265</td>\n",
       "      <td>0.205246</td>\n",
       "      <td>-0.209847</td>\n",
       "      <td>-0.292048</td>\n",
       "      <td>-0.355501</td>\n",
       "      <td>0.128069</td>\n",
       "      <td>-0.613808</td>\n",
       "      <td>0.695360</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>AGE</th>\n",
       "      <td>0.352734</td>\n",
       "      <td>-0.569537</td>\n",
       "      <td>0.644779</td>\n",
       "      <td>0.086518</td>\n",
       "      <td>0.731470</td>\n",
       "      <td>-0.240265</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>-0.747881</td>\n",
       "      <td>0.456022</td>\n",
       "      <td>0.506456</td>\n",
       "      <td>0.261515</td>\n",
       "      <td>-0.273534</td>\n",
       "      <td>0.602339</td>\n",
       "      <td>-0.376955</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>DIS</th>\n",
       "      <td>-0.379670</td>\n",
       "      <td>0.664408</td>\n",
       "      <td>-0.708027</td>\n",
       "      <td>-0.099176</td>\n",
       "      <td>-0.769230</td>\n",
       "      <td>0.205246</td>\n",
       "      <td>-0.747881</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>-0.494588</td>\n",
       "      <td>-0.534432</td>\n",
       "      <td>-0.232471</td>\n",
       "      <td>0.291512</td>\n",
       "      <td>-0.496996</td>\n",
       "      <td>0.249929</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>RAD</th>\n",
       "      <td>0.625505</td>\n",
       "      <td>-0.311948</td>\n",
       "      <td>0.595129</td>\n",
       "      <td>-0.007368</td>\n",
       "      <td>0.611441</td>\n",
       "      <td>-0.209847</td>\n",
       "      <td>0.456022</td>\n",
       "      <td>-0.494588</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.910228</td>\n",
       "      <td>0.464741</td>\n",
       "      <td>-0.444413</td>\n",
       "      <td>0.488676</td>\n",
       "      <td>-0.381626</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>TAX</th>\n",
       "      <td>0.582764</td>\n",
       "      <td>-0.314563</td>\n",
       "      <td>0.720760</td>\n",
       "      <td>-0.035587</td>\n",
       "      <td>0.668023</td>\n",
       "      <td>-0.292048</td>\n",
       "      <td>0.506456</td>\n",
       "      <td>-0.534432</td>\n",
       "      <td>0.910228</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.460853</td>\n",
       "      <td>-0.441808</td>\n",
       "      <td>0.543993</td>\n",
       "      <td>-0.468536</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>PTRATIO</th>\n",
       "      <td>0.289946</td>\n",
       "      <td>-0.391679</td>\n",
       "      <td>0.383248</td>\n",
       "      <td>-0.121515</td>\n",
       "      <td>0.188933</td>\n",
       "      <td>-0.355501</td>\n",
       "      <td>0.261515</td>\n",
       "      <td>-0.232471</td>\n",
       "      <td>0.464741</td>\n",
       "      <td>0.460853</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>-0.177383</td>\n",
       "      <td>0.374044</td>\n",
       "      <td>-0.507787</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>B</th>\n",
       "      <td>-0.385064</td>\n",
       "      <td>0.175520</td>\n",
       "      <td>-0.356977</td>\n",
       "      <td>0.048788</td>\n",
       "      <td>-0.380051</td>\n",
       "      <td>0.128069</td>\n",
       "      <td>-0.273534</td>\n",
       "      <td>0.291512</td>\n",
       "      <td>-0.444413</td>\n",
       "      <td>-0.441808</td>\n",
       "      <td>-0.177383</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>-0.366087</td>\n",
       "      <td>0.333461</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>LSTAT</th>\n",
       "      <td>0.455621</td>\n",
       "      <td>-0.412995</td>\n",
       "      <td>0.603800</td>\n",
       "      <td>-0.053929</td>\n",
       "      <td>0.590879</td>\n",
       "      <td>-0.613808</td>\n",
       "      <td>0.602339</td>\n",
       "      <td>-0.496996</td>\n",
       "      <td>0.488676</td>\n",
       "      <td>0.543993</td>\n",
       "      <td>0.374044</td>\n",
       "      <td>-0.366087</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>-0.737663</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>price</th>\n",
       "      <td>-0.388305</td>\n",
       "      <td>0.360445</td>\n",
       "      <td>-0.483725</td>\n",
       "      <td>0.175260</td>\n",
       "      <td>-0.427321</td>\n",
       "      <td>0.695360</td>\n",
       "      <td>-0.376955</td>\n",
       "      <td>0.249929</td>\n",
       "      <td>-0.381626</td>\n",
       "      <td>-0.468536</td>\n",
       "      <td>-0.507787</td>\n",
       "      <td>0.333461</td>\n",
       "      <td>-0.737663</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "             CRIM        ZN     INDUS      CHAS       NOX        RM       AGE  \\\n",
       "CRIM     1.000000 -0.200469  0.406583 -0.055892  0.420972 -0.219247  0.352734   \n",
       "ZN      -0.200469  1.000000 -0.533828 -0.042697 -0.516604  0.311991 -0.569537   \n",
       "INDUS    0.406583 -0.533828  1.000000  0.062938  0.763651 -0.391676  0.644779   \n",
       "CHAS    -0.055892 -0.042697  0.062938  1.000000  0.091203  0.091251  0.086518   \n",
       "NOX      0.420972 -0.516604  0.763651  0.091203  1.000000 -0.302188  0.731470   \n",
       "RM      -0.219247  0.311991 -0.391676  0.091251 -0.302188  1.000000 -0.240265   \n",
       "AGE      0.352734 -0.569537  0.644779  0.086518  0.731470 -0.240265  1.000000   \n",
       "DIS     -0.379670  0.664408 -0.708027 -0.099176 -0.769230  0.205246 -0.747881   \n",
       "RAD      0.625505 -0.311948  0.595129 -0.007368  0.611441 -0.209847  0.456022   \n",
       "TAX      0.582764 -0.314563  0.720760 -0.035587  0.668023 -0.292048  0.506456   \n",
       "PTRATIO  0.289946 -0.391679  0.383248 -0.121515  0.188933 -0.355501  0.261515   \n",
       "B       -0.385064  0.175520 -0.356977  0.048788 -0.380051  0.128069 -0.273534   \n",
       "LSTAT    0.455621 -0.412995  0.603800 -0.053929  0.590879 -0.613808  0.602339   \n",
       "price   -0.388305  0.360445 -0.483725  0.175260 -0.427321  0.695360 -0.376955   \n",
       "\n",
       "              DIS       RAD       TAX   PTRATIO         B     LSTAT     price  \n",
       "CRIM    -0.379670  0.625505  0.582764  0.289946 -0.385064  0.455621 -0.388305  \n",
       "ZN       0.664408 -0.311948 -0.314563 -0.391679  0.175520 -0.412995  0.360445  \n",
       "INDUS   -0.708027  0.595129  0.720760  0.383248 -0.356977  0.603800 -0.483725  \n",
       "CHAS    -0.099176 -0.007368 -0.035587 -0.121515  0.048788 -0.053929  0.175260  \n",
       "NOX     -0.769230  0.611441  0.668023  0.188933 -0.380051  0.590879 -0.427321  \n",
       "RM       0.205246 -0.209847 -0.292048 -0.355501  0.128069 -0.613808  0.695360  \n",
       "AGE     -0.747881  0.456022  0.506456  0.261515 -0.273534  0.602339 -0.376955  \n",
       "DIS      1.000000 -0.494588 -0.534432 -0.232471  0.291512 -0.496996  0.249929  \n",
       "RAD     -0.494588  1.000000  0.910228  0.464741 -0.444413  0.488676 -0.381626  \n",
       "TAX     -0.534432  0.910228  1.000000  0.460853 -0.441808  0.543993 -0.468536  \n",
       "PTRATIO -0.232471  0.464741  0.460853  1.000000 -0.177383  0.374044 -0.507787  \n",
       "B        0.291512 -0.444413 -0.441808 -0.177383  1.000000 -0.366087  0.333461  \n",
       "LSTAT   -0.496996  0.488676  0.543993  0.374044 -0.366087  1.000000 -0.737663  \n",
       "price    0.249929 -0.381626 -0.468536 -0.507787  0.333461 -0.737663  1.000000  "
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataframe.corr() #相关系数"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.axes._subplots.AxesSubplot at 0x7f26859b6090>"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "sns.heatmap(dataframe.corr(), annot=True, fmt='.1f') # 查看特征与price的相关性系数，正相关和负相关"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 基于以上分析，房屋里卧室个数和房价最成正相关\n",
    "\n",
    "## 简单化：如何依据房屋里边卧室个数来估计房子的面积？\n",
    "\n",
    "## 在1970s的时候，大家有一个这样的想法"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_rm = dataframe['RM'].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "Y = dataframe['price'].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{6.575: 24.0,\n",
       " 6.421: 21.6,\n",
       " 7.185: 34.9,\n",
       " 6.998: 33.4,\n",
       " 7.147: 36.2,\n",
       " 6.43: 28.7,\n",
       " 6.012: 22.9,\n",
       " 6.172: 27.1,\n",
       " 5.631: 16.5,\n",
       " 6.004: 20.3,\n",
       " 6.377: 15.0,\n",
       " 6.009: 21.7,\n",
       " 5.889: 21.7,\n",
       " 5.949: 20.4,\n",
       " 6.096: 13.5,\n",
       " 5.834: 19.9,\n",
       " 5.935: 8.4,\n",
       " 5.99: 17.5,\n",
       " 5.456: 20.2,\n",
       " 5.727: 18.2,\n",
       " 5.57: 13.6,\n",
       " 5.965: 19.6,\n",
       " 6.142: 15.2,\n",
       " 5.813: 16.6,\n",
       " 5.924: 15.6,\n",
       " 5.599: 13.9,\n",
       " 6.047: 14.8,\n",
       " 6.495: 26.4,\n",
       " 6.674: 21.0,\n",
       " 5.713: 20.1,\n",
       " 6.072: 14.5,\n",
       " 5.95: 13.2,\n",
       " 5.701: 13.1,\n",
       " 5.933: 18.9,\n",
       " 5.841: 20.0,\n",
       " 5.85: 21.0,\n",
       " 5.966: 16.0,\n",
       " 6.595: 30.8,\n",
       " 7.024: 34.9,\n",
       " 6.77: 26.6,\n",
       " 6.169: 25.3,\n",
       " 6.211: 25.0,\n",
       " 6.069: 21.2,\n",
       " 5.682: 19.3,\n",
       " 5.786: 20.0,\n",
       " 6.03: 11.9,\n",
       " 5.399: 14.4,\n",
       " 5.602: 19.4,\n",
       " 5.963: 19.7,\n",
       " 6.115: 20.5,\n",
       " 6.511: 25.0,\n",
       " 5.998: 23.4,\n",
       " 5.888: 23.3,\n",
       " 7.249: 35.4,\n",
       " 6.383: 24.7,\n",
       " 6.816: 31.6,\n",
       " 6.145: 23.3,\n",
       " 5.927: 19.6,\n",
       " 5.741: 18.7,\n",
       " 6.456: 22.2,\n",
       " 6.762: 25.0,\n",
       " 7.104: 33.0,\n",
       " 6.29: 23.5,\n",
       " 5.787: 19.4,\n",
       " 5.878: 22.0,\n",
       " 5.594: 17.4,\n",
       " 5.885: 20.9,\n",
       " 6.417: 13.0,\n",
       " 5.961: 20.5,\n",
       " 6.065: 22.8,\n",
       " 6.245: 23.4,\n",
       " 6.273: 24.1,\n",
       " 6.286: 21.4,\n",
       " 6.279: 20.0,\n",
       " 6.14: 20.8,\n",
       " 6.232: 21.2,\n",
       " 5.874: 20.3,\n",
       " 6.727: 27.5,\n",
       " 6.619: 23.9,\n",
       " 6.302: 24.8,\n",
       " 6.167: 19.9,\n",
       " 6.389: 23.9,\n",
       " 6.63: 27.9,\n",
       " 6.015: 22.5,\n",
       " 6.121: 22.2,\n",
       " 7.007: 23.6,\n",
       " 7.079: 28.7,\n",
       " 6.405: 12.5,\n",
       " 6.442: 22.9,\n",
       " 6.249: 20.6,\n",
       " 6.625: 28.4,\n",
       " 6.163: 21.4,\n",
       " 8.069: 38.7,\n",
       " 7.82: 45.4,\n",
       " 7.416: 33.2,\n",
       " 6.781: 26.5,\n",
       " 6.137: 19.3,\n",
       " 5.851: 19.5,\n",
       " 5.836: 19.5,\n",
       " 6.127: 22.7,\n",
       " 6.474: 19.8,\n",
       " 6.229: 21.4,\n",
       " 6.195: 21.7,\n",
       " 6.715: 22.8,\n",
       " 5.913: 18.8,\n",
       " 6.092: 18.7,\n",
       " 6.254: 18.5,\n",
       " 5.928: 18.3,\n",
       " 6.176: 21.2,\n",
       " 6.021: 19.2,\n",
       " 5.872: 20.4,\n",
       " 5.731: 19.3,\n",
       " 5.87: 22.0,\n",
       " 5.856: 21.1,\n",
       " 5.879: 18.8,\n",
       " 5.986: 21.4,\n",
       " 5.613: 15.7,\n",
       " 5.693: 16.2,\n",
       " 6.431: 24.6,\n",
       " 5.637: 14.3,\n",
       " 6.458: 19.2,\n",
       " 6.326: 24.4,\n",
       " 6.372: 23.0,\n",
       " 5.822: 18.4,\n",
       " 5.757: 15.0,\n",
       " 6.335: 18.1,\n",
       " 5.942: 17.4,\n",
       " 6.454: 17.1,\n",
       " 5.857: 13.3,\n",
       " 6.151: 17.8,\n",
       " 6.174: 14.0,\n",
       " 5.019: 14.4,\n",
       " 5.403: 13.4,\n",
       " 5.468: 15.6,\n",
       " 4.903: 11.8,\n",
       " 6.13: 13.8,\n",
       " 5.628: 15.6,\n",
       " 4.926: 14.6,\n",
       " 5.186: 17.8,\n",
       " 5.597: 15.4,\n",
       " 6.122: 22.1,\n",
       " 5.404: 19.3,\n",
       " 5.012: 15.3,\n",
       " 5.709: 19.4,\n",
       " 6.129: 17.0,\n",
       " 6.152: 8.7,\n",
       " 5.272: 13.1,\n",
       " 6.943: 41.3,\n",
       " 6.066: 24.3,\n",
       " 6.51: 23.3,\n",
       " 6.25: 27.0,\n",
       " 7.489: 50.0,\n",
       " 7.802: 50.0,\n",
       " 8.375: 50.0,\n",
       " 5.854: 10.8,\n",
       " 6.101: 25.0,\n",
       " 7.929: 50.0,\n",
       " 5.877: 23.8,\n",
       " 6.319: 23.8,\n",
       " 6.402: 22.3,\n",
       " 5.875: 50.0,\n",
       " 5.88: 19.1,\n",
       " 5.572: 23.1,\n",
       " 6.416: 23.6,\n",
       " 5.859: 22.6,\n",
       " 6.546: 29.4,\n",
       " 6.02: 23.2,\n",
       " 6.315: 22.3,\n",
       " 6.86: 29.9,\n",
       " 6.98: 29.8,\n",
       " 7.765: 39.8,\n",
       " 6.144: 19.8,\n",
       " 7.155: 37.9,\n",
       " 6.563: 32.5,\n",
       " 5.604: 26.4,\n",
       " 6.153: 29.6,\n",
       " 7.831: 50.0,\n",
       " 6.782: 7.5,\n",
       " 6.556: 29.8,\n",
       " 6.951: 26.7,\n",
       " 6.739: 30.5,\n",
       " 7.178: 36.4,\n",
       " 6.8: 31.1,\n",
       " 6.604: 29.1,\n",
       " 7.875: 50.0,\n",
       " 7.287: 33.3,\n",
       " 7.107: 30.3,\n",
       " 7.274: 34.6,\n",
       " 6.975: 34.9,\n",
       " 7.135: 32.9,\n",
       " 6.162: 13.3,\n",
       " 7.61: 42.3,\n",
       " 7.853: 48.5,\n",
       " 8.034: 50.0,\n",
       " 5.891: 22.6,\n",
       " 5.783: 22.5,\n",
       " 6.064: 24.4,\n",
       " 5.344: 20.0,\n",
       " 5.96: 21.7,\n",
       " 5.807: 22.4,\n",
       " 6.375: 28.1,\n",
       " 5.412: 23.7,\n",
       " 6.182: 25.0,\n",
       " 6.642: 28.7,\n",
       " 5.951: 21.5,\n",
       " 6.373: 23.0,\n",
       " 6.164: 21.7,\n",
       " 6.879: 27.5,\n",
       " 6.618: 30.1,\n",
       " 8.266: 44.8,\n",
       " 8.725: 50.0,\n",
       " 8.04: 37.6,\n",
       " 7.163: 31.6,\n",
       " 7.686: 46.7,\n",
       " 6.552: 31.5,\n",
       " 5.981: 24.3,\n",
       " 7.412: 31.7,\n",
       " 8.337: 41.7,\n",
       " 8.247: 48.3,\n",
       " 6.726: 29.0,\n",
       " 6.086: 24.0,\n",
       " 6.631: 25.1,\n",
       " 7.358: 31.5,\n",
       " 6.481: 23.7,\n",
       " 6.606: 23.3,\n",
       " 6.897: 22.0,\n",
       " 6.095: 20.1,\n",
       " 6.358: 22.2,\n",
       " 6.393: 23.7,\n",
       " 5.593: 17.6,\n",
       " 5.605: 18.5,\n",
       " 6.108: 21.9,\n",
       " 6.226: 20.5,\n",
       " 6.433: 24.5,\n",
       " 6.718: 26.2,\n",
       " 6.487: 24.4,\n",
       " 6.438: 24.8,\n",
       " 6.957: 29.6,\n",
       " 8.259: 42.8,\n",
       " 5.876: 20.9,\n",
       " 7.454: 44.0,\n",
       " 8.704: 50.0,\n",
       " 7.333: 36.0,\n",
       " 6.842: 30.1,\n",
       " 7.203: 33.8,\n",
       " 7.52: 43.1,\n",
       " 8.398: 48.8,\n",
       " 7.327: 31.0,\n",
       " 7.206: 36.5,\n",
       " 5.56: 22.8,\n",
       " 7.014: 30.7,\n",
       " 8.297: 50.0,\n",
       " 7.47: 43.5,\n",
       " 5.92: 20.7,\n",
       " 6.24: 25.2,\n",
       " 6.538: 24.4,\n",
       " 7.691: 35.2,\n",
       " 6.758: 32.4,\n",
       " 6.854: 32.0,\n",
       " 7.267: 33.2,\n",
       " 6.826: 33.1,\n",
       " 6.482: 29.1,\n",
       " 6.812: 35.1,\n",
       " 6.968: 10.4,\n",
       " 7.645: 46.0,\n",
       " 7.923: 50.0,\n",
       " 7.088: 32.2,\n",
       " 6.453: 22.0,\n",
       " 6.23: 20.1,\n",
       " 6.209: 21.4,\n",
       " 6.565: 24.8,\n",
       " 6.861: 28.5,\n",
       " 7.148: 37.3,\n",
       " 6.678: 28.6,\n",
       " 6.549: 27.1,\n",
       " 5.79: 20.3,\n",
       " 6.345: 22.5,\n",
       " 7.041: 29.0,\n",
       " 6.871: 24.8,\n",
       " 6.59: 22.0,\n",
       " 6.982: 33.1,\n",
       " 7.236: 36.1,\n",
       " 6.616: 28.4,\n",
       " 7.42: 33.4,\n",
       " 6.849: 28.2,\n",
       " 6.635: 24.5,\n",
       " 5.972: 20.3,\n",
       " 4.973: 16.1,\n",
       " 6.023: 19.4,\n",
       " 6.266: 21.6,\n",
       " 6.567: 23.8,\n",
       " 5.705: 16.2,\n",
       " 5.914: 17.8,\n",
       " 5.782: 19.8,\n",
       " 6.382: 23.1,\n",
       " 6.113: 21.0,\n",
       " 6.426: 23.8,\n",
       " 6.376: 17.7,\n",
       " 6.041: 20.4,\n",
       " 5.708: 18.5,\n",
       " 6.415: 25.0,\n",
       " 6.312: 21.2,\n",
       " 6.083: 22.2,\n",
       " 5.868: 19.3,\n",
       " 6.333: 22.6,\n",
       " 5.706: 17.1,\n",
       " 6.031: 19.4,\n",
       " 6.316: 22.2,\n",
       " 6.31: 20.7,\n",
       " 6.037: 21.1,\n",
       " 5.869: 19.5,\n",
       " 5.895: 18.5,\n",
       " 6.059: 20.6,\n",
       " 5.985: 19.0,\n",
       " 5.968: 18.7,\n",
       " 7.241: 32.7,\n",
       " 6.54: 16.5,\n",
       " 6.696: 23.9,\n",
       " 6.874: 31.2,\n",
       " 6.014: 17.5,\n",
       " 5.898: 17.2,\n",
       " 6.516: 23.1,\n",
       " 6.939: 26.6,\n",
       " 6.49: 22.9,\n",
       " 6.579: 24.1,\n",
       " 5.884: 18.6,\n",
       " 6.728: 14.9,\n",
       " 5.663: 18.2,\n",
       " 5.936: 13.5,\n",
       " 6.212: 17.8,\n",
       " 6.395: 21.7,\n",
       " 6.112: 22.6,\n",
       " 6.398: 25.0,\n",
       " 6.251: 12.6,\n",
       " 5.362: 20.8,\n",
       " 5.803: 16.8,\n",
       " 8.78: 21.9,\n",
       " 3.561: 27.5,\n",
       " 4.963: 21.9,\n",
       " 3.863: 23.1,\n",
       " 4.97: 50.0,\n",
       " 6.683: 50.0,\n",
       " 7.016: 50.0,\n",
       " 6.216: 50.0,\n",
       " 4.906: 13.8,\n",
       " 4.138: 11.9,\n",
       " 7.313: 15.0,\n",
       " 6.649: 13.9,\n",
       " 6.794: 22.0,\n",
       " 6.38: 9.5,\n",
       " 6.223: 10.2,\n",
       " 6.545: 10.9,\n",
       " 5.536: 11.3,\n",
       " 5.52: 12.3,\n",
       " 4.368: 8.8,\n",
       " 5.277: 7.2,\n",
       " 4.652: 10.5,\n",
       " 5.0: 7.4,\n",
       " 4.88: 10.2,\n",
       " 5.39: 19.7,\n",
       " 6.051: 23.2,\n",
       " 5.036: 9.7,\n",
       " 6.193: 11.0,\n",
       " 5.887: 12.7,\n",
       " 6.471: 13.1,\n",
       " 5.747: 8.5,\n",
       " 5.453: 5.0,\n",
       " 5.852: 6.3,\n",
       " 5.987: 5.6,\n",
       " 6.343: 7.2,\n",
       " 6.404: 12.1,\n",
       " 5.349: 8.3,\n",
       " 5.531: 8.5,\n",
       " 5.683: 5.0,\n",
       " 5.608: 27.9,\n",
       " 5.617: 17.2,\n",
       " 6.852: 27.5,\n",
       " 6.657: 17.2,\n",
       " 4.628: 17.9,\n",
       " 5.155: 16.3,\n",
       " 4.519: 7.0,\n",
       " 6.434: 7.2,\n",
       " 5.304: 12.0,\n",
       " 5.957: 8.8,\n",
       " 6.824: 8.4,\n",
       " 6.411: 16.7,\n",
       " 6.006: 14.2,\n",
       " 5.648: 20.8,\n",
       " 6.103: 13.4,\n",
       " 5.565: 11.7,\n",
       " 5.896: 8.3,\n",
       " 5.837: 10.2,\n",
       " 6.202: 10.9,\n",
       " 6.348: 14.5,\n",
       " 6.833: 14.1,\n",
       " 6.425: 16.1,\n",
       " 6.436: 14.3,\n",
       " 6.208: 11.7,\n",
       " 6.629: 13.4,\n",
       " 6.461: 9.6,\n",
       " 5.627: 12.8,\n",
       " 5.818: 10.5,\n",
       " 6.406: 17.1,\n",
       " 6.219: 18.4,\n",
       " 6.485: 15.4,\n",
       " 6.459: 11.8,\n",
       " 6.341: 14.9,\n",
       " 6.185: 14.6,\n",
       " 6.749: 13.4,\n",
       " 6.655: 15.2,\n",
       " 6.297: 16.1,\n",
       " 7.393: 17.8,\n",
       " 6.525: 14.1,\n",
       " 5.976: 12.7,\n",
       " 6.301: 14.9,\n",
       " 6.081: 20.0,\n",
       " 6.701: 16.4,\n",
       " 6.317: 19.5,\n",
       " 6.513: 20.2,\n",
       " 5.759: 19.9,\n",
       " 5.952: 19.0,\n",
       " 6.003: 19.1,\n",
       " 5.926: 24.5,\n",
       " 6.437: 23.2,\n",
       " 5.427: 13.8,\n",
       " 6.484: 16.7,\n",
       " 6.242: 23.0,\n",
       " 6.75: 23.7,\n",
       " 7.061: 25.0,\n",
       " 5.762: 21.8,\n",
       " 5.871: 20.6,\n",
       " 6.114: 19.1,\n",
       " 5.905: 20.6,\n",
       " 5.454: 15.2,\n",
       " 5.414: 7.0,\n",
       " 5.093: 8.1,\n",
       " 5.983: 20.1,\n",
       " 5.707: 21.8,\n",
       " 5.67: 23.1,\n",
       " 5.794: 18.3,\n",
       " 6.019: 21.2,\n",
       " 5.569: 17.5,\n",
       " 6.027: 16.8,\n",
       " 6.593: 22.4,\n",
       " 6.12: 20.6,\n",
       " 6.976: 23.9}"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rm_to_price = {r:y for r,y in zip(X_rm,Y)}\n",
    "rm_to_price"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "13.6"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rm_to_price[5.57]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_price_by_similar(history_price,query_x,topn=3):\n",
    "    '''\n",
    "    代码的可读性一定是大于简洁性\n",
    "    '''\n",
    "    most_similar_items = sorted(history_price.items(),key=lambda x_y:(x_y[0] -query_x)**2)[:topn]\n",
    "    most_similar_prices = [p for x,p in most_similar_items]\n",
    "    average_prices = np.mean(most_similar_prices)\n",
    "    \n",
    "    return average_prices\n",
    "#     return np.mean([p for x,p in sorted(history_price.items(),key=lambda x_y:(x_y[0] -query_x)**2)[:topn]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "29.233333333333334"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "find_price_by_similar(rm_to_price,7)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "14.6"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "find_price_by_similar(rm_to_price,4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## MIT计算机系名言：代码是给人看的，偶尔运行一下"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## KNN -- find_price_by_similar,这是一个非常非常经典的机器学习算法"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 什么是机器学习？\n",
    "+ 学习：通过观察到的数据，解决没见过的问题，预测 or 分类\n",
    "+ 机器学习：用机器通过观察已有数据，去预测未知数据，实现分类或回归"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 对KNN最大的问题就是，数据量变大的时候，运行时间会变得很长 --Lazy learning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## A more efficient learning way\n",
    "## 如果可以找到X_rm 和Y之间的函数关系，每次要计算的时候，输入这个函数，就能直接获得预测值\n",
    "## 拟合函数关系"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "29.233333333333334"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "find_price_by_similar(rm_to_price,7)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.collections.PathCollection at 0x7f267f793f50>"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAW4AAAD2CAYAAAD24G0VAAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADh0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uMy4xLjMsIGh0dHA6Ly9tYXRwbG90bGliLm9yZy+AADFEAAAgAElEQVR4nO2df5Ac5Znfv8+OGmkWO5qVvanAREI4rhOJLKQ91rbKIneWrkC58KM2YCIbKLuSVFGV3FUKTG0iVWEjLrqzLlsp8NW5LqEqf6SMi5NBsEHWOfIP6fJDd4KsvFopclBigvkxkJw4tOJgRzDaffPHbI96evrtfrunf8/3U0Wxmp7pfrt359tvP8/3fR5RSoEQQkhxGMp6AIQQQsJB4SaEkIJB4SaEkIJB4SaEkIJB4SaEkIKxIukDfPKTn1Tr169P+jCEEFIqTp48+Y5SatRrW+LCvX79eszMzCR9GEIIKRUi8ppuG0MlhBBSMCjchBBSMCjchBBSMCjchBBSMCjchBBSMBJ3lZBsmZ5tYOrIObw138S1tSomd27AxFg962FFJu7zSfP6cOxX9tOYb6IigkWlUKtaEAEuLLQwJMDSct27WtXC3js3YmKs3tfxvT4LILZrNz3bwGOHzuLCQqtn3EkhQdUBReSzAJ4H8Mvll34LwO8CWAvgNICvKp+djI+PK9oBs2F6toE9z51Bs7XYea1qVfCtuzYVUrzjPp80rw/H7r2fIKwhwa7PrcXBk41Ix/c6plURQAGtpSuyFfXaTc82MPnsHFqL3RJoDQmm7tnc1+9CRE4qpca9tpmESkYA/JFS6mal1M0APgvgTaXU5uVtt0QeGUmUqSPner4kzdYipo6cy2hE/RH3+aR5fTh27/0E0VpSePrFNyIf3+uYrUXVJdph9ue1f7do2+NO8ntmKtx3i8hLInIQwG8A+PHytqMAtrs/ICIPiMiMiMycP38+vtGSULw13wz1et6J+3zSvD4ce/SxLWoe6E32F+aYUcbn95kkv2cmwv0LAN9QSn0OwDUA7gJwcXnbewDWuD+glHpSKTWulBofHfVcsUlS4NpaNdTreSfu80nz+nDs0cdWEYm8vzDHjDI+v88k+T0zEe5fAviJ4+clAKuX/70awDuxj4rEwuTODahala7Xqlalk5wpGnGfT5rXh2P33k8Q1pDgK59fG/n4Xse0KgJrqPtmEPXaTe7c0I6Ze4w7ye+Ziavk6wD+l4h8F8BnADwM4FYABwHsAPB4YqMjfWEnRsriKon7fNK8Phx7937CukrGr1sT6fi6sQPA3hfOYr7ZdoKssqI5o+3959FVcg2ApwFcDeBPAPwe2qK9DsAc6CohhEQgS6tqERxXfq6SwBm3UuptAF90vXx7DOMihAwobuFszDex57kzABCLcAbdFPycMnkRbj+4cpIQkjpJ2hntm0JjvgmFKzeF6dlG5z1Fd1xRuAkhqZOkcJrcFIruuKJwE0JSJ0nhNLkpFN1xReEmhKROksJpclOYGKvjW3dtQr1WhQCo16q5SkwGwSJThJDUSdLOOLlzg6djxH1TmBirF0ao3VC4CSGZkJRwlm39ghcUbkJI7GRdTrjIs2kTKNyEkFhJ2qNNmJwkhMRM2coJ5xHOuAkhseDsbuNFURa3FAEKNyGkb0y62xRlcUsRoHATQvomqLuN046XdeKyDFC4CSF94xcGqTvEmYnLeGBykhDSN7owSL1WxfHdO7q81Uxc9g+FmxDSN6ZL2NOsyjc928C2/Udx/e7D2Lb/aFd1wKLDUAkhpG9MVyteW6t6uk7iTlyWPSRD4SaExILJakXTOiL9UvRGCUFQuAkhqZFWHZGiN0oIgsJNCNGShHUvjToiaYVksoLJSUKIJyYtwKLuN+mkYdEbJQRB4SaEeKKLE+994WzkfSZ1M3BT9EYJQTBUQgjxRBcPnm+2MD3biCSCaSYNy1zalTNuQognfvHgBw+cihTmKHvSMC0o3IQQT4LiwVHCHGl2Vy/zAhwKNyHEk4mxOkaGLd/3hF2unlbSMK1YelZQuAkhWh69Y2OP0LppzDeNZ7ZpJQ3LXhOFyUlCiBbnghldgwQBOttMlpankTQseyydM25CiC8TY3Uc370DT+za0jP7FgDK9f48zGzTjKVnAYWbEGKEV5jDLdo2Wc9sdbH07TeMliJhyVAJIQUgL11j3GGObfuP5nJpuVdNlO03jOLgyUYpKgZSuAnJOXkuUZpWtb8oeN1kylIxkKESQnJOnh0SRVpaXqaEJWfchOScvAtOUZaWl6liIGfchOScsjsk0qJMFQMp3ITknDIJTpYUKawTBEMlhOSctLrGDAJFCesEYSzcIvJ1AH8fwJcBPA+gBuCwUmp3QmMjhCxTdMHJi52xLBiFSkTkOgBfW/7ngwAOA9gM4DdF5FcSGhshpASUveBTFpjGuL8NYM/yzzsA/FgptQTgPwPY7n6ziDwgIjMiMnP+/Pl4RkoIKSR5tjMWlUDhFpF7AcwB+PnyS58AcHH55/cArHF/Rin1pFJqXCk1Pjo6GtdYCSEFJO92xiJiEuO+HcA6ADsBbACwBGD18rbVAF5LZmiEkDJQJv90XgiccSul7lVK3Yx2UvIkgO8AuFVEhgD8OoBjyQ6REFJk4rQzlrmrTRii2AH/AG1XyX0ADimlfhHvkAghZSIuO2Oea7akjSilK8wYD+Pj42pmZibRYxBCyo+uEmG9VsXx3TsyGFGyiMhJpdS41zYuwCGkpJTNO80k5xW45J2QElJG7zRrtlyBwk1ICSmbd3p6toGFjy73vD6oNVsYKiGkYJiEQHSNfYsYVnAnJW1qVQt779xY6PBPVCjchBSE6dkGHjt0FhcWWp3XvJwV07MNzya+QDHDCl5PDwBw9coVAynaAEMlhBQCe9bpFG0bdwhk6sg5T9EWoJBhBSYle6FwE1IAdLNOG6eI6QRNoZh+ZyYle6FwE1IAgmaXThHzE7T1BVxxyEYSvVC4yUBR1CXTfmLsFrHJnRtgVUT7/sZ8Ew8eOIWx3/lRIc6/TJ1r4oLJSTIwFHnJ9OTODeGcFQYLoi8stLDnuTOYee1dHHv5fK4X6hS9kUTcULjJwODnbc67KISp9zF15BxaS2alLJqtRXzvxOsdnS/SzWyQYaiEDAxFdydMjNUxuXMDrq1V8dZ8E1NHznmGOsKej1vii7xQZ1DgjJsMDEWvC20a6tGdZxj6vZmVrU5K3uCMmwwMeXcnBCVOTZexe51nWPq5mZWxTkreoHCTgSHP7gQTsdPNghvzzS6hd5/nsDUE0ZtMeuj3Zla2Oil5hKESMlDk1Z1gkjitDVueKyeB3rCJ/d8j02fw1InXfY8ty/ueX2jFEtYoei6hCFC4CckBJmIX1PPEyyHz9ItvBB5bAbjUWsLju7bEclMrei6hCDBUQkgOMFnWfbHpPdt24r4BLBp2uIozlJH3XEIZoHATkgNMxM5kxup+TyVEcDuuUEaecwllgaESQnKAyQIb3epJG69Z7Vc+vzYwxm0TZygjr7mEskDhJiQnBImdU9wb802IXIl7jwxbePSO3qXv+yY24fmfNfDBR/rKgkCv6NOHnW8o3ITkiCDBnBirY+a1d9vL1B3h60utJe0+FwJEWwDcfdOVm8b0bAOTz86htdg+QGO+icln5zrHJ9lD4SYkJ5isjJyebXTVFrFxJhftGXlFBItKdf6vQwE48NIbGL9uDSbG6njs0NmOaNu0FhUeO3SWwp0TmJwkJCeYLFzRdbcBrgi9bcWzxdrEWdJaUp3j6LziutdJ+nDGTUiM9BMbNvFy+zk/KiK+XXJ0fShN9k3yBYWbkJiIUu/bKfRDmpCGu7uN1+IWQfDMOmjebR+nVrUw7+EZr1WtgD2QtGCohJCYCFujw12fxEt4BcD2G0Y7//byewuAL/ytNQhRjqQHa0g6rpK9d26ENSQ92/feubGPI5A4oXATEhNha3QENQAG2rPkgycb2gJS9VoVj+/agl/+ZdN3Rl21KhgZ9p4xiwBT92zuPBVMjNUxdc/mrmM4t5PsYaiEkJgIW6PDNKbsrkHi5fd+6MAp332ssoZw243X4ODJRtfNompVuKqxgFC4CYmJyZ0bMPnMXFfbMGcIwk2YhgcmXd799nVhoYWDJxu4+6a6tr+kHW9vzDe7EplsZ5Y/KNyExIk70OwTeJ7cuQEPHThl0tcXqwMSg0HL4YH2zP3Yy+dxfPeOnm3uxKrOJ07hzgeMcRMSE1NHznkuXNElJyfG6rhv6zqjfX/w0WXfDjLu2LcOr5n79GwDD39/LjDeTrtgfqBwExITfh1qdKK7b2KTNmnoxOsG4G51BgDHd+/Aq/tvQ92gTKy9jz3PnTFapMN62vmBwk1ITPgJm1fPRVt4TVckOm8MQa3OTGtimzhbdJ8l2UHhJiQm/Jr0uv3cTuE1xXlj0HnGHzt0Ftv2H8VDB05h5YohjAxbvjWx/cIfdsiF9bTzR2ByUkRWAHgawLUAzgH4ZwCeBbAWwGkAX1XKsM0GISXGFrYHNdY8O2QyMVbH3hfOGs10bdwzXp3gXlhodWbw880WqlbFtyWZzo1SEcG/+Yf0bucVkxn3BIA5pdQ2ANcA+G0AbyqlNgMYAXBLguMjpHD4dZ3Z89wZPDJ9xnNJuY3Xp3913eou296QYWeboJZkupAKRTvfmNgB/xOAP1meedcA/CqAg8vbjgLYDuBHyQyPkPhJqkmASaKv2Vr0beCrK8H6Z6+8i0emz+AHc2/7ir4XfuEQk847JH8ECrdS6n0AEJEXAbwN4BMALi5vfg9AT8ZCRB4A8AAArFtnZnciJA2iFIIy5bFDZuEPP2HXbVOAZx1um4oIPr5qhaeoB7lB2GaseASGSkTkEyKyEsAX0A6NfAbA6uXNqwG84/6MUupJpdS4Ump8dHTUvZmQzAhbCCoI2xmyfvdhY3eILpQyMmxpbXyAf3W/RaUggp7iUHSDlBOTGPfDAO5RSi0CWADwuwBuXd62A8CxhMZGSOyELQTlRxRniFURbP3UiOe22268BpM7N0Su8ndhoQVIu/wqu6uXG5MY93cAfFdEfgvAKwD+PYCDInIawByAnyY4PkJiJWwhKC+cNT3C0lpUOP7Ku57bjr18HvsmNl3pKenYFtQEwbn/v7p02ddJQoqPSYy7gfbM2sntyQyHkGTxqukRJpzgjpHHiT3r3zexCQDw9ItvdHpGbv3UCH72+kXjGDqLQpUbFpkiA0UUF4VJl5o4sGf9j0yf6ZpxLyqFn71+saey38JHl7VxdRaFKjcUbpI6SdnxTAnjonDPsINE2w5piABh9N2qtMu/+nVxd1f2C5r9syhUeeGSd5IqQTU28oZpLQ/A0Y1m/21mAWkn6srxdB91C7FdEVDnUmFRqPKS2xl31rMykgx+drw8/n5NZq1eXWTCNEkAgNaS6vy966h5VBG0j9lP3J4Uj1zOuIs2KyPmxGnHSwPdrLUi4mu5iyKajfmmb8OE9y951+T26kNJG2C5yeWMu2izMmJOHHa8OHA/0W2/YdSzpZfOhaITRud+TS18NhUR+JUgsWflXsfl6sfBIpfCXbRZGTGnXzteHHgte3/qxOud7V7L4E3CdiZWwapV0W5fVArzAasv+R0gQE6FOy+zMhI/eShqZJJwdD7hmc5mg/ZbEelY+nTx7yA3igKwfvdh1KoW9t65kbPsASWXMW7T7h2ERMF01mryPmf7sKBk5KJSOHiyge03jGobLiwZxlbmmy1MPjPHvM+AkkvhZrKlvOQh8Wz65Bb0Pve5mNBsLeIHc29jleX/1bMtfn61ve2YNxk8chkqAZhsKSt5SDx7xdndmDzhhfF4OzGpp72kVNsPDuD63YeNvd1kMMjljJuUlzwknr2e6O7fui70E15Qv0a/2XIQztm+38yfeZ/BJLczblJO8pJ4juOJTncu9VoVx3fviFyQyl7+bjO5cwMmn51Da7F73m0NCfM+AwqFm6RKHuyAgLmP24+gc/Fy0PgVhgLazRQevaPbLWL//Nihs53P0lUy2FC4SapkZQd0CnVt2MLFhRaWlreZ+LjtfbjF8+6b6jh8+u3OaytX+Ecfb7vxGhw82egSe3uhTt3nWvg9IbA8xOAhKqESlTbj4+NqZmYm0WMQ4ide/dTQtsV05rV3u8TdyRDQuQkA7RDGx1atwIWFVs/qyapV6fJye23/1l3tetxRF/34rewkxUFETiqlxj23UbhJUXF2otEJ4MRYHdv2H43UrSYpalULpx69VTuuWtXCh5eXjMRYtw87zk6Ki59w01VCCom736NX/eo9z53G2O/8KFeiDbTtgNOzDa0rZb7ZMm5onAeXDkkfCjcpJGbL1peMO6+nzdSRc6GdNF5irNsHbYLlhslJUjimZxu5m0WHpTHfxMiwBWtI0HKsc69aFayyhjxvONfWqp5uGHeyk+Uhyg+Fm8RCWs4GO0RSBtzibFv8AO/GCNtvGPWsali1hjAybGF+oUVXyYBA4SZ941UmNaku41GXmReBDy+3vSk6y6Tu3JutJQCCx3dtoWAPCKUWbvpb08G0/oju9xHm91TmpFtQKdmHDpwy+iwpP6UV7jRngYOOibNB9/uYee3drhht0O9pddUyKtJUVPxuTEF9LMt8UyPdlNZV4jcLJPFi4mzY+8JZz9/H0y++Yfx7mp5t4OKl8oo24O8G8apTb/pZUi5KO+OmvzU9gmp2TM82tLPkRc0CsLfmm5iebWDvC2dLPcN2EuQG8apZYvpZUi5KO+OmvzU9ghpf+D3l6Eqf1oYtTD4zNzCiXRExWqY+MVbH7DdvxRO7trDRyABT2iXvrOGQH/waAdy/dZ2nD3nliqGBEW2beq3aV+KWlAu/Je+lDZXkoSlt1uTlS69Lqo0MW9g3sQnj163p1BypiKDZWoxk+RsZtnK7UjIIATrXKGriluSDNL53pZ1xDzp5euIwGUs/FfzioFa10Fpcwgcf9Xf8J3ZtwcPfn9PG7r1wF8iyqYh47ocFpPJLnN87FpkaQPLkqjFp/pz1wpr5Zqtv0Qba5+on2l5t0nTv9kvcknyS1veutKGSQSdvrpqgVmFlEKORYQtAW4zDlFrVlWbVzbiZYM8vaX3vOOMuKWm4aqZnG9i2/yiu330Y2/YfxfRsI/Lnh/porJsHrIrg0TvadUa8/NZ2rRGv66V7/1c+v9bzddr+8ktabjYKd0nRiUFcX3pnPWyFK4kzU/F2fz5MTDhvVEQw9aXNnScKr9DQ3TfVcfBkw/N66UJJ+yY2BYaYSL5I+ntnw+RkiUkyu91v5xW/8MCSUqgNW1AKuNhsBS71zhJrSDB1z+bA68pONYNDXN+7gbQDkuC4cj/oYnaN+Sa27T8a+Eer+/ySUnh1/20Aur8AeUCk7T7x6rQe9GXNW86BJEeS3zsbI+EWkf8AYAOAvwBwL4A/BrAWwGkAX1VJT9tJ7qhpPNNefmSg13esm0UPieD63YcxfFUlFpdHnCgFzH7z1p7XTQqa6c6XiUYShcAYt4jcDGCFUmorgL8G4B8DeFMptRnACIBbkh0iyRvTsw28f+my5zav3o9eVihdwaRFpaCA3Ik20A5rOLGTqw8eOBVoAUsr9kkGA5Pk5P8D8G3H+/cC+PHyv48C2B7/sEiemTpyrqvdVhBe4QB3Qk5XsyRPbL9htPOzu1mxF87zNvGyE2JKYKhEKfW/AUBE/gGAJQCzAC4ub34P7RBKFyLyAIAHAGDdunVxjZXkhLBxWXc4wB0Pvm/rOjx14vU4h5gIB156A+PXrcHEWN1owZD7vNOIfZLBwMgOKCJ3AvjnAO4A8H8BrF7etBrAO+73K6WeVEqNK6XGR0dH3ZtJwQkTl3WHA7xshEUQbQBoLalO+CPI5cIwCEkSkxj33wAwCeB2pdRfAfgpADtDswPAseSGR/JIUEF/mzwube8Xu064X2CHYRCSNCaukq8BuAbAEWnHIb8LoC4ipwHMoS3kZICwBSmoyYGXP7no9rchETyo6f0oABv2klQwiXH/PoDfd73875IZDikKdrz273zjh1hoLfVst+t2uMnzYhoT/FZ40hNL0oJL3klf/N5dN8KqdAcOnHU73DidGXlmSICVK8J/PcIs+yckKlw5mTPy0vzAlLANK469fD7N4fmiq4MNAEsK+Ohy75NEELZ/O8+/M1J8KNw5wmQFXhpjCHvjCGNzy1OMOyi0ETX0kadzjELRJg+DCEMlOSLr5gf9Vvwz2f8gUORl7En/DZB4oHDniKwLESV545iebeDhZ+YKk8CLuo5T59/ut3Z5WmQ9eSBmULhzRFpF2HUkeeOYOnIOiyGWydtYQ4Krrwr2jMfNfVvXGXnVnej820WaxWY9eSBmULhzRNaFiJK8cZh88d2zXAHwuetHUBu+qu/jh6Feq3Y1MTBl4SPvwltFmsVmPXkgZlC4c0TWhYiSvHGYfPHd83EF4M9eeTcW37dp6MN5vhNjdRzfvQNP7Npi9PkLCy08dOAU1rvCIUWaxWY9eSBm0FWSM7IuRLTKGurMDp2NAvplcucGPPzMXOhwSVwxcb/91GtVXwfFxFhdu1pSdxynI8iv9rjduiwvhLV3kmygcJccU2uX24oIAB9G8DHrsI/5Lw+ejnW/ToatIc9VnH54tQ7zuma6zu1+2OGQyZ0beq4t0F6Fmbbd04SsJw8kGIZKSkyYpFgacdiJsTrO7fvN2PY3Mmx1Qkr3b12HkatX+r7fJASgu2ZRV3y+Nd/shMC8ao7nNdZN8g2Fu8SEEeM447BB1rda1buOSRjqtSoevWNjJwzxvROv+86Ia1WrK9lYEelcC+f4dNfs2MvnMWyF/7rYsf2JsTqWNHVO8hjrJvmGwl1iwohxXG6CoFn+I9NnfCsKmlC1Kth+w2hXBxq/GLY1JJ1YvZ18s4tFucfnd82aIcMw7hk9HRskLijcJUYnCHZSzElcboK9L5zVzvKnZxv4Xp9NE2ynzbGXzxvV9a7Xqpi6Z3MnZvvYIf34AH9xNRFYOxjidgRNzzbwwYe9dkE6NkgUmJwsMWGSYnG4CaZnG9rZ9FvzTUwdOdeXS0Rwpcb3QwYuD3ficXq24dmZ3h4f0L5mk8/M9fTUbMw3YdIWU2mO6/V7GJLum0YWCUHWJSkmFO4SY38BH/7+XE8daa8qdv26CfySbNcuW+76YbUjNh5U19uqCD748DKu3324I0hB4+ugEWifUtxduM9T1/XHvjdkUUwMyEdRMxINhkpKiDM5OHXknLb4f9xJMb/9Te7c0Hcsd77Z6iQ7vUI7tt6ODFuAar/fjrM/eOCUr9DbrpGpI+fQWjRTaF1nevd5mlznLNwlRVrRSbqhcJcMr+Sg7gm/pulSE+WY2/Yf1YZBRoatrsRgPzhnhe5Vpo/v2oIndm3Be83LPaGOIA6ebGB6thHqZraoVM+19YpZm96w0naXFGlFJ+mGoZKS4TWL0knY+5cu971yTxe/talalU43HGcc3b6hRIl527PC47t3dI3dHotfe7GgfYZtraZwpSGD02IIXDlfXa7BzbW1aqoxZ9250uWSfzjj9qEopTidhBGd1pLq+7HYr2t7UK2VfhKVXrPCfjvIN+abkZ4KbPHWWQyB7jZow9ZQT7s3t8UxjSqCrEtSXCjcGopUitNmerYRuo50v4/FQXFtp2g/Mn0GDwXEmoG2CNaqlrbhMOA9K4yrCXHYqoBA703IaYHc81y3d11BsOuza3uKiXlZHJOMOWdd1IxER1SEx8owjI+Pq5mZmUSPkQTb9h/1FAKv2hZ5QTdmQduR4WXV6/d8dMd073t6toGHDpwKnGWbWulGhq1OCMYOvcSBe8zuY4cJ7wRdd9vp8tZ8E7VhS2tVFACv7r8t3ImQwiMiJ5VS417bOOPWUMTEjW5sCsDeOzdGfiz2Cxn51fBwjsfEwy1oz5qdx7Bnhe5l8hcWWph8Zg6Tz87FJtruMTtnpEA7hm2HRUyoDXuLNnDlCc5+otOJNsCYM+mFwq2hiMuTdWOr16qRH4uDQkZ+Xdud4wm64Tlnsu5jTIzVcfXK3jx6a0kZW/dMcV9Dr2XyJkesWhVf37edyDTZD2POxA2FW0MREzdBY7YbA7y6/7YeR4aOIK9vUIzbxu+GNyTeMeLHDp3t/DuNJx2vGT8QPulp3xQv+tRkMXW+MOZMvKBwayhi4iaJMQeFjHSCXKtaXcf1c2voLNcXFlodAY3ypGNVJFQlQt2MP8xNw16WPzFW1455ZNgySn7aT0qEuKGP24ciFpSPe8xBXl8vj3LVqmDvne3EodOXXBu2sHLFUKjqgPayfFMvtJOrr1qBvXdu9Kw94sQr4egsCRDG2+0Ua921sZOqQf73PD/dkWzhjJv4opspL3x0ZfGObpbvjo9fWGiF7n5jz3bt44ThYrOFibE6pu7Z3DXzHraGupow6CTdWXjKJCHpFlu/a+PeNjJsoVa1CvN0R7KFdkAfilg5LYkxT882sPeFsz0z5apV8RUYP6ugKW57YNh91gOuwfRsw7MIl/vY63cf9j1OnP05CQFoB4xEURfgJDFmnavDTiDqrIJhYsNWRWAN9a4mdIcLvJ4ArCHpWYlo43cN/JbIu4+tKyhlk1QfTUK8oHBrKGLltCTHrBPhCwst7Y0iKKFYEemEBqa+tBlT92wOTKx6hR+m7tmMqS9t1ib8dNdA5xapiPQcO8gFkve/DVIumJzUUKYFOHGM2TRB50zqTe7cgMln57Re6yWlelYEmoQadAnYibE6rt992DNm7XUNdNdlSame/Zt0ec/z3wYpF5xxayjTApw4xhym+JIzoXj1Vfq5QZR+lkFFv8Jcg+GrvM/H670m518btgpXlIwUEwq3hjIuwOkHrxCFziPtFD6/RShhxmUavze9Bo9Mn8EHH3mESYbEc1zu5e/uiLdVEbx/6XKhciKkuDBUoiGOHoxRieoMSXvMt2++BgdPNnp8yu4Vk14hBvcCHT90zg9d+zUg+Bo8/eIbnsdaWuoNkzj37Wz+6zzGBx9e7nHdeI2vX4rodCLxQztgzvCqSBdku3N+NqkvtW5cd99Ux7GXz2uP2c/56D7vJGrlPD973xO7toS+brrYepyV/fq9lqRY+NkBjWbcIunazPIAAAd/SURBVGIBeE4pdYeIrALwLIC1AE4D+KpKWv0HCD9niN+XM+nGr7pxHXv5vG9Z2H6fAoLqhESN31dEtE6RKNctjW4yUf82SPkIFG4RqQJ4EcCvLL90P4A3lVK3i8gPANwC4EfJDXGwiOoMMf1SR52V9+NY6WcZvt/+BeHi5E6+8vm1eOrE657booihbnl7nDmRIjqdSDIECrdSqgngRhH5xfJLOwAcXP75KIDtoHDHRtSZm8mXup9ZeVb9Cf1siArRnyb2TbSXz+vE23093Te87TeM9oSIvnXXpkTjz+wRSWyiuEo+AeDi8s/vAVjjfoOIPCAiMyIyc/68vl4z6SWqM8TEBtfPAp2sXDZ+dULCthdzs29C36LMed28HC1PnXi9x0ECIHTZ3DAU0elEkiGKcL8DYPXyz6uX/92FUupJpdS4Ump8dFTfIYX0ErU0q8mXut9wRxZlbifG6rhv67oe8Y5LsEyum0k97jRWThax1DBJhih2wJ8CuBXtcMkOAI/HOqIS0K+7I0pM2CQJ2O+jdlZlbvdNbML4dWu059bP9Ta5bqYx5DRizUUsNUziJ4pwfw/AXSJyGsAc2kJOlkna3eFH0Jc6jQSamyiiqvuM1+eiXG+v/fs5Y0yX+zPWTNLCWLiVUp9e/v+HAG5PbEQFJ8+WrTgX6JgIclRRNfmMfXwvQfW73lHGZNLEIa4bIBfYEBO45D1m8m7ZcvadnNy5AVNHzoWurWG6/DxKMtTkM87j69Bd7yhj8oot3791Xeyx5iKWEibZwCXvMVMUy1Y/IR3Tp4ooNzGTz5gkC3XXO+qNNY3Ycp6f1ki+4Iw7Zopi2erHGmgqflGqFZp8xuTpRXe981z1Me9PayQ/ULhjJq+WLXdJVF2YwUQkTMUvyk3M5DNBIjsyrC9glccbq/270dWNyMNNheQLhkoSIG+WLa+wiFdnc0AvEs6k2eqqBasiXQ0SvMQvSjLU5DN+yUJnF/Wo+9edt/3esOfkR1ARraxvKiSfsDpgRqTpHtDNsN3iras05yUu1pDgY6tWYH6hpV0CnuTNy+kqsQtGBTUGjnIMr/OGoOemFfWpyu/pJ+7zIcWi7+qAJF7S9nrrwh8KbXEIEluveHhrSWH4qhWY/eatmXjXs0oWtpZ6Jzr9JBB1vxsBfL3lZLChcGdA2u4BndOlXqsaiUNQ0qysbogwScGoCcSiuJBIvmByMgPSdg/0m5ALSkamdT4mPSfjJIx4RhXaPCZLSf6hcGdA2pa0fp0uQeKSxvlksTjF67ytIYFV6S551Y/Q5tWFRPINk5MZUMQWVH7J1DTOR5fEMw33RCVpVwkhOpiczBlZNiKOil8yMI3zMW0UEfcYdOed598VKT8U7ozIm9e7X5I+n6AkXpZVGQlJG8a4SSEIirP3s4SfkKLBGTeJlTjrbzsJCsewzgcZJCjcJDaSrL9t/1u3H/qhySDBUAmJjaTqb5tAPzQZJDjjJrGRVP1tE4ro1CEkKhRuEhtRwhVxhjjK5tQhRAdDJSQ2kqq/TQjphjNuEhtJ1d8mhHTDJe+EEJJD/Ja8M1RCCCEFg8JNCCEFg8JNCCEFg8JNCCEFg8JNCCEFI3FXiYicB/Baogfpn08CeCfrQaTEoJwrz7NcDOJ5XqeUGvV6U+LCXQREZEZnuykbg3KuPM9ywfPshqESQggpGBRuQggpGBTuNk9mPYAUGZRz5XmWC56nA8a4CSGkYHDGTQghBYPCTQghBYPCDUBEvi4iP8l6HEkhIp8VkTdF5L8t/1fqYtci8i9E5ISI/FBErsp6PEkgIl90/D7fEJGvZT2mJBCRq0XkP4rIcRH511mPJylEZERE/nT5PL8R9P6BF24RuQ5AKf/oHYwA+COl1M3L/4Vr6FggRORTADYqpbYC+CGAv5nxkBJBKfWn9u8TwGkAs1mPKSHuA3BCKbUNwEYR+dtZDygh7gVwdvk8t4nI9X5vHnjhBvBtAHuyHkTCjAC4W0ReEpGDIiJZDyhBfgPAiIj8FwB/F8CrGY8nUURkGMCnlVKnsx5LQswD+JiIVABUAXyU8XiSQgB8fPm7KQC2+L15oIVbRO4FMAfg51mPJWF+AeAbSqnPAbgGwK9nPJ4kGQVwXin1a2jPtm/OeDxJcwuAn2Y9iAR5HsDfA/AKgP+plHol4/EkxVMAagAOAvgQ7ZuUloEWbgC3oz1D+2MAN4nIb2c8nqT4JYCfOH7+65mNJHneA2CHgv4PgLL3QLsDwA+yHkSC7EE7zLcewBoR+ULG40mSf6KUugtt4f4LvzcOtHArpe5djhF+GcBJpdQfZj2mhPg6gC+LyBCAzwD4HxmPJ0lOArBrPXwabfEuJcuP1V8EcDTjoSTJxwFcWv75QwAfy3AsSfJrAP6tiKxEO0xywu/NAy3cA8QfAvhHAF4E8LxSqrShIaXUnwP4SxH57wDOKaVeynpMCfJZAD9XSl0KfGdx+Q6Afyoif452+KCsYaEfAlgF4L8C+FdKqff93syVk4QQUjA44yaEkIJB4SaEkIJB4SaEkIJB4SaEkIJB4SaEkIJB4SaEkILx/wG+Zj26Zv0RUwAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.scatter(dataframe['RM'], dataframe['price'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## f(x) = k*X_rm + b,旨在找到最好的f(x)\n",
    "## 问题：什么叫做好？\n",
    "## Loss函数\n",
    "## 今天课程的loss函数叫做Mean Squared Error:MSE\n",
    "$$loss(y,\\hat{y}) = \\frac{1}{N}{\\sum_{i \\in N}(y_i-\\hat{y_i})^2}$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "def loss(y_hat, y):\n",
    "    return np.mean((np.array(y_hat) - np.array(y)) ** 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "real_y = [3,6,7]\n",
    "y_hats_1 = [3,4,7]\n",
    "y_hats_2 = [3,6,6]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1.3333333333333333"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "loss(y_hats_1, real_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.3333333333333333"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "loss(y_hats_2, real_y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 问题：有了判断标准，怎样获得最优的k和b呢？\n",
    "+ 1.直接用微积分的方法做计算"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$ f(rm) = k * rm + b $$ \n",
    "$$ Loss(k, b) = \\frac{1}{n} \\sum_{i \\in N} (\\hat{y_i} - y_i) ^ 2 $$\n",
    "$$ Loss(k, b) = \\frac{1}{n} \\sum_{i \\in N} ((k * rm_i + b) - y_i) ^ 2 $$\n",
    "loss函数可转化为：$Loss(k, b)=Ak^2+Bk+C+A'b^2+B'b+C'$,极值为：$-\\frac{A}{2B},-\\frac{A'}{2B'}$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "但是函数可能会很复杂，当函数变得极其复杂的时候，用微积分是求不到极值点的\n",
    "+ 2.随机模拟"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "在第0步，我们获得了函数 f(rm) = -60 * rm + -69, 此时loss是: 221992.4172750988\n",
      "在第1步，我们获得了函数 f(rm) = -54 * rm + -33, 此时loss是: 157953.88124318578\n",
      "在第2步，我们获得了函数 f(rm) = 18 * rm + 48, 此时loss是: 19289.96636052964\n",
      "在第3步，我们获得了函数 f(rm) = 14 * rm + 13, 此时loss是: 6210.148070379447\n",
      "在第26步，我们获得了函数 f(rm) = 4 * rm + -34, 此时loss是: 1042.026267098814\n",
      "在第59步，我们获得了函数 f(rm) = 14 * rm + -75, 此时loss是: 146.5828529881423\n",
      "在第152步，我们获得了函数 f(rm) = 13 * rm + -66, 此时loss是: 97.7702097687747\n",
      "在第245步，我们获得了函数 f(rm) = 11 * rm + -41, 此时loss是: 76.7147642035573\n",
      "在第373步，我们获得了函数 f(rm) = 13 * rm + -57, 此时loss是: 55.784142575098805\n",
      "在第512步，我们获得了函数 f(rm) = 14 * rm + -65, 此时loss是: 55.62435496442688\n"
     ]
    }
   ],
   "source": [
    "min_loss = float('inf') # inf无穷大\n",
    "best_k, bes_b = None, None\n",
    "\n",
    "for step in range(1000):\n",
    "    min_v, max_v = -100, 100\n",
    "    k, b = random.randrange(min_v, max_v), random.randrange(min_v, max_v)\n",
    "    y_hats = [k * rm_i  + b for rm_i in X_rm]\n",
    "    current_loss = loss(y_hats, Y)\n",
    "    \n",
    "    if current_loss < min_loss:\n",
    "        min_loss = current_loss\n",
    "        best_k, best_b = k, b\n",
    "        print('在第{}步，我们获得了函数 f(rm) = {} * rm + {}, 此时loss是: {}'.format(step, k, b, current_loss))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 我们可以看到，当刚开始的时候，更新会更加的频繁，随着时间的流逝，更新会越来越困难"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.collections.PathCollection at 0x7f267f71bbd0>"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.scatter(X_rm, Y)\n",
    "plt.scatter(X_rm, [best_k * rm + best_b for rm in X_rm])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## How to get better k and b?\n",
    "+ 3.梯度下降 Gradient Descent\n",
    "$$ k_{n+1} = k_{n} + (-1)*\\frac{\\partial{loss(k,b)}}{\\partial{k_{n}}}\\alpha $$\n",
    "$$ b_{n+1} = b_{n} + (-1)*\\frac{\\partial{loss(k,b)}}{\\partial{b_{n}}}\\alpha $$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 其中\n",
    "$$ Loss(k, b) = \\frac{1}{n} \\sum_{i \\in N} ((k * rm_i + b) - y_i) ^ 2 $$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$ \\frac{\\partial{loss(k, b)}}{\\partial{k}} = \\frac{2}{n}\\sum_{i \\in N}(k * rm_i + b - y_i) * rm_i $$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$ \\frac{\\partial{loss(k, b)}}{\\partial{b}} = \\frac{2}{n}\\sum_{i \\in N}(k * rm_i + b - y_i)$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 深度学习的核心：通过梯度下降的方法，获得一组参数，使得loss最小"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "在第0步，我们获得了函数 f(rm) = 2.978495837236008 * rm + 1.3064103702714776, 此时loss是: 68.36420260486207\n",
      "在第1步，我们获得了函数 f(rm) = 3.3540291698622893 * rm + 1.3565631421227105, 此时loss是: 59.88889539678889\n",
      "在第2步，我们获得了函数 f(rm) = 3.4229121790752886 * rm + 1.35851106462052, 此时loss是: 59.60546348905175\n",
      "在第3步，我们获得了函数 f(rm) = 3.436458648378781 * rm + 1.3517619381002879, 此时loss是: 59.588792431572216\n",
      "在第4步，我们获得了函数 f(rm) = 3.440019150371641 * rm + 1.3434451019742215, 此时loss是: 59.580811656630495\n",
      "在第5步，我们获得了函数 f(rm) = 3.441777376452194 * rm + 1.3348470735054652, 此时loss是: 59.57311744570157\n",
      "在第6步，我们获得了函数 f(rm) = 3.443210106371244 * rm + 1.3262000094443525, 此时loss是: 59.565436141409094\n",
      "在第7步，我们获得了函数 f(rm) = 3.4445838309773493 * rm + 1.317545802990121, 此时loss是: 59.55775883075386\n",
      "在第8步，我们获得了函数 f(rm) = 3.445946639669346 * rm + 1.3088920135270086, 此时loss是: 59.55008522177779\n",
      "在第9步，我们获得了函数 f(rm) = 3.447307210435152 * rm + 1.3002400047657763, 此时loss是: 59.54241530325613\n",
      "在第10步，我们获得了函数 f(rm) = 3.4486671093166987 * rm + 1.2915900223833443, 此时loss是: 59.53474907310663\n",
      "在第11步，我们获得了函数 f(rm) = 3.450026618976943 * rm + 1.2829421103030754, 此时loss是: 59.52708652954559\n",
      "在第12步，我们获得了函数 f(rm) = 3.4513857904879814 * rm + 1.2742962760411976, 此时loss是: 59.51942767079986\n",
      "在第13步，我们获得了函数 f(rm) = 3.4527446331304033 * rm + 1.2656525205442257, 此时loss是: 59.51177249509741\n",
      "在第14步，我们获得了函数 f(rm) = 3.454103148643337 * rm + 1.2570108435732419, 此时loss是: 59.50412100066717\n",
      "在第15步，我们获得了函数 f(rm) = 3.455461337405011 * rm + 1.248371244675511, 此时loss是: 59.49647318573882\n",
      "在第16步，我们获得了函数 f(rm) = 3.4568191995480566 * rm + 1.2397337233598122, 此时loss是: 59.48882904854297\n",
      "在第17步，我们获得了函数 f(rm) = 3.4581767351607704 * rm + 1.231098279128078, 此时loss是: 59.48118858731102\n",
      "在第18步，我们获得了函数 f(rm) = 3.459533944323436 * rm + 1.2224649114811044, 此时loss是: 59.47355180027527\n",
      "在第19步，我们获得了函数 f(rm) = 3.4608908271148713 * rm + 1.2138336199195798, 此时loss是: 59.46591868566883\n",
      "在第20步，我们获得了函数 f(rm) = 3.462247383613618 * rm + 1.2052044039442724, 此时loss是: 59.45828924172569\n",
      "在第21步，我们获得了函数 f(rm) = 3.46360361389815 * rm + 1.1965772630560632, 此时loss是: 59.45066346668067\n",
      "在第22步，我们获得了函数 f(rm) = 3.4649595180469155 * rm + 1.1879521967559512, 此时loss是: 59.44304135876944\n",
      "在第23步，我们获得了函数 f(rm) = 3.4663150961383415 * rm + 1.1793292045450559, 此时loss是: 59.435422916228546\n",
      "在第24步，我们获得了函数 f(rm) = 3.4676703482508353 * rm + 1.170708285924616, 此时loss是: 59.427808137295344\n",
      "在第25步，我们获得了函数 f(rm) = 3.4690252744627856 * rm + 1.162089440395991, 此时loss是: 59.42019702020805\n",
      "在第26步，我们获得了函数 f(rm) = 3.4703798748525627 * rm + 1.1534726674606592, 此时loss是: 59.41258956320574\n",
      "在第27步，我们获得了函数 f(rm) = 3.471734149498518 * rm + 1.1448579666202199, 此时loss是: 59.404985764528334\n",
      "在第28步，我们获得了函数 f(rm) = 3.4730880984789843 * rm + 1.1362453373763917, 此时loss是: 59.397385622416564\n",
      "在第29步，我们获得了函数 f(rm) = 3.4744417218722736 * rm + 1.1276347792310129, 此时loss是: 59.389789135112075\n",
      "在第30步，我们获得了函数 f(rm) = 3.475795019756682 * rm + 1.1190262916860418, 此时loss是: 59.38219630085728\n",
      "在第31步，我们获得了函数 f(rm) = 3.477147992210485 * rm + 1.1104198742435565, 此时loss是: 59.374607117895515\n",
      "在第32步，我们获得了函数 f(rm) = 3.478500639311939 * rm + 1.1018155264057545, 此时loss是: 59.367021584470876\n",
      "在第33步，我们获得了函数 f(rm) = 3.479852961139283 * rm + 1.0932132476749536, 此时loss是: 59.35943969882839\n",
      "在第34步，我们获得了函数 f(rm) = 3.481204957770736 * rm + 1.084613037553591, 此时loss是: 59.35186145921388\n",
      "在第35步，我们获得了函数 f(rm) = 3.482556629284499 * rm + 1.0760148955442232, 此时loss是: 59.344286863874\n",
      "在第36步，我们获得了函数 f(rm) = 3.483907975758753 * rm + 1.067418821149527, 此时loss是: 59.33671591105628\n",
      "在第37步，我们获得了函数 f(rm) = 3.485258997271662 * rm + 1.0588248138722982, 此时loss是: 59.32914859900908\n",
      "在第38步，我们获得了函数 f(rm) = 3.4866096939013693 * rm + 1.0502328732154524, 此时loss是: 59.321584925981604\n",
      "在第39步，我们获得了函数 f(rm) = 3.487960065726001 * rm + 1.0416429986820248, 此时loss是: 59.314024890223884\n",
      "在第40步，我们获得了函数 f(rm) = 3.489310112823663 * rm + 1.0330551897751699, 此时loss是: 59.30646848998681\n",
      "在第41步，我们获得了函数 f(rm) = 3.490659835272444 * rm + 1.0244694459981618, 此时loss是: 59.29891572352213\n",
      "在第42步，我们获得了函数 f(rm) = 3.4920092331504122 * rm + 1.0158857668543941, 此时loss是: 59.29136658908237\n",
      "在第43步，我们获得了函数 f(rm) = 3.493358306535618 * rm + 1.0073041518473798, 此时loss是: 59.28382108492098\n",
      "在第44步，我们获得了函数 f(rm) = 3.4947070555060935 * rm + 0.9987246004807512, 此时loss是: 59.27627920929216\n",
      "在第45步，我们获得了函数 f(rm) = 3.496055480139851 * rm + 0.99014711225826, 此时loss是: 59.268740960451055\n",
      "在第46步，我们获得了函数 f(rm) = 3.497403580514885 * rm + 0.9815716866837775, 此时loss是: 59.26120633665355\n",
      "在第47步，我们获得了函数 f(rm) = 3.4987513567091697 * rm + 0.9729983232612938, 此时loss是: 59.25367533615642\n",
      "在第48步，我们获得了函数 f(rm) = 3.5000988088006624 * rm + 0.9644270214949185, 此时loss是: 59.246147957217275\n",
      "在第49步，我们获得了函数 f(rm) = 3.5014459368673005 * rm + 0.9558577808888807, 此时loss是: 59.23862419809453\n",
      "在第50步，我们获得了函数 f(rm) = 3.5027927409870037 * rm + 0.9472906009475285, 此时loss是: 59.231104057047496\n",
      "在第51步，我们获得了函数 f(rm) = 3.504139221237671 * rm + 0.9387254811753291, 此时loss是: 59.223587532336275\n",
      "在第52步，我们获得了函数 f(rm) = 3.505485377697185 * rm + 0.9301624210768689, 此时loss是: 59.21607462222181\n",
      "在第53步，我们获得了函数 f(rm) = 3.5068312104434076 * rm + 0.9216014201568536, 此时loss是: 59.20856532496592\n",
      "在第54步，我们获得了函数 f(rm) = 3.508176719554184 * rm + 0.913042477920108, 此时loss是: 59.20105963883119\n",
      "在第55步，我们获得了函数 f(rm) = 3.5095219051073383 * rm + 0.9044855938715756, 此时loss是: 59.19355756208111\n",
      "在第56步，我们获得了函数 f(rm) = 3.5108667671806777 * rm + 0.8959307675163194, 此时loss是: 59.186059092979946\n",
      "在第57步，我们获得了函数 f(rm) = 3.5122113058519906 * rm + 0.8873779983595212, 此时loss是: 59.178564229792855\n",
      "在第58步，我们获得了函数 f(rm) = 3.5135555211990455 * rm + 0.878827285906482, 此时loss是: 59.17107297078579\n",
      "在第59步，我们获得了函数 f(rm) = 3.5148994132995934 * rm + 0.8702786296626213, 此时loss是: 59.16358531422556\n",
      "在第60步，我们获得了函数 f(rm) = 3.5162429822313657 * rm + 0.8617320291334779, 此时loss是: 59.15610125837977\n",
      "在第61步，我们获得了函数 f(rm) = 3.517586228072076 * rm + 0.8531874838247095, 此时loss是: 59.148620801516905\n",
      "在第62步，我们获得了函数 f(rm) = 3.5189291508994183 * rm + 0.8446449932420927, 此时loss是: 59.14114394190625\n",
      "在第63步，我们获得了函数 f(rm) = 3.5202717507910686 * rm + 0.8361045568915227, 此时loss是: 59.133670677817946\n",
      "在第64步，我们获得了函数 f(rm) = 3.5216140278246835 * rm + 0.8275661742790137, 此时loss是: 59.12620100752296\n",
      "在第65步，我们获得了函数 f(rm) = 3.522955982077902 * rm + 0.8190298449106986, 此时loss是: 59.118734929293055\n",
      "在第66步，我们获得了函数 f(rm) = 3.524297613628344 * rm + 0.8104955682928293, 此时loss是: 59.11127244140089\n",
      "在第67步，我们获得了函数 f(rm) = 3.5256389225536098 * rm + 0.8019633439317762, 此时loss是: 59.103813542119894\n",
      "在第68步，我们获得了函数 f(rm) = 3.526979908931282 * rm + 0.7934331713340282, 此时loss是: 59.096358229724366\n",
      "在第69步，我们获得了函数 f(rm) = 3.528320572838925 * rm + 0.7849050500061935, 此时loss是: 59.08890650248941\n",
      "在第70步，我们获得了函数 f(rm) = 3.5296609143540834 * rm + 0.7763789794549983, 此时loss是: 59.08145835869098\n",
      "在第71步，我们获得了函数 f(rm) = 3.5310009335542833 * rm + 0.7678549591872879, 此时loss是: 59.07401379660584\n",
      "在第72步，我们获得了函数 f(rm) = 3.5323406305170333 * rm + 0.7593329887100259, 此时loss是: 59.06657281451159\n",
      "在第73步，我们获得了函数 f(rm) = 3.5336800053198227 * rm + 0.7508130675302945, 此时loss是: 59.05913541068668\n",
      "在第74步，我们获得了函数 f(rm) = 3.535019058040121 * rm + 0.7422951951552946, 此时loss是: 59.051701583410335\n",
      "在第75步，我们获得了函数 f(rm) = 3.5363577887553803 * rm + 0.7337793710923455, 此时loss是: 59.04427133096268\n",
      "在第76步，我们获得了函数 f(rm) = 3.5376961975430343 * rm + 0.7252655948488848, 此时loss是: 59.03684465162459\n",
      "在第77步，我们获得了函数 f(rm) = 3.539034284480498 * rm + 0.7167538659324688, 此时loss是: 59.02942154367782\n",
      "在第78步，我们获得了函数 f(rm) = 3.5403720496451667 * rm + 0.7082441838507723, 此时loss是: 59.02200200540494\n",
      "在第79步，我们获得了函数 f(rm) = 3.541709493114418 * rm + 0.6997365481115879, 此时loss是: 59.01458603508935\n",
      "在第80步，我们获得了函数 f(rm) = 3.5430466149656112 * rm + 0.6912309582228274, 此时loss是: 59.00717363101524\n",
      "在第81步，我们获得了函数 f(rm) = 3.544383415276086 * rm + 0.6827274136925203, 此时loss是: 58.99976479146767\n",
      "在第82步，我们获得了函数 f(rm) = 3.545719894123164 * rm + 0.6742259140288147, 此时loss是: 58.9923595147325\n",
      "在第83步，我们获得了函数 f(rm) = 3.547056051584149 * rm + 0.665726458739977, 此时loss是: 58.98495779909642\n",
      "在第84步，我们获得了函数 f(rm) = 3.5483918877363245 * rm + 0.6572290473343916, 此时loss是: 58.97755964284695\n",
      "在第85步，我们获得了函数 f(rm) = 3.549727402656957 * rm + 0.6487336793205613, 此时loss是: 58.97016504427244\n",
      "在第86步，我们获得了函数 f(rm) = 3.5510625964232934 * rm + 0.640240354207107, 此时loss是: 58.96277400166201\n",
      "在第87步，我们获得了函数 f(rm) = 3.552397469112562 * rm + 0.6317490715027678, 此时loss是: 58.955386513305704\n",
      "在第88步，我们获得了函数 f(rm) = 3.553732020801974 * rm + 0.6232598307164012, 此时loss是: 58.94800257749428\n",
      "在第89步，我们获得了函数 f(rm) = 3.5550662515687206 * rm + 0.6147726313569825, 此时loss是: 58.940622192519385\n",
      "在第90步，我们获得了函数 f(rm) = 3.5564001614899743 * rm + 0.6062874729336049, 此时loss是: 58.93324535667346\n",
      "在第91步，我们获得了函数 f(rm) = 3.55773375064289 * rm + 0.59780435495548, 此时loss是: 58.92587206824978\n",
      "在第92步，我们获得了函数 f(rm) = 3.5590670191046034 * rm + 0.5893232769319374, 此时loss是: 58.91850232554246\n",
      "在第93步，我们获得了函数 f(rm) = 3.5603999669522315 * rm + 0.5808442383724245, 此时loss是: 58.91113612684639\n",
      "在第94步，我们获得了函数 f(rm) = 3.5617325942628737 * rm + 0.5723672387865069, 此时loss是: 58.90377347045729\n",
      "在第95步，我们获得了函数 f(rm) = 3.56306490111361 * rm + 0.5638922776838678, 此时loss是: 58.89641435467172\n",
      "在第96步，我们获得了函数 f(rm) = 3.5643968875815024 * rm + 0.5554193545743088, 此时loss是: 58.88905877778707\n",
      "在第97步，我们获得了函数 f(rm) = 3.5657285537435937 * rm + 0.5469484689677488, 此时loss是: 58.88170673810152\n",
      "在第98步，我们获得了函数 f(rm) = 3.5670598996769085 * rm + 0.538479620374225, 此时loss是: 58.874358233914066\n",
      "在第99步，我们获得了函数 f(rm) = 3.5683909254584534 * rm + 0.5300128083038923, 此时loss是: 58.86701326352455\n",
      "在第100步，我们获得了函数 f(rm) = 3.569721631165216 * rm + 0.5215480322670233, 此时loss是: 58.8596718252336\n",
      "在第101步，我们获得了函数 f(rm) = 3.5710520168741646 * rm + 0.5130852917740083, 此时loss是: 58.852333917342676\n",
      "在第102步，我们获得了函数 f(rm) = 3.5723820826622505 * rm + 0.5046245863353556, 此时loss是: 58.84499953815408\n",
      "在第103步，我们获得了函数 f(rm) = 3.5737118286064056 * rm + 0.496165915461691, 此时loss是: 58.837668685970876\n",
      "在第104步，我们获得了函数 f(rm) = 3.5750412547835437 * rm + 0.48770927866375813, 此时loss是: 58.830341359097005\n",
      "在第105步，我们获得了函数 f(rm) = 3.57637036127056 * rm + 0.47925467545241823, 此时loss是: 58.82301755583716\n",
      "在第106步，我们获得了函数 f(rm) = 3.5776991481443305 * rm + 0.47080210533865, 此时loss是: 58.81569727449691\n",
      "在第107步，我们获得了函数 f(rm) = 3.579027615481714 * rm + 0.46235156783354997, 此时loss是: 58.808380513382595\n",
      "在第108步，我们获得了函数 f(rm) = 3.5803557633595493 * rm + 0.453903062448332, 此时loss是: 58.80106727080139\n",
      "在第109步，我们获得了函数 f(rm) = 3.581683591854658 * rm + 0.44545658869432775, 此时loss是: 58.79375754506127\n",
      "在第110步，我们获得了函数 f(rm) = 3.5830111010438435 * rm + 0.4370121460829863, 此时loss是: 58.78645133447104\n",
      "在第111步，我们获得了函数 f(rm) = 3.5843382910038892 * rm + 0.42856973412587407, 此时loss是: 58.77914863734031\n",
      "在第112步，我们获得了函数 f(rm) = 3.585665161811561 * rm + 0.4201293523346751, 此时loss是: 58.77184945197952\n",
      "在第113步，我们获得了函数 f(rm) = 3.586991713543606 * rm + 0.41169100022119065, 此时loss是: 58.764553776699884\n",
      "在第114步，我们获得了函数 f(rm) = 3.588317946276754 * rm + 0.40325467729733977, 此时loss是: 58.75726160981347\n",
      "在第115步，我们获得了函数 f(rm) = 3.5896438600877136 * rm + 0.39482038307515843, 此时loss是: 58.74997294963311\n",
      "在第116步，我们获得了函数 f(rm) = 3.590969455053178 * rm + 0.38638811706680026, 此时loss是: 58.7426877944725\n",
      "在第117步，我们获得了函数 f(rm) = 3.592294731249821 * rm + 0.37795787878453607, 此时loss是: 58.73540614264611\n",
      "在第118步，我们获得了函数 f(rm) = 3.593619688754297 * rm + 0.36952966774075396, 此时loss是: 58.72812799246925\n",
      "在第119步，我们获得了函数 f(rm) = 3.5949443276432427 * rm + 0.36110348344795934, 此时loss是: 58.72085334225801\n",
      "在第120步，我们获得了函数 f(rm) = 3.596268647993276 * rm + 0.3526793254187747, 此时loss是: 58.7135821903293\n",
      "在第121步，我们获得了函数 f(rm) = 3.597592649880998 * rm + 0.34425719316594006, 此时loss是: 58.70631453500085\n",
      "在第122步，我们获得了函数 f(rm) = 3.5989163333829883 * rm + 0.3358370862023121, 此时loss是: 58.699050374591195\n",
      "在第123步，我们获得了函数 f(rm) = 3.600239698575811 * rm + 0.3274190040408652, 此时loss是: 58.691789707419666\n",
      "在第124步，我们获得了函数 f(rm) = 3.6015627455360097 * rm + 0.31900294619469044, 此时loss是: 58.684532531806404\n",
      "在第125步，我们获得了函数 f(rm) = 3.6028854743401117 * rm + 0.31058891217699625, 此时loss是: 58.677278846072376\n",
      "在第126步，我们获得了函数 f(rm) = 3.604207885064624 * rm + 0.30217690150110793, 此时loss是: 58.67002864853936\n",
      "在第127步，我们获得了函数 f(rm) = 3.605529977786036 * rm + 0.2937669136804679, 此时loss是: 58.6627819375299\n",
      "在第128步，我们获得了函数 f(rm) = 3.6068517525808184 * rm + 0.28535894822863567, 此时loss是: 58.6555387113674\n",
      "在第129步，我们获得了函数 f(rm) = 3.608173209525424 * rm + 0.2769530046592875, 此时loss是: 58.648298968376004\n",
      "在第130步，我们获得了函数 f(rm) = 3.6094943486962876 * rm + 0.26854908248621695, 此时loss是: 58.64106270688074\n",
      "在第131步，我们获得了函数 f(rm) = 3.6108151701698237 * rm + 0.260147181223334, 此时loss是: 58.6338299252074\n",
      "在第132步，我们获得了函数 f(rm) = 3.61213567402243 * rm + 0.25174730038466603, 此时loss是: 58.62660062168255\n",
      "在第133步，我们获得了函数 f(rm) = 3.6134558603304865 * rm + 0.24334943948435697, 此时loss是: 58.61937479463363\n",
      "在第134步，我们获得了函数 f(rm) = 3.614775729170353 * rm + 0.23495359803666763, 此时loss是: 58.612152442388826\n",
      "在第135步，我们获得了函数 f(rm) = 3.6160952806183717 * rm + 0.2265597755559757, 此时loss是: 58.604933563277164\n",
      "在第136步，我们获得了函数 f(rm) = 3.617414514750867 * rm + 0.21816797155677564, 此时loss是: 58.59771815562846\n",
      "在第137步，我们获得了函数 f(rm) = 3.6187334316441446 * rm + 0.20977818555367866, 此时loss是: 58.59050621777333\n",
      "在第138步，我们获得了函数 f(rm) = 3.6200520313744913 * rm + 0.20139041706141259, 此时loss是: 58.583297748043194\n",
      "在第139步，我们获得了函数 f(rm) = 3.6213703140181757 * rm + 0.19300466559482204, 此时loss是: 58.576092744770285\n",
      "在第140步，我们获得了函数 f(rm) = 3.622688279651449 * rm + 0.18462093066886837, 此时loss是: 58.56889120628763\n",
      "在第141步，我们获得了函数 f(rm) = 3.624005928350543 * rm + 0.17623921179862945, 此时loss是: 58.56169313092905\n",
      "在第142步，我们获得了函数 f(rm) = 3.6253232601916716 * rm + 0.1678595084992997, 此时loss是: 58.55449851702917\n",
      "在第143步，我们获得了函数 f(rm) = 3.626640275251031 * rm + 0.15948182028619035, 此时loss是: 58.54730736292343\n",
      "在第144步，我们获得了函数 f(rm) = 3.627956973604798 * rm + 0.15110614667472902, 此时loss是: 58.54011966694806\n",
      "在第145步，我们获得了函数 f(rm) = 3.629273355329131 * rm + 0.14273248718045986, 此时loss是: 58.532935427440094\n",
      "在第146步，我们获得了函数 f(rm) = 3.630589420500171 * rm + 0.13436084131904355, 此时loss是: 58.52575464273735\n",
      "在第147步，我们获得了函数 f(rm) = 3.6319051691940407 * rm + 0.12599120860625726, 此时loss是: 58.51857731117847\n",
      "在第148步，我们获得了函数 f(rm) = 3.6332206014868436 * rm + 0.11762358855799457, 此时loss是: 58.51140343110287\n",
      "在第149步，我们获得了函数 f(rm) = 3.6345357174546655 * rm + 0.10925798069026554, 此时loss是: 58.5042330008508\n",
      "在第150步，我们获得了函数 f(rm) = 3.6358505171735738 * rm + 0.10089438451919647, 此时loss是: 58.49706601876328\n",
      "在第151步，我们获得了函数 f(rm) = 3.6371650007196172 * rm + 0.09253279956103015, 此时loss是: 58.489902483182114\n",
      "在第152步，我们获得了函数 f(rm) = 3.638479168168827 * rm + 0.0841732253321257, 此时loss是: 58.482742392449936\n",
      "在第153步，我们获得了函数 f(rm) = 3.639793019597216 * rm + 0.07581566134895851, 此时loss是: 58.47558574491016\n",
      "在第154步，我们获得了函数 f(rm) = 3.641106555080778 * rm + 0.0674601071281202, 此时loss是: 58.46843253890701\n",
      "在第155步，我们获得了函数 f(rm) = 3.6424197746954885 * rm + 0.05910656218631863, 此时loss是: 58.461282772785495\n",
      "在第156步，我们获得了函数 f(rm) = 3.6437326785173063 * rm + 0.050755026040377994, 此时loss是: 58.45413644489141\n",
      "在第157步，我们获得了函数 f(rm) = 3.6450452666221698 * rm + 0.04240549820723855, 此时loss是: 58.44699355357137\n",
      "在第158步，我们获得了函数 f(rm) = 3.646357539086001 * rm + 0.03405797820395683, 此时loss是: 58.43985409717277\n",
      "在第159步，我们获得了函数 f(rm) = 3.647669495984702 * rm + 0.02571246554770537, 此时loss是: 58.43271807404379\n",
      "在第160步，我们获得了函数 f(rm) = 3.648981137394158 * rm + 0.017368959755772944, 此时loss是: 58.42558548253343\n",
      "在第161步，我们获得了函数 f(rm) = 3.650292463390235 * rm + 0.009027460345564295, 此时loss是: 58.41845632099145\n",
      "在第162步，我们获得了函数 f(rm) = 3.6516034740487817 * rm + 0.0006879668346003696, 此时loss是: 58.411330587768454\n",
      "在第163步，我们获得了函数 f(rm) = 3.6529141694456277 * rm + -0.007649521259482045, 此时loss是: 58.4042082812158\n",
      "在第164步，我们获得了函数 f(rm) = 3.6542245496565853 * rm + -0.015985004418930075, 此时loss是: 58.39708939968563\n",
      "在第165步，我们获得了函数 f(rm) = 3.655534614757447 * rm + -0.0243184831258749, 此时loss是: 58.38997394153091\n",
      "在第166步，我们获得了函数 f(rm) = 3.656844364823989 * rm + -0.032649957862331734, 此时loss是: 58.38286190510539\n",
      "在第167步，我们获得了函数 f(rm) = 3.6581537999319673 * rm + -0.040979429110199896, 此时loss是: 58.3757532887636\n",
      "在第168步，我们获得了函数 f(rm) = 3.6594629201571207 * rm + -0.0493068973512629, 此时loss是: 58.36864809086086\n",
      "在第169步，我们获得了函数 f(rm) = 3.660771725575171 * rm + -0.057632363067188185, 此时loss是: 58.36154630975329\n",
      "在第170步，我们获得了函数 f(rm) = 3.66208021626182 * rm + -0.06595582673952755, 此时loss是: 58.35444794379781\n",
      "在第171步，我们获得了函数 f(rm) = 3.6633883922927515 * rm + -0.07427728884971696, 此时loss是: 58.34735299135211\n",
      "在第172步，我们获得了函数 f(rm) = 3.6646962537436316 * rm + -0.08259674987907663, 此时loss是: 58.34026145077467\n",
      "在第173步，我们获得了函数 f(rm) = 3.6660038006901083 * rm + -0.09091421030881099, 此时loss是: 58.33317332042479\n",
      "在第174步，我们获得了函数 f(rm) = 3.667311033207811 * rm + -0.09922967062000876, 此时loss是: 58.32608859866251\n",
      "在第175步，我们获得了函数 f(rm) = 3.668617951372351 * rm + -0.10754313129364293, 此时loss是: 58.31900728384871\n",
      "在第176步，我们获得了函数 f(rm) = 3.6699245552593216 * rm + -0.11585459281057088, 此时loss是: 58.31192937434503\n",
      "在第177步，我们获得了函数 f(rm) = 3.6712308449442985 * rm + -0.12416405565153432, 此时loss是: 58.30485486851389\n",
      "在第178步，我们获得了函数 f(rm) = 3.6725368205028377 * rm + -0.1324715202971595, 此时loss是: 58.29778376471851\n",
      "在第179步，我们获得了函数 f(rm) = 3.6738424820104782 * rm + -0.1407769872279569, 此时loss是: 58.29071606132291\n",
      "在第180步，我们获得了函数 f(rm) = 3.67514782954274 * rm + -0.14908045692432154, 此时loss是: 58.28365175669186\n",
      "在第181步，我们获得了函数 f(rm) = 3.6764528631751268 * rm + -0.15738192986653285, 此时loss是: 58.27659084919097\n",
      "在第182步，我们获得了函数 f(rm) = 3.6777575829831215 * rm + -0.16568140653475488, 此时loss是: 58.26953333718658\n",
      "在第183步，我们获得了函数 f(rm) = 3.679061989042191 * rm + -0.1739788874090361, 此时loss是: 58.262479219045844\n",
      "在第184步，我们获得了函数 f(rm) = 3.6803660814277825 * rm + -0.1822743729693097, 此时loss是: 58.25542849313671\n",
      "在第185步，我们获得了函数 f(rm) = 3.6816698602153264 * rm + -0.19056786369539322, 此时loss是: 58.248381157827886\n",
      "在第186步，我们获得了函数 f(rm) = 3.682973325480234 * rm + -0.19885936006698907, 此时loss是: 58.241337211488904\n",
      "在第187步，我们获得了函数 f(rm) = 3.684276477297899 * rm + -0.2071488625636841, 此时loss是: 58.23429665249001\n",
      "在第188步，我们获得了函数 f(rm) = 3.6855793157436967 * rm + -0.21543637166494997, 此时loss是: 58.22725947920231\n",
      "在第189步，我们获得了函数 f(rm) = 3.686881840892984 * rm + -0.22372188785014302, 此时loss是: 58.22022568999766\n",
      "在第190步，我们获得了函数 f(rm) = 3.6881840528211005 * rm + -0.23200541159850427, 此时loss是: 58.21319528324868\n",
      "在第191步，我们获得了函数 f(rm) = 3.6894859516033667 * rm + -0.24028694338915949, 此时loss是: 58.206168257328805\n",
      "在第192步，我们获得了函数 f(rm) = 3.690787537315086 * rm + -0.24856648370111925, 此时loss是: 58.19914461061224\n",
      "在第193步，我们获得了函数 f(rm) = 3.692088810031543 * rm + -0.25684403301327896, 此时loss是: 58.192124341473956\n",
      "在第194步，我们获得了函数 f(rm) = 3.6933897698280047 * rm + -0.2651195918044188, 此时loss是: 58.18510744828974\n",
      "在第195步，我们获得了函数 f(rm) = 3.6946904167797188 * rm + -0.273393160553204, 此时loss是: 58.17809392943613\n",
      "在第196步，我们获得了函数 f(rm) = 3.6959907509619168 * rm + -0.2816647397381844, 此时loss是: 58.17108378329048\n",
      "在第197步，我们获得了函数 f(rm) = 3.69729077244981 * rm + -0.28993432983779505, 此时loss是: 58.16407700823085\n",
      "在第198步，我们获得了函数 f(rm) = 3.698590481318594 * rm + -0.29820193133035566, 此时loss是: 58.15707360263617\n",
      "在第199步，我们获得了函数 f(rm) = 3.699889877643444 * rm + -0.30646754469407117, 此时loss是: 58.15007356488609\n",
      "在第200步，我们获得了函数 f(rm) = 3.701188961499519 * rm + -0.3147311704070313, 此时loss是: 58.14307689336107\n",
      "在第201步，我们获得了函数 f(rm) = 3.702487732961958 * rm + -0.3229928089472111, 此时loss是: 58.13608358644233\n",
      "在第202步，我们获得了函数 f(rm) = 3.7037861921058837 * rm + -0.3312524607924704, 此时loss是: 58.12909364251188\n",
      "在第203步，我们获得了函数 f(rm) = 3.7050843390063997 * rm + -0.3395101264205543, 此时loss是: 58.1221070599525\n",
      "在第204步，我们获得了函数 f(rm) = 3.706382173738592 * rm + -0.3477658063090929, 此时loss是: 58.115123837147756\n",
      "在第205步，我们获得了函数 f(rm) = 3.707679696377529 * rm + -0.3560195009356014, 此时loss是: 58.108143972481976\n",
      "在第206步，我们获得了函数 f(rm) = 3.7089769069982603 * rm + -0.36427121077748037, 此时loss是: 58.10116746434029\n",
      "在第207步，我们获得了函数 f(rm) = 3.7102738056758167 * rm + -0.37252093631201544, 此时loss是: 58.09419431110858\n",
      "在第208步，我们获得了函数 f(rm) = 3.7115703924852124 * rm + -0.3807686780163775, 此时loss是: 58.08722451117353\n",
      "在第209步，我们获得了函数 f(rm) = 3.7128666675014435 * rm + -0.38901443636762256, 此时loss是: 58.080258062922546\n",
      "在第210步，我们获得了函数 f(rm) = 3.7141626307994877 * rm + -0.3972582118426921, 此时loss是: 58.073294964743894\n",
      "在第211步，我们获得了函数 f(rm) = 3.7154582824543034 * rm + -0.405500004918413, 此时loss是: 58.06633521502654\n",
      "在第212步，我们获得了函数 f(rm) = 3.716753622540833 * rm + -0.4137398160714971, 此时loss是: 58.05937881216027\n",
      "在第213步，我们获得了函数 f(rm) = 3.718048651134 * rm + -0.42197764577854197, 此时loss是: 58.05242575453561\n",
      "在第214步，我们获得了函数 f(rm) = 3.71934336830871 * rm + -0.4302134945160303, 此时loss是: 58.045476040543896\n",
      "在第215步，我们获得了函数 f(rm) = 3.72063777413985 * rm + -0.4384473627603304, 此时loss是: 58.038529668577205\n",
      "在第216步，我们获得了函数 f(rm) = 3.72193186870229 * rm + -0.44667925098769595, 此时loss是: 58.03158663702842\n",
      "在第217步，我们获得了函数 f(rm) = 3.723225652070881 * rm + -0.45490915967426615, 此时loss是: 58.02464694429115\n",
      "在第218步，我们获得了函数 f(rm) = 3.724519124320457 * rm + -0.46313708929606556, 此时loss是: 58.01771058875982\n",
      "在第219步，我们获得了函数 f(rm) = 3.7258122855258335 * rm + -0.4713630403290044, 此时loss是: 58.010777568829624\n",
      "在第220步，我们获得了函数 f(rm) = 3.727105135761808 * rm + -0.47958701324887837, 此时loss是: 58.0038478828965\n",
      "在第221步，我们获得了函数 f(rm) = 3.7283976751031593 * rm + -0.48780900853136894, 此时loss是: 57.996921529357174\n",
      "在第222步，我们获得了函数 f(rm) = 3.7296899036246494 * rm + -0.49602902665204296, 此时loss是: 57.989998506609155\n",
      "在第223步，我们获得了函数 f(rm) = 3.730981821401022 * rm + -0.5042470680863529, 此时loss是: 57.98307881305069\n",
      "在第224步，我们获得了函数 f(rm) = 3.732273428507003 * rm + -0.5124631333096371, 此时loss是: 57.97616244708083\n",
      "在第225步，我们获得了函数 f(rm) = 3.7335647250172994 * rm + -0.5206772227971196, 此时loss是: 57.96924940709939\n",
      "在第226步，我们获得了函数 f(rm) = 3.7348557110066007 * rm + -0.52888933702391, 此时loss是: 57.96233969150692\n",
      "在第227步，我们获得了函数 f(rm) = 3.736146386549579 * rm + -0.5370994764650036, 此时loss是: 57.95543329870479\n",
      "在第228步，我们获得了函数 f(rm) = 3.7374367517208884 * rm + -0.5453076415952816, 此时loss是: 57.9485302270951\n",
      "在第229步，我们获得了函数 f(rm) = 3.738726806595164 * rm + -0.553513832889511, 此时loss是: 57.941630475080736\n",
      "在第230步，我们获得了函数 f(rm) = 3.740016551247024 * rm + -0.5617180508223447, 此时loss是: 57.934734041065354\n",
      "在第231步，我们获得了函数 f(rm) = 3.741305985751068 * rm + -0.5699202958683214, 此时loss是: 57.92784092345337\n",
      "在第232步，我们获得了函数 f(rm) = 3.742595110181878 * rm + -0.5781205685018657, 此时loss是: 57.92095112064997\n",
      "在第233步，我们获得了函数 f(rm) = 3.743883924614018 * rm + -0.5863188691972879, 此时loss是: 57.914064631061095\n",
      "在第234步，我们获得了函数 f(rm) = 3.7451724291220345 * rm + -0.5945151984287845, 此时loss是: 57.90718145309349\n",
      "在第235步，我们获得了函数 f(rm) = 3.746460623780455 * rm + -0.602709556670438, 此时loss是: 57.900301585154615\n",
      "在第236步，我们获得了函数 f(rm) = 3.7477485086637903 * rm + -0.6109019443962167, 此时loss是: 57.893425025652725\n",
      "在第237步，我们获得了函数 f(rm) = 3.749036083846533 * rm + -0.619092362079975, 此时loss是: 57.88655177299685\n",
      "在第238步，我们获得了函数 f(rm) = 3.7503233494031565 * rm + -0.6272808101954533, 此时loss是: 57.87968182559677\n",
      "在第239步，我们获得了函数 f(rm) = 3.751610305408118 * rm + -0.6354672892162784, 此时loss是: 57.872815181863025\n",
      "在第240步，我们获得了函数 f(rm) = 3.7528969519358557 * rm + -0.6436517996159626, 此时loss是: 57.86595184020692\n",
      "在第241步，我们获得了函数 f(rm) = 3.7541832890607902 * rm + -0.651834341867905, 此时loss是: 57.85909179904054\n",
      "在第242步，我们获得了函数 f(rm) = 3.7554693168573245 * rm + -0.6600149164453903, 此时loss是: 57.85223505677672\n",
      "在第243步，我们获得了函数 f(rm) = 3.7567550353998445 * rm + -0.6681935238215898, 此时loss是: 57.84538161182906\n",
      "在第244步，我们获得了函数 f(rm) = 3.7580404447627154 * rm + -0.6763701644695608, 此时loss是: 57.838531462611925\n",
      "在第245步，我们获得了函数 f(rm) = 3.7593255450202876 * rm + -0.6845448388622468, 此时loss是: 57.83168460754044\n",
      "在第246步，我们获得了函数 f(rm) = 3.760610336246892 * rm + -0.6927175474724777, 此时loss是: 57.824841045030496\n",
      "在第247步，我们获得了函数 f(rm) = 3.7618948185168413 * rm + -0.7008882907729698, 此时loss是: 57.81800077349875\n",
      "在第248步，我们获得了函数 f(rm) = 3.7631789919044323 * rm + -0.7090570692363253, 此时loss是: 57.81116379136262\n",
      "在第249步，我们获得了函数 f(rm) = 3.764462856483942 * rm + -0.7172238833350332, 此时loss是: 57.80433009704025\n",
      "在第250步，我们获得了函数 f(rm) = 3.76574641232963 * rm + -0.7253887335414687, 此时loss是: 57.7974996889506\n",
      "在第251步，我们获得了函数 f(rm) = 3.767029659515738 * rm + -0.7335516203278936, 此时loss是: 57.79067256551336\n",
      "在第252步，我们获得了函数 f(rm) = 3.768312598116491 * rm + -0.7417125441664556, 此时loss是: 57.783848725148985\n",
      "在第253步，我们获得了函数 f(rm) = 3.7695952282060947 * rm + -0.7498715055291895, 此时loss是: 57.777028166278676\n",
      "在第254步，我们获得了函数 f(rm) = 3.770877549858737 * rm + -0.7580285048880162, 此时loss是: 57.77021088732443\n",
      "在第255步，我们获得了函数 f(rm) = 3.7721595631485894 * rm + -0.7661835427147432, 此时loss是: 57.763396886708954\n",
      "在第256步，我们获得了函数 f(rm) = 3.7734412681498033 * rm + -0.7743366194810647, 此时loss是: 57.75658616285575\n",
      "在第257步，我们获得了函数 f(rm) = 3.7747226649365153 * rm + -0.7824877356585611, 此时loss是: 57.74977871418907\n",
      "在第258步，我们获得了函数 f(rm) = 3.776003753582841 * rm + -0.7906368917186999, 此时loss是: 57.742974539133904\n",
      "在第259步，我们获得了函数 f(rm) = 3.77728453416288 * rm + -0.7987840881328347, 此时loss是: 57.73617363611604\n",
      "在第260步，我们获得了函数 f(rm) = 3.7785650067507137 * rm + -0.8069293253722062, 此时loss是: 57.72937600356198\n",
      "在第261步，我们获得了函数 f(rm) = 3.7798451714204058 * rm + -0.8150726039079415, 此时loss是: 57.72258163989902\n",
      "在第262步，我们获得了函数 f(rm) = 3.7811250282460027 * rm + -0.8232139242110543, 此时loss是: 57.715790543555165\n",
      "在第263步，我们获得了函数 f(rm) = 3.782404577301531 * rm + -0.8313532867524457, 此时loss是: 57.709002712959226\n",
      "在第264步，我们获得了函数 f(rm) = 3.783683818661002 * rm + -0.8394906920029027, 此时loss是: 57.70221814654075\n",
      "在第265步，我们获得了函数 f(rm) = 3.7849627523984073 * rm + -0.8476261404330997, 此时loss是: 57.69543684273003\n",
      "在第266步，我们获得了函数 f(rm) = 3.786241378587722 * rm + -0.8557596325135975, 此时loss是: 57.68865879995812\n",
      "在第267步，我们获得了函数 f(rm) = 3.7875196973029026 * rm + -0.8638911687148443, 此时loss是: 57.68188401665683\n",
      "在第268步，我们获得了函数 f(rm) = 3.788797708617888 * rm + -0.8720207495071745, 此时loss是: 57.67511249125874\n",
      "在第269步，我们获得了函数 f(rm) = 3.7900754126066003 * rm + -0.8801483753608096, 此时loss是: 57.66834422219714\n",
      "在第270步，我们获得了函数 f(rm) = 3.7913528093429423 * rm + -0.8882740467458583, 此时loss是: 57.66157920790612\n",
      "在第271步，我们获得了函数 f(rm) = 3.792629898900799 * rm + -0.8963977641323162, 此时loss是: 57.6548174468205\n",
      "在第272步，我们获得了函数 f(rm) = 3.793906681354039 * rm + -0.9045195279900656, 此时loss是: 57.64805893737586\n",
      "在第273步，我们获得了函数 f(rm) = 3.7951831567765124 * rm + -0.9126393387888762, 此时loss是: 57.64130367800852\n",
      "在第274步，我们获得了函数 f(rm) = 3.796459325242052 * rm + -0.9207571969984042, 此时loss是: 57.634551667155556\n",
      "在第275步，我们获得了函数 f(rm) = 3.7977351868244718 * rm + -0.9288731030881933, 此时loss是: 57.627802903254825\n",
      "在第276步，我们获得了函数 f(rm) = 3.799010741597569 * rm + -0.9369870575276741, 此时loss是: 57.621057384744894\n",
      "在第277步，我们获得了函数 f(rm) = 3.8002859896351224 * rm + -0.9450990607861645, 此时loss是: 57.61431511006511\n",
      "在第278步，我们获得了函数 f(rm) = 3.8015609310108935 * rm + -0.9532091133328693, 此时loss是: 57.60757607765553\n",
      "在第279步，我们获得了函数 f(rm) = 3.802835565798626 * rm + -0.9613172156368806, 此时loss是: 57.60084028595702\n",
      "在第280步，我们获得了函数 f(rm) = 3.8041098940720466 * rm + -0.9694233681671776, 此时loss是: 57.59410773341115\n",
      "在第281步，我们获得了函数 f(rm) = 3.805383915904862 * rm + -0.9775275713926269, 此时loss是: 57.587378418460254\n",
      "在第282步，我们获得了函数 f(rm) = 3.806657631370764 * rm + -0.9856298257819822, 此时loss是: 57.58065233954743\n",
      "在第283步，我们获得了函数 f(rm) = 3.8079310405434246 * rm + -0.9937301318038847, 此时loss是: 57.573929495116474\n",
      "在第284步，我们获得了函数 f(rm) = 3.8092041434964994 * rm + -1.0018284899268624, 此时loss是: 57.56720988361202\n",
      "在第285步，我们获得了函数 f(rm) = 3.8104769403036247 * rm + -1.0099249006193314, 此时loss是: 57.56049350347937\n",
      "在第286步，我们获得了函数 f(rm) = 3.811749431038421 * rm + -1.0180193643495943, 此时loss是: 57.553780353164576\n",
      "在第287步，我们获得了函数 f(rm) = 3.8130216157744896 * rm + -1.0261118815858417, 此时loss是: 57.5470704311145\n",
      "在第288步，我们获得了函数 f(rm) = 3.8142934945854154 * rm + -1.0342024527961513, 此时loss是: 57.540363735776694\n",
      "在第289步，我们获得了函数 f(rm) = 3.8155650675447643 * rm + -1.0422910784484887, 此时loss是: 57.53366026559949\n",
      "在第290步，我们获得了函数 f(rm) = 3.816836334726086 * rm + -1.0503777590107062, 此时loss是: 57.526960019031925\n",
      "在第291步，我们获得了函数 f(rm) = 3.8181072962029106 * rm + -1.0584624949505443, 此时loss是: 57.520262994523826\n",
      "在第292步，我们获得了函数 f(rm) = 3.8193779520487516 * rm + -1.0665452867356306, 此时loss是: 57.51356919052574\n",
      "在第293步，我们获得了函数 f(rm) = 3.820648302337106 * rm + -1.0746261348334805, 此时loss是: 57.506878605488986\n",
      "在第294步，我们获得了函数 f(rm) = 3.82191834714145 * rm + -1.0827050397114968, 此时loss是: 57.50019123786558\n",
      "在第295步，我们获得了函数 f(rm) = 3.8231880865352457 * rm + -1.09078200183697, 此时loss是: 57.49350708610832\n",
      "在第296步，我们获得了函数 f(rm) = 3.824457520591935 * rm + -1.0988570216770783, 此时loss是: 57.48682614867075\n",
      "在第297步，我们获得了函数 f(rm) = 3.8257266493849436 * rm + -1.1069300996988873, 此时loss是: 57.48014842400713\n",
      "在第298步，我们获得了函数 f(rm) = 3.826995472987678 * rm + -1.1150012363693504, 此时loss是: 57.4734739105725\n",
      "在第299步，我们获得了函数 f(rm) = 3.8282639914735297 * rm + -1.1230704321553089, 此时loss是: 57.466802606822604\n",
      "在第300步，我们获得了函数 f(rm) = 3.8295322049158687 * rm + -1.1311376875234918, 此时loss是: 57.46013451121396\n",
      "在第301步，我们获得了函数 f(rm) = 3.830800113388051 * rm + -1.1392030029405154, 此时loss是: 57.4534696222038\n",
      "在第302步，我们获得了函数 f(rm) = 3.832067716963413 * rm + -1.1472663788728843, 此时loss是: 57.44680793825014\n",
      "在第303步，我们获得了函数 f(rm) = 3.833335015715274 * rm + -1.1553278157869908, 此时loss是: 57.44014945781168\n",
      "在第304步，我们获得了函数 f(rm) = 3.834602009716935 * rm + -1.1633873141491153, 此时loss是: 57.43349417934791\n",
      "在第305步，我们获得了函数 f(rm) = 3.835868699041681 * rm + -1.1714448744254253, 此时loss是: 57.42684210131904\n",
      "在第306步，我们获得了函数 f(rm) = 3.8371350837627776 * rm + -1.179500497081977, 此时loss是: 57.420193222186015\n",
      "在第307步，我们获得了函数 f(rm) = 3.838401163953474 * rm + -1.1875541825847138, 此时loss是: 57.41354754041055\n",
      "在第308步，我们获得了函数 f(rm) = 3.8396669396870005 * rm + -1.195605931399468, 此时loss是: 57.40690505445506\n",
      "在第309步，我们获得了函数 f(rm) = 3.840932411036572 * rm + -1.203655743991959, 此时loss是: 57.40026576278272\n",
      "在第310步，我们获得了函数 f(rm) = 3.842197578075383 * rm + -1.2117036208277947, 此时loss是: 57.393629663857446\n",
      "在第311步，我们获得了函数 f(rm) = 3.843462440876613 * rm + -1.2197495623724708, 此时loss是: 57.38699675614389\n",
      "在第312步，我们获得了函数 f(rm) = 3.844726999513422 * rm + -1.2277935690913713, 此时loss是: 57.38036703810743\n",
      "在第313步，我们获得了函数 f(rm) = 3.8459912540589527 * rm + -1.235835641449768, 此时loss是: 57.373740508214205\n",
      "在第314步，我们获得了函数 f(rm) = 3.8472552045863315 * rm + -1.2438757799128208, 此时loss是: 57.36711716493107\n",
      "在第315步，我们获得了函数 f(rm) = 3.8485188511686657 * rm + -1.2519139849455783, 此时loss是: 57.36049700672563\n",
      "在第316步，我们获得了函数 f(rm) = 3.849782193879046 * rm + -1.2599502570129764, 此时loss是: 57.35388003206623\n",
      "在第317步，我们获得了函数 f(rm) = 3.8510452327905456 * rm + -1.2679845965798398, 此时loss是: 57.347266239421934\n",
      "在第318步，我们获得了函数 f(rm) = 3.8523079679762184 * rm + -1.2760170041108814, 此时loss是: 57.340655627262564\n",
      "在第319步，我们获得了函数 f(rm) = 3.853570399509103 * rm + -1.2840474800707022, 此时loss是: 57.33404819405866\n",
      "在第320步，我们获得了函数 f(rm) = 3.8548325274622193 * rm + -1.2920760249237917, 此时loss是: 57.327443938281505\n",
      "在第321步，我们获得了函数 f(rm) = 3.85609435190857 * rm + -1.3001026391345272, 此时loss是: 57.32084285840312\n",
      "在第322步，我们获得了函数 f(rm) = 3.85735587292114 * rm + -1.3081273231671746, 此时loss是: 57.314244952896246\n",
      "在第323步，我们获得了函数 f(rm) = 3.8586170905728965 * rm + -1.3161500774858883, 此时loss是: 57.30765022023438\n",
      "在第324步，我们获得了函数 f(rm) = 3.8598780049367893 * rm + -1.324170902554711, 此时loss是: 57.30105865889175\n",
      "在第325步，我们获得了函数 f(rm) = 3.8611386160857513 * rm + -1.332189798837574, 此时loss是: 57.29447026734331\n",
      "在第326步，我们获得了函数 f(rm) = 3.8623989240926964 * rm + -1.3402067667982964, 此时loss是: 57.28788504406474\n",
      "在第327步，我们获得了函数 f(rm) = 3.8636589290305223 * rm + -1.3482218069005865, 此时loss是: 57.28130298753245\n",
      "在第328步，我们获得了函数 f(rm) = 3.8649186309721086 * rm + -1.3562349196080405, 此时loss是: 57.27472409622361\n",
      "在第329步，我们获得了函数 f(rm) = 3.866178029990318 * rm + -1.3642461053841437, 此时loss是: 57.26814836861611\n",
      "在第330步，我们获得了函数 f(rm) = 3.867437126157994 * rm + -1.3722553646922695, 此时loss是: 57.26157580318856\n",
      "在第331步，我们获得了函数 f(rm) = 3.868695919547965 * rm + -1.38026269799568, 此时loss是: 57.25500639842031\n",
      "在第332步，我们获得了函数 f(rm) = 3.8699544102330403 * rm + -1.3882681057575261, 此时loss是: 57.248440152791424\n",
      "在第333步，我们获得了函数 f(rm) = 3.8712125982860117 * rm + -1.396271588440847, 此时loss是: 57.24187706478275\n",
      "在第334步，我们获得了函数 f(rm) = 3.8724704837796544 * rm + -1.4042731465085705, 此时loss是: 57.2353171328758\n",
      "在第335步，我们获得了函数 f(rm) = 3.873728066786725 * rm + -1.4122727804235136, 此时loss是: 57.22876035555287\n",
      "在第336步，我们获得了函数 f(rm) = 3.874985347379963 * rm + -1.4202704906483814, 此时loss是: 57.22220673129694\n",
      "在第337步，我们获得了函数 f(rm) = 3.876242325632091 * rm + -1.4282662776457682, 此时loss是: 57.215656258591764\n",
      "在第338步，我们获得了函数 f(rm) = 3.877499001615813 * rm + -1.4362601418781569, 此时loss是: 57.2091089359218\n",
      "在第339步，我们获得了函数 f(rm) = 3.8787553754038173 * rm + -1.444252083807919, 此时loss是: 57.20256476177221\n",
      "在第340步，我们获得了函数 f(rm) = 3.8800114470687723 * rm + -1.4522421038973148, 此时loss是: 57.19602373462894\n",
      "在第341步，我们获得了函数 f(rm) = 3.8812672166833315 * rm + -1.460230202608494, 此时loss是: 57.18948585297863\n",
      "在第342步，我们获得了函数 f(rm) = 3.8825226843201284 * rm + -1.4682163804034944, 此时loss是: 57.18295111530865\n",
      "在第343步，我们获得了函数 f(rm) = 3.883777850051781 * rm + -1.4762006377442434, 此时loss是: 57.17641952010711\n",
      "在第344步，我们获得了函数 f(rm) = 3.8850327139508893 * rm + -1.4841829750925568, 此时loss是: 57.16989106586282\n",
      "在第345步，我们获得了函数 f(rm) = 3.8862872760900355 * rm + -1.4921633929101394, 此时loss是: 57.16336575106536\n",
      "在第346步，我们获得了函数 f(rm) = 3.8875415365417836 * rm + -1.5001418916585856, 此时loss是: 57.156843574205006\n",
      "在第347步，我们获得了函数 f(rm) = 3.888795495378682 * rm + -1.5081184717993776, 此时loss是: 57.150324533772746\n",
      "在第348步，我们获得了函数 f(rm) = 3.8900491526732606 * rm + -1.5160931337938879, 此时loss是: 57.14380862826033\n",
      "在第349步，我们获得了函数 f(rm) = 3.8913025084980317 * rm + -1.5240658781033771, 此时loss是: 57.13729585616023\n",
      "在第350步，我们获得了函数 f(rm) = 3.8925555629254913 * rm + -1.5320367051889954, 此时loss是: 57.13078621596561\n",
      "在第351步，我们获得了函数 f(rm) = 3.8938083160281156 * rm + -1.540005615511782, 此时loss是: 57.12427970617037\n",
      "在第352步，我们获得了函数 f(rm) = 3.8950607678783653 * rm + -1.5479726095326654, 此时loss是: 57.11777632526917\n",
      "在第353步，我们获得了函数 f(rm) = 3.8963129185486842 * rm + -1.5559376877124627, 此时loss是: 57.111276071757345\n",
      "在第354步，我们获得了函数 f(rm) = 3.897564768111497 * rm + -1.5639008505118805, 此时loss是: 57.10477894413098\n",
      "在第355步，我们获得了函数 f(rm) = 3.898816316639212 * rm + -1.571862098391515, 此时loss是: 57.098284940886884\n",
      "在第356步，我们获得了函数 f(rm) = 3.9000675642042193 * rm + -1.579821431811851, 此时loss是: 57.091794060522574\n",
      "在第357步，我们获得了函数 f(rm) = 3.9013185108788924 * rm + -1.5877788512332631, 此时loss是: 57.08530630153631\n",
      "在第358步，我们获得了函数 f(rm) = 3.902569156735587 * rm + -1.5957343571160147, 此时loss是: 57.07882166242705\n",
      "在第359步，我们获得了函数 f(rm) = 3.9038195018466415 * rm + -1.603687949920259, 此时loss是: 57.072340141694504\n",
      "在第360步，我们获得了函数 f(rm) = 3.905069546284377 * rm + -1.6116396301060383, 此时loss是: 57.065861737839064\n",
      "在第361步，我们获得了函数 f(rm) = 3.906319290121097 * rm + -1.619589398133284, 此时loss是: 57.059386449361895\n",
      "在第362步，我们获得了函数 f(rm) = 3.9075687334290876 * rm + -1.6275372544618176, 此时loss是: 57.0529142747648\n",
      "在第363步，我们获得了函数 f(rm) = 3.908817876280618 * rm + -1.6354831995513495, 此时loss是: 57.04644521255043\n",
      "在第364步，我们获得了函数 f(rm) = 3.910066718747939 * rm + -1.6434272338614795, 此时loss是: 57.039979261222015\n",
      "在第365步，我们获得了函数 f(rm) = 3.9113152609032853 * rm + -1.651369357851697, 此时loss是: 57.033516419283615\n",
      "在第366步，我们获得了函数 f(rm) = 3.912563502818873 * rm + -1.6593095719813813, 此时loss是: 57.027056685239934\n",
      "在第367步，我们获得了函数 f(rm) = 3.9138114445669028 * rm + -1.6672478767098007, 此时loss是: 57.02060005759644\n",
      "在第368步，我们获得了函数 f(rm) = 3.915059086219555 * rm + -1.6751842724961132, 此时loss是: 57.014146534859314\n",
      "在第369步，我们获得了函数 f(rm) = 3.916306427848995 * rm + -1.6831187597993664, 此时loss是: 57.00769611553545\n",
      "在第370步，我们获得了函数 f(rm) = 3.9175534695273697 * rm + -1.6910513390784976, 此时loss是: 57.001248798132444\n",
      "在第371步，我们获得了函数 f(rm) = 3.918800211326809 * rm + -1.6989820107923337, 此时loss是: 56.994804581158625\n",
      "在第372步，我们获得了函数 f(rm) = 3.9200466533194263 * rm + -1.7069107753995911, 此时loss是: 56.988363463123044\n",
      "在第373步，我们获得了函数 f(rm) = 3.921292795577316 * rm + -1.7148376333588764, 此时loss是: 56.98192544253546\n",
      "在第374步，我们获得了函数 f(rm) = 3.922538638172556 * rm + -1.7227625851286852, 此时loss是: 56.97549051790636\n",
      "在第375步，我们获得了函数 f(rm) = 3.9237841811772074 * rm + -1.7306856311674033, 此时loss是: 56.96905868774692\n",
      "在第376步，我们获得了函数 f(rm) = 3.9250294246633133 * rm + -1.738606771933306, 此时loss是: 56.96262995056907\n",
      "在第377步，我们获得了函数 f(rm) = 3.9262743687028987 * rm + -1.746526007884559, 此时loss是: 56.95620430488542\n",
      "在第378步，我们获得了函数 f(rm) = 3.927519013367973 * rm + -1.7544433394792172, 此时loss是: 56.94978174920933\n",
      "在第379步，我们获得了函数 f(rm) = 3.928763358730528 * rm + -1.7623587671752252, 此时loss是: 56.943362282054835\n",
      "在第380步，我们获得了函数 f(rm) = 3.9300074048625366 * rm + -1.7702722914304183, 此时loss是: 56.93694590193672\n",
      "在第381步，我们获得了函数 f(rm) = 3.9312511518359563 * rm + -1.778183912702521, 此时loss是: 56.93053260737047\n",
      "在第382步，我们获得了函数 f(rm) = 3.9324945997227254 * rm + -1.786093631449148, 此时loss是: 56.924122396872285\n",
      "在第383步，我们获得了函数 f(rm) = 3.933737748594767 * rm + -1.7940014481278037, 此时loss是: 56.917715268959064\n",
      "在第384步，我们获得了函数 f(rm) = 3.9349805985239854 * rm + -1.8019073631958828, 此时loss是: 56.911311222148456\n",
      "在第385步，我们获得了函数 f(rm) = 3.936223149582268 * rm + -1.80981137711067, 此时loss是: 56.90491025495877\n",
      "在第386步，我们获得了函数 f(rm) = 3.9374654018414845 * rm + -1.8177134903293402, 此时loss是: 56.89851236590909\n",
      "在第387步，我们获得了函数 f(rm) = 3.938707355373489 * rm + -1.8256137033089577, 此时loss是: 56.892117553519164\n",
      "在第388步，我们获得了函数 f(rm) = 3.939949010250116 * rm + -1.8335120165064773, 此时loss是: 56.885725816309446\n",
      "在第389步，我们获得了函数 f(rm) = 3.9411903665431844 * rm + -1.841408430378744, 此时loss是: 56.879337152801156\n",
      "在第390步，我们获得了函数 f(rm) = 3.942431424324495 * rm + -1.8493029453824927, 此时loss是: 56.87295156151618\n",
      "在第391步，我们获得了函数 f(rm) = 3.943672183665832 * rm + -1.857195561974349, 此时loss是: 56.866569040977126\n",
      "在第392步，我们获得了函数 f(rm) = 3.9449126446389617 * rm + -1.865086280610828, 此时loss是: 56.860189589707325\n",
      "在第393步，我们获得了函数 f(rm) = 3.946152807315633 * rm + -1.8729751017483351, 此时loss是: 56.853813206230775\n",
      "在第394步，我们获得了函数 f(rm) = 3.9473926717675782 * rm + -1.8808620258431668, 此时loss是: 56.847439889072234\n",
      "在第395步，我们获得了函数 f(rm) = 3.9486322380665126 * rm + -1.8887470533515087, 此时loss是: 56.841069636757176\n",
      "在第396步，我们获得了函数 f(rm) = 3.9498715062841327 * rm + -1.8966301847294376, 此时loss是: 56.83470244781171\n",
      "在第397步，我们获得了函数 f(rm) = 3.95111047649212 * rm + -1.9045114204329199, 此时loss是: 56.82833832076273\n",
      "在第398步，我们获得了函数 f(rm) = 3.9523491487621363 * rm + -1.912390760917813, 此时loss是: 56.821977254137806\n",
      "在第399步，我们获得了函数 f(rm) = 3.953587523165828 * rm + -1.9202682066398644, 此时loss是: 56.81561924646523\n",
      "在第400步，我们获得了函数 f(rm) = 3.954825599774824 * rm + -1.9281437580547118, 此时loss是: 56.80926429627397\n",
      "在第401步，我们获得了函数 f(rm) = 3.9560633786607355 * rm + -1.9360174156178838, 此时loss是: 56.80291240209374\n",
      "在第402步，我们获得了函数 f(rm) = 3.957300859895156 * rm + -1.943889179784799, 此时loss是: 56.79656356245495\n",
      "在第403步，我们获得了函数 f(rm) = 3.9585380435496633 * rm + -1.9517590510107667, 此时loss是: 56.7902177758887\n",
      "在第404步，我们获得了函数 f(rm) = 3.959774929695817 * rm + -1.9596270297509868, 此时loss是: 56.78387504092682\n",
      "在第405步，我们获得了函数 f(rm) = 3.9610115184051593 * rm + -1.9674931164605498, 此时loss是: 56.777535356101815\n",
      "在第406步，我们获得了函数 f(rm) = 3.9622478097492153 * rm + -1.9753573115944363, 此时loss是: 56.77119871994693\n",
      "在第407步，我们获得了函数 f(rm) = 3.9634838037994933 * rm + -1.983219615607518, 此时loss是: 56.76486513099609\n",
      "在第408步，我们获得了函数 f(rm) = 3.9647195006274845 * rm + -1.9910800289545567, 此时loss是: 56.758534587783956\n",
      "在第409步，我们获得了函数 f(rm) = 3.9659549003046624 * rm + -1.9989385520902054, 此时loss是: 56.75220708884585\n",
      "在第410步，我们获得了函数 f(rm) = 3.9671900029024836 * rm + -2.0067951854690076, 此时loss是: 56.745882632717816\n",
      "在第411步，我们获得了函数 f(rm) = 3.968424808492387 * rm + -2.014649929545397, 此时loss是: 56.73956121793664\n",
      "在第412步，我们获得了函数 f(rm) = 3.9696593171457955 * rm + -2.0225027847736987, 此时loss是: 56.733242843039754\n",
      "在第413步，我们获得了函数 f(rm) = 3.9708935289341136 * rm + -2.030353751608128, 此时loss是: 56.72692750656532\n",
      "在第414步，我们获得了函数 f(rm) = 3.9721274439287293 * rm + -2.038202830502792, 此时loss是: 56.7206152070522\n",
      "在第415步，我们获得了函数 f(rm) = 3.9733610622010134 * rm + -2.046050021911687, 此时loss是: 56.71430594303997\n",
      "在第416步，我们获得了函数 f(rm) = 3.9745943838223186 * rm + -2.053895326288701, 此时loss是: 56.70799971306889\n",
      "在第417步，我们获得了函数 f(rm) = 3.9758274088639824 * rm + -2.061738744087614, 此时loss是: 56.70169651567991\n",
      "在第418步，我们获得了函数 f(rm) = 3.977060137397323 * rm + -2.0695802757620942, 此时loss是: 56.69539634941475\n",
      "在第419步，我们获得了函数 f(rm) = 3.9782925694936435 * rm + -2.0774199217657032, 此时loss是: 56.689099212815755\n",
      "在第420步，我们获得了函数 f(rm) = 3.9795247052242275 * rm + -2.0852576825518923, 此时loss是: 56.68280510442599\n",
      "在第421步，我们获得了函数 f(rm) = 3.9807565446603435 * rm + -2.0930935585740036, 此时loss是: 56.67651402278924\n",
      "在第422步，我们获得了函数 f(rm) = 3.9819880878732423 * rm + -2.1009275502852707, 此时loss是: 56.67022596644999\n",
      "在第423步，我们获得了函数 f(rm) = 3.9832193349341565 * rm + -2.1087596581388186, 此时loss是: 56.66394093395339\n",
      "在第424步，我们获得了函数 f(rm) = 3.984450285914303 * rm + -2.1165898825876623, 此时loss是: 56.65765892384535\n",
      "在第425步，我们获得了函数 f(rm) = 3.985680940884881 * rm + -2.124418224084709, 此时loss是: 56.651379934672384\n",
      "在第426步，我们获得了函数 f(rm) = 3.986911299917073 * rm + -2.1322446830827557, 此时loss是: 56.645103964981836\n",
      "在第427步，我们获得了函数 f(rm) = 3.9881413630820433 * rm + -2.1400692600344917, 此时loss是: 56.63883101332163\n",
      "在第428步，我们获得了函数 f(rm) = 3.9893711304509396 * rm + -2.1478919553924967, 此时loss是: 56.63256107824045\n",
      "在第429步，我们获得了函数 f(rm) = 3.990600602094893 * rm + -2.1557127696092415, 此时loss是: 56.626294158287656\n",
      "在第430步，我们获得了函数 f(rm) = 3.991829778085018 * rm + -2.163531703137089, 此时loss是: 56.62003025201332\n",
      "在第431步，我们获得了函数 f(rm) = 3.99305865849241 * rm + -2.171348756428292, 此时loss是: 56.61376935796821\n",
      "在第432步，我们获得了函数 f(rm) = 3.9942872433881487 * rm + -2.179163929934996, 此时loss是: 56.60751147470376\n",
      "在第433步，我们获得了函数 f(rm) = 3.9955155328432967 * rm + -2.1869772241092367, 此时loss是: 56.60125660077217\n",
      "在第434步，我们获得了函数 f(rm) = 3.9967435269288987 * rm + -2.1947886394029417, 此时loss是: 56.59500473472625\n",
      "在第435步，我们获得了函数 f(rm) = 3.9979712257159834 * rm + -2.202598176267929, 此时loss是: 56.58875587511957\n",
      "在第436步，我们获得了函数 f(rm) = 3.999198629275562 * rm + -2.2104058351559095, 此时loss是: 56.582510020506355\n",
      "在第437步，我们获得了函数 f(rm) = 4.000425737678628 * rm + -2.2182116165184835, 此时loss是: 56.576267169441564\n",
      "在第438步，我们获得了函数 f(rm) = 4.00165255099616 * rm + -2.226015520807145, 此时loss是: 56.57002732048081\n",
      "在第439步，我们获得了函数 f(rm) = 4.002879069299115 * rm + -2.233817548473277, 此时loss是: 56.56379047218046\n",
      "在第440步，我们获得了函数 f(rm) = 4.004105292658438 * rm + -2.241617699968156, 此时loss是: 56.557556623097476\n",
      "在第441步，我们获得了函数 f(rm) = 4.005331221145054 * rm + -2.249415975742949, 此时loss是: 56.55132577178963\n",
      "在第442步，我们获得了函数 f(rm) = 4.006556854829872 * rm + -2.2572123762487144, 此时loss是: 56.545097916815294\n",
      "在第443步，我们获得了函数 f(rm) = 4.007782193783783 * rm + -2.2650069019364025, 此时loss是: 56.5388730567336\n",
      "在第444步，我们获得了函数 f(rm) = 4.009007238077663 * rm + -2.2727995532568555, 此时loss是: 56.53265119010432\n",
      "在第445步，我们获得了函数 f(rm) = 4.010231987782368 * rm + -2.280590330660806, 此时loss是: 56.52643231548795\n",
      "在第446步，我们获得了函数 f(rm) = 4.01145644296874 * rm + -2.2883792345988794, 此时loss是: 56.52021643144568\n",
      "在第447步，我们获得了函数 f(rm) = 4.012680603707601 * rm + -2.296166265521592, 此时loss是: 56.51400353653937\n",
      "在第448步，我们获得了函数 f(rm) = 4.013904470069758 * rm + -2.3039514238793526, 此时loss是: 56.507793629331594\n",
      "在第449步，我们获得了函数 f(rm) = 4.015128042126001 * rm + -2.3117347101224603, 此时loss是: 56.5015867083856\n",
      "在第450步，我们获得了函数 f(rm) = 4.016351319947102 * rm + -2.3195161247011074, 此时loss是: 56.495382772265344\n",
      "在第451步，我们获得了函数 f(rm) = 4.0175743036038165 * rm + -2.327295668065377, 此时loss是: 56.48918181953546\n",
      "在第452步，我们获得了函数 f(rm) = 4.018796993166883 * rm + -2.3350733406652444, 此时loss是: 56.48298384876128\n",
      "在第453步，我们获得了函数 f(rm) = 4.020019388707022 * rm + -2.3428491429505764, 此时loss是: 56.476788858508804\n",
      "在第454步，我们获得了函数 f(rm) = 4.021241490294939 * rm + -2.350623075371132, 此时loss是: 56.47059684734473\n",
      "在第455步，我们获得了函数 f(rm) = 4.02246329800132 * rm + -2.3583951383765616, 此时loss是: 56.464407813836495\n",
      "在第456步，我们获得了函数 f(rm) = 4.023684811896837 * rm + -2.366165332416408, 此时loss是: 56.45822175655216\n",
      "在第457步，我们获得了函数 f(rm) = 4.024906032052142 * rm + -2.373933657940105, 此时loss是: 56.452038674060496\n",
      "在第458步，我们获得了函数 f(rm) = 4.026126958537873 * rm + -2.3817001153969795, 此时loss是: 56.44585856493097\n",
      "在第459步，我们获得了函数 f(rm) = 4.027347591424648 * rm + -2.389464705236249, 此时loss是: 56.43968142773372\n",
      "在第460步，我们获得了函数 f(rm) = 4.028567930783069 * rm + -2.397227427907025, 此时loss是: 56.433507261039594\n",
      "在第461步，我们获得了函数 f(rm) = 4.029787976683723 * rm + -2.4049882838583083, 此时loss是: 56.42733606342013\n",
      "在第462步，我们获得了函数 f(rm) = 4.031007729197177 * rm + -2.412747273538994, 此时loss是: 56.42116783344751\n",
      "在第463步，我们获得了函数 f(rm) = 4.032227188393983 * rm + -2.4205043973978686, 此时loss是: 56.41500256969466\n",
      "在第464步，我们获得了函数 f(rm) = 4.033446354344676 * rm + -2.42825965588361, 此时loss是: 56.40884027073515\n",
      "在第465步，我们获得了函数 f(rm) = 4.034665227119773 * rm + -2.4360130494447887, 此时loss是: 56.40268093514325\n",
      "在第466步，我们获得了函数 f(rm) = 4.035883806789775 * rm + -2.4437645785298674, 此时loss是: 56.39652456149391\n",
      "在第467步，我们获得了函数 f(rm) = 4.037102093425165 * rm + -2.451514243587201, 此时loss是: 56.390371148362796\n",
      "在第468步，我们获得了函数 f(rm) = 4.038320087096411 * rm + -2.4592620450650364, 此时loss是: 56.38422069432622\n",
      "在第469步，我们获得了函数 f(rm) = 4.039537787873961 * rm + -2.467007983411513, 此时loss是: 56.378073197961164\n",
      "在第470步，我们获得了函数 f(rm) = 4.040755195828249 * rm + -2.4747520590746617, 此时loss是: 56.37192865784537\n",
      "在第471步，我们获得了函数 f(rm) = 4.04197231102969 * rm + -2.482494272502407, 此时loss是: 56.3657870725572\n",
      "在第472步，我们获得了函数 f(rm) = 4.043189133548685 * rm + -2.490234624142564, 此时loss是: 56.35964844067571\n",
      "在第473步，我们获得了函数 f(rm) = 4.044405663455613 * rm + -2.4979731144428414, 此时loss是: 56.35351276078063\n",
      "在第474步，我们获得了函数 f(rm) = 4.045621900820842 * rm + -2.50570974385084, 此时loss是: 56.34738003145244\n",
      "在第475步，我们获得了函数 f(rm) = 4.046837845714718 * rm + -2.5134445128140523, 此时loss是: 56.34125025127219\n",
      "在第476步，我们获得了函数 f(rm) = 4.048053498207572 * rm + -2.521177421779864, 此时loss是: 56.33512341882172\n",
      "在第477步，我们获得了函数 f(rm) = 4.049268858369721 * rm + -2.5289084711955527, 此时loss是: 56.3289995326835\n",
      "在第478步，我们获得了函数 f(rm) = 4.050483926271459 * rm + -2.536637661508289, 此时loss是: 56.32287859144064\n",
      "在第479步，我们获得了函数 f(rm) = 4.05169870198307 * rm + -2.544364993165135, 此时loss是: 56.316760593677046\n",
      "在第480步，我们获得了函数 f(rm) = 4.052913185574815 * rm + -2.552090466613046, 此时loss是: 56.310645537977216\n",
      "在第481步，我们获得了函数 f(rm) = 4.054127377116942 * rm + -2.55981408229887, 此时loss是: 56.30453342292633\n",
      "在第482步，我们获得了函数 f(rm) = 4.05534127667968 * rm + -2.5675358406693474, 此时loss是: 56.29842424711027\n",
      "在第483步，我们获得了函数 f(rm) = 4.056554884333243 * rm + -2.5752557421711106, 此时loss是: 56.29231800911562\n",
      "在第484步，我们获得了函数 f(rm) = 4.057768200147827 * rm + -2.5829737872506855, 此时loss是: 56.28621470752962\n",
      "在第485步，我们获得了函数 f(rm) = 4.05898122419361 * rm + -2.5906899763544895, 此时loss是: 56.28011434094017\n",
      "在第486步，我们获得了函数 f(rm) = 4.060193956540754 * rm + -2.5984043099288336, 此时loss是: 56.27401690793588\n",
      "在第487步，我们获得了函数 f(rm) = 4.0614063972594066 * rm + -2.6061167884199214, 此时loss是: 56.26792240710603\n",
      "在第488步，我们获得了函数 f(rm) = 4.062618546419695 * rm + -2.613827412273849, 此时loss是: 56.26183083704057\n",
      "在第489步，我们获得了函数 f(rm) = 4.06383040409173 * rm + -2.6215361819366048, 此时loss是: 56.25574219633015\n",
      "在第490步，我们获得了函数 f(rm) = 4.065041970345609 * rm + -2.6292430978540704, 此时loss是: 56.249656483566056\n",
      "在第491步，我们获得了函数 f(rm) = 4.066253245251407 * rm + -2.6369481604720204, 此时loss是: 56.24357369734029\n",
      "在第492步，我们获得了函数 f(rm) = 4.067464228879188 * rm + -2.644651370236122, 此时loss是: 56.23749383624553\n",
      "在第493步，我们获得了函数 f(rm) = 4.068674921298994 * rm + -2.6523527275919347, 此时loss是: 56.231416898875096\n",
      "在第494步，我们获得了函数 f(rm) = 4.069885322580854 * rm + -2.6600522329849117, 此时loss是: 56.22534288382303\n",
      "在第495步，我们获得了函数 f(rm) = 4.0710954327947775 * rm + -2.6677498868603986, 此时loss是: 56.21927178968402\n",
      "在第496步，我们获得了函数 f(rm) = 4.072305252010759 * rm + -2.6754456896636336, 此时loss是: 56.21320361505344\n",
      "在第497步，我们获得了函数 f(rm) = 4.073514780298774 * rm + -2.683139641839749, 此时loss是: 56.20713835852734\n",
      "在第498步，我们获得了函数 f(rm) = 4.074724017728785 * rm + -2.6908317438337686, 此时loss是: 56.20107601870242\n",
      "在第499步，我们获得了函数 f(rm) = 4.075932964370734 * rm + -2.6985219960906104, 此时loss是: 56.1950165941761\n",
      "在第500步，我们获得了函数 f(rm) = 4.077141620294547 * rm + -2.7062103990550845, 此时loss是: 56.18896008354645\n",
      "在第501步，我们获得了函数 f(rm) = 4.078349985570135 * rm + -2.713896953171895, 此时loss是: 56.18290648541221\n",
      "在第502步，我们获得了函数 f(rm) = 4.079558060267389 * rm + -2.7215816588856376, 此时loss是: 56.176855798372806\n",
      "在第503步，我们获得了函数 f(rm) = 4.080765844456186 * rm + -2.7292645166408027, 此时loss是: 56.170808021028314\n",
      "在第504步，我们获得了函数 f(rm) = 4.081973338206386 * rm + -2.7369455268817724, 此时loss是: 56.16476315197953\n",
      "在第505步，我们获得了函数 f(rm) = 4.08318054158783 * rm + -2.7446246900528233, 此时loss是: 56.15872118982786\n",
      "在第506步，我们获得了函数 f(rm) = 4.0843874546703445 * rm + -2.752302006598124, 此时loss是: 56.15268213317543\n",
      "在第507步，我们获得了函数 f(rm) = 4.085594077523738 * rm + -2.7599774769617373, 此时loss是: 56.14664598062504\n",
      "在第508步，我们获得了函数 f(rm) = 4.086800410217803 * rm + -2.767651101587618, 此时loss是: 56.14061273078013\n",
      "在第509步，我们获得了函数 f(rm) = 4.088006452822314 * rm + -2.7753228809196155, 此时loss是: 56.134582382244815\n",
      "在第510步，我们获得了函数 f(rm) = 4.08921220540703 * rm + -2.7829928154014714, 此时loss是: 56.12855493362391\n",
      "在第511步，我们获得了函数 f(rm) = 4.090417668041693 * rm + -2.790660905476821, 此时loss是: 56.12253038352291\n",
      "在第512步，我们获得了函数 f(rm) = 4.091622840796027 * rm + -2.798327151589193, 此时loss是: 56.11650873054789\n",
      "在第513步，我们获得了函数 f(rm) = 4.092827723739742 * rm + -2.8059915541820097, 此时loss是: 56.110489973305725\n",
      "在第514步，我们获得了函数 f(rm) = 4.094032316942527 * rm + -2.8136541136985858, 此时loss是: 56.104474110403864\n",
      "在第515步，我们获得了函数 f(rm) = 4.095236620474059 * rm + -2.8213148305821303, 此时loss是: 56.09846114045047\n",
      "在第516步，我们获得了函数 f(rm) = 4.096440634403994 * rm + -2.8289737052757453, 此时loss是: 56.09245106205434\n",
      "在第517步，我们获得了函数 f(rm) = 4.097644358801974 * rm + -2.8366307382224263, 此时loss是: 56.08644387382499\n",
      "在第518步，我们获得了函数 f(rm) = 4.098847793737622 * rm + -2.8442859298650625, 此时loss是: 56.08043957437256\n",
      "在第519步，我们获得了函数 f(rm) = 4.100050939280549 * rm + -2.8519392806464365, 此时loss是: 56.074438162307885\n",
      "在第520步，我们获得了函数 f(rm) = 4.101253795500343 * rm + -2.859590791009224, 此时loss是: 56.06843963624244\n",
      "在第521步，我们获得了函数 f(rm) = 4.102456362466579 * rm + -2.8672404613959954, 此时loss是: 56.06244399478841\n",
      "在第522步，我们获得了函数 f(rm) = 4.103658640248814 * rm + -2.874888292249213, 此时loss是: 56.05645123655862\n",
      "在第523步，我们获得了函数 f(rm) = 4.10486062891659 * rm + -2.8825342840112342, 此时loss是: 56.05046136016656\n",
      "在第524步，我们获得了函数 f(rm) = 4.106062328539431 * rm + -2.8901784371243093, 此时loss是: 56.04447436422638\n",
      "在第525步，我们获得了函数 f(rm) = 4.107263739186843 * rm + -2.897820752030582, 此时loss是: 56.03849024735295\n",
      "在第526步，我们获得了函数 f(rm) = 4.108464860928317 * rm + -2.9054612291720905, 此时loss是: 56.0325090081617\n",
      "在第527步，我们获得了函数 f(rm) = 4.109665693833327 * rm + -2.913099868990766, 此时loss是: 56.02653064526885\n",
      "在第528步，我们获得了函数 f(rm) = 4.11086623797133 * rm + -2.920736671928434, 此时loss是: 56.020555157291206\n",
      "在第529步，我们获得了函数 f(rm) = 4.112066493411767 * rm + -2.928371638426813, 此时loss是: 56.01458254284624\n",
      "在第530步，我们获得了函数 f(rm) = 4.113266460224062 * rm + -2.936004768927516, 此时loss是: 56.00861280055214\n",
      "在第531步，我们获得了函数 f(rm) = 4.114466138477621 * rm + -2.9436360638720496, 此时loss是: 56.00264592902771\n",
      "在第532步，我们获得了函数 f(rm) = 4.115665528241833 * rm + -2.951265523701814, 此时loss是: 55.99668192689244\n",
      "在第533步，我们获得了函数 f(rm) = 4.116864629586075 * rm + -2.9588931488581034, 此时loss是: 55.990720792766474\n",
      "在第534步，我们获得了函数 f(rm) = 4.118063442579703 * rm + -2.9665189397821057, 此时loss是: 55.98476252527063\n",
      "在第535步，我们获得了函数 f(rm) = 4.119261967292057 * rm + -2.9741428969149033, 此时loss是: 55.978807123026385\n",
      "在第536步，我们获得了函数 f(rm) = 4.12046020379246 * rm + -2.981765020697472, 此时loss是: 55.97285458465586\n",
      "在第537步，我们获得了函数 f(rm) = 4.12165815215022 * rm + -2.989385311570681, 此时loss是: 55.96690490878188\n",
      "在第538步，我们获得了函数 f(rm) = 4.122855812434626 * rm + -2.9970037699752954, 此时loss是: 55.96095809402789\n",
      "在第539步，我们获得了函数 f(rm) = 4.124053184714952 * rm + -3.004620396351972, 此时loss是: 55.95501413901804\n",
      "在第540步，我们获得了函数 f(rm) = 4.125250269060456 * rm + -3.0122351911412633, 此时loss是: 55.949073042377094\n",
      "在第541步，我们获得了函数 f(rm) = 4.126447065540377 * rm + -3.019848154783615, 此时loss是: 55.943134802730505\n",
      "在第542步，我们获得了函数 f(rm) = 4.12764357422394 * rm + -3.027459287719367, 此时loss是: 55.93719941870438\n",
      "在第543步，我们获得了函数 f(rm) = 4.128839795180352 * rm + -3.0350685903887538, 此时loss是: 55.9312668889255\n",
      "在第544步，我们获得了函数 f(rm) = 4.130035728478803 * rm + -3.0426760632319034, 此时loss是: 55.925337212021276\n",
      "在第545步，我们获得了函数 f(rm) = 4.1312313741884665 * rm + -3.0502817066888386, 此时loss是: 55.91941038661983\n",
      "在第546步，我们获得了函数 f(rm) = 4.1324267323785 * rm + -3.0578855211994758, 此时loss是: 55.913486411349886\n",
      "在第547步，我们获得了函数 f(rm) = 4.133621803118043 * rm + -3.0654875072036263, 此时loss是: 55.90756528484087\n",
      "在第548步，我们获得了函数 f(rm) = 4.13481658647622 * rm + -3.0730876651409944, 此时loss是: 55.90164700572282\n",
      "在第549步，我们获得了函数 f(rm) = 4.136011082522139 * rm + -3.08068599545118, 此时loss是: 55.89573157262651\n",
      "在第550步，我们获得了函数 f(rm) = 4.137205291324889 * rm + -3.088282498573677, 此时loss是: 55.88981898418329\n",
      "在第551步，我们获得了函数 f(rm) = 4.138399212953546 * rm + -3.095877174947873, 此时loss是: 55.88390923902522\n",
      "在第552步，我们获得了函数 f(rm) = 4.139592847477167 * rm + -3.1034700250130496, 此时loss是: 55.878002335785006\n",
      "在第553步，我们获得了函数 f(rm) = 4.140786194964791 * rm + -3.1110610492083848, 此时loss是: 55.87209827309598\n",
      "在第554步，我们获得了函数 f(rm) = 4.141979255485444 * rm + -3.1186502479729485, 此时loss是: 55.8661970495922\n",
      "在第555步，我们获得了函数 f(rm) = 4.1431720291081335 * rm + -3.126237621745707, 此时loss是: 55.86029866390831\n",
      "在第556步，我们获得了函数 f(rm) = 4.14436451590185 * rm + -3.13382317096552, 此时loss是: 55.854403114679634\n",
      "在第557步，我们获得了函数 f(rm) = 4.145556715935568 * rm + -3.1414068960711417, 此时loss是: 55.84851040054219\n",
      "在第558步，我们获得了函数 f(rm) = 4.146748629278244 * rm + -3.1489887975012207, 此时loss是: 55.8426205201326\n",
      "在第559步，我们获得了函数 f(rm) = 4.147940255998821 * rm + -3.156568875694301, 此时loss是: 55.83673347208815\n",
      "在第560步，我们获得了函数 f(rm) = 4.1491315961662245 * rm + -3.1641471310888205, 此时loss是: 55.83084925504683\n",
      "在第561步，我们获得了函数 f(rm) = 4.15032264984936 * rm + -3.1717235641231114, 此时loss是: 55.82496786764721\n",
      "在第562步，我们获得了函数 f(rm) = 4.151513417117121 * rm + -3.1792981752354006, 此时loss是: 55.819089308528575\n",
      "在第563步，我们获得了函数 f(rm) = 4.152703898038381 * rm + -3.18687096486381, 此时loss是: 55.813213576330845\n",
      "在第564步，我们获得了函数 f(rm) = 4.153894092682 * rm + -3.194441933446356, 此时loss是: 55.80734066969457\n",
      "在第565步，我们获得了函数 f(rm) = 4.155084001116818 * rm + -3.2020110814209497, 此时loss是: 55.80147058726099\n",
      "在第566步，我们获得了函数 f(rm) = 4.1562736234116615 * rm + -3.2095784092253967, 此时loss是: 55.79560332767199\n",
      "在第567步，我们获得了函数 f(rm) = 4.157462959635339 * rm + -3.2171439172973972, 此时loss是: 55.78973888957009\n",
      "在第568步，我们获得了函数 f(rm) = 4.1586520098566435 * rm + -3.224707606074547, 此时loss是: 55.78387727159847\n",
      "在第569步，我们获得了函数 f(rm) = 4.159840774144349 * rm + -3.2322694759943356, 此时loss是: 55.77801847240097\n",
      "在第570步，我们获得了函数 f(rm) = 4.161029252567215 * rm + -3.2398295274941478, 此时loss是: 55.7721624906221\n",
      "在第571步，我们获得了函数 f(rm) = 4.162217445193986 * rm + -3.247387761011263, 此时loss是: 55.76630932490698\n",
      "在第572步，我们获得了函数 f(rm) = 4.163405352093386 * rm + -3.254944176982856, 此时loss是: 55.7604589739014\n",
      "在第573步，我们获得了函数 f(rm) = 4.164592973334123 * rm + -3.2624987758459962, 此时loss是: 55.7546114362518\n",
      "在第574步，我们获得了函数 f(rm) = 4.1657803089848935 * rm + -3.2700515580376472, 此时loss是: 55.74876671060531\n",
      "在第575步，我们获得了函数 f(rm) = 4.1669673591143725 * rm + -3.2776025239946684, 此时loss是: 55.74292479560964\n",
      "在第576步，我们获得了函数 f(rm) = 4.16815412379122 * rm + -3.285151674153814, 此时loss是: 55.73708568991319\n",
      "在第577步，我们获得了函数 f(rm) = 4.16934060308408 * rm + -3.292699008951733, 此时loss是: 55.73124939216501\n",
      "在第578步，我们获得了函数 f(rm) = 4.170526797061577 * rm + -3.300244528824969, 此时loss是: 55.7254159010148\n",
      "在第579步，我们获得了函数 f(rm) = 4.1717127057923244 * rm + -3.307788234209961, 此时loss是: 55.71958521511289\n",
      "在第580步，我们获得了函数 f(rm) = 4.172898329344915 * rm + -3.315330125543044, 此时loss是: 55.713757333110294\n",
      "在第581步，我们获得了函数 f(rm) = 4.174083667787926 * rm + -3.3228702032604462, 此时loss是: 55.70793225365865\n",
      "在第582步，我们获得了函数 f(rm) = 4.175268721189918 * rm + -3.330408467798293, 此时loss是: 55.70210997541023\n",
      "在第583步，我们获得了函数 f(rm) = 4.176453489619437 * rm + -3.3379449195926023, 此时loss是: 55.69629049701799\n",
      "在第584步，我们获得了函数 f(rm) = 4.177637973145009 * rm + -3.3454795590792896, 此时loss是: 55.690473817135505\n",
      "在第585步，我们获得了函数 f(rm) = 4.178822171835147 * rm + -3.3530123866941643, 此时loss是: 55.68465993441703\n",
      "在第586步，我们获得了函数 f(rm) = 4.180006085758345 * rm + -3.3605434028729317, 此时loss是: 55.67884884751742\n",
      "在第587步，我们获得了函数 f(rm) = 4.181189714983082 * rm + -3.368072608051192, 此时loss是: 55.67304055509221\n",
      "在第588步，我们获得了函数 f(rm) = 4.182373059577821 * rm + -3.3756000026644397, 此时loss是: 55.66723505579759\n",
      "在第589步，我们获得了函数 f(rm) = 4.183556119611006 * rm + -3.3831255871480663, 此时loss是: 55.661432348290354\n",
      "在第590步，我们获得了函数 f(rm) = 4.1847388951510665 * rm + -3.3906493619373577, 此时loss是: 55.655632431227986\n",
      "在第591步，我们获得了函数 f(rm) = 4.185921386266417 * rm + -3.3981713274674945, 此时loss是: 55.649835303268596\n",
      "在第592步，我们获得了函数 f(rm) = 4.187103593025451 * rm + -3.405691484173554, 此时loss是: 55.64404096307094\n",
      "在第593步，我们获得了函数 f(rm) = 4.188285515496551 * rm + -3.4132098324905082, 此时loss是: 55.63824940929444\n",
      "在第594步，我们获得了函数 f(rm) = 4.189467153748078 * rm + -3.4207263728532245, 此时loss是: 55.632460640599106\n",
      "在第595步，我们获得了函数 f(rm) = 4.19064850784838 * rm + -3.4282411056964657, 此时loss是: 55.626674655645644\n",
      "在第596步，我们获得了函数 f(rm) = 4.1918295778657875 * rm + -3.43575403145489, 此时loss是: 55.62089145309541\n",
      "在第597步，我们获得了函数 f(rm) = 4.1930103638686145 * rm + -3.443265150563051, 此时loss是: 55.61511103161036\n",
      "在第598步，我们获得了函数 f(rm) = 4.194190865925159 * rm + -3.4507744634553985, 此时loss是: 55.60933338985312\n",
      "在第599步，我们获得了函数 f(rm) = 4.1953710841037015 * rm + -3.4582819705662766, 此时loss是: 55.60355852648697\n",
      "在第600步，我们获得了函数 f(rm) = 4.1965510184725066 * rm + -3.465787672329926, 此时loss是: 55.597786440175796\n",
      "在第601步，我们获得了函数 f(rm) = 4.197730669099824 * rm + -3.473291569180483, 此时loss是: 55.59201712958417\n",
      "在第602步，我们获得了函数 f(rm) = 4.198910036053884 * rm + -3.480793661551979, 此时loss是: 55.58625059337728\n",
      "在第603步，我们获得了函数 f(rm) = 4.200089119402904 * rm + -3.4882939498783405, 此时loss是: 55.58048683022094\n",
      "在第604步，我们获得了函数 f(rm) = 4.2012679192150815 * rm + -3.4957924345933913, 此时loss是: 55.57472583878167\n",
      "在第605步，我们获得了函数 f(rm) = 4.2024464355586 * rm + -3.503289116130849, 此时loss是: 55.56896761772655\n",
      "在第606步，我们获得了函数 f(rm) = 4.203624668501626 * rm + -3.5107839949243282, 此时loss是: 55.56321216572335\n",
      "在第607步，我们获得了函数 f(rm) = 4.204802618112309 * rm + -3.518277071407339, 此时loss是: 55.55745948144047\n",
      "在第608步，我们获得了函数 f(rm) = 4.205980284458783 * rm + -3.5257683460132867, 此时loss是: 55.55170956354696\n",
      "在第609步，我们获得了函数 f(rm) = 4.207157667609165 * rm + -3.533257819175473, 此时loss是: 55.5459624107125\n",
      "在第610步，我们获得了函数 f(rm) = 4.208334767631555 * rm + -3.5407454913270953, 此时loss是: 55.54021802160738\n",
      "在第611步，我们获得了函数 f(rm) = 4.209511584594038 * rm + -3.5482313629012463, 此时loss是: 55.53447639490259\n",
      "在第612步，我们获得了函数 f(rm) = 4.210688118564683 * rm + -3.5557154343309154, 此时loss是: 55.52873752926972\n",
      "在第613步，我们获得了函数 f(rm) = 4.211864369611541 * rm + -3.563197706048987, 此时loss是: 55.52300142338102\n",
      "在第614步，我们获得了函数 f(rm) = 4.213040337802647 * rm + -3.570678178488242, 此时loss是: 55.517268075909335\n",
      "在第615步，我们获得了函数 f(rm) = 4.214216023206021 * rm + -3.578156852081357, 此时loss是: 55.5115374855282\n",
      "在第616步，我们获得了函数 f(rm) = 4.215391425889663 * rm + -3.585633727260905, 此时loss是: 55.50580965091177\n",
      "在第617步，我们获得了函数 f(rm) = 4.216566545921562 * rm + -3.5931088044593538, 此时loss是: 55.500084570734835\n",
      "在第618步，我们获得了函数 f(rm) = 4.217741383369687 * rm + -3.6005820841090683, 此时loss是: 55.4943622436728\n",
      "在第619步，我们获得了函数 f(rm) = 4.218915938301992 * rm + -3.6080535666423095, 此时loss是: 55.48864266840175\n",
      "在第620步，我们获得了函数 f(rm) = 4.220090210786413 * rm + -3.6155232524912337, 此时loss是: 55.48292584359839\n",
      "在第621步，我们获得了函数 f(rm) = 4.2212642008908725 * rm + -3.6229911420878937, 此时loss是: 55.47721176794002\n",
      "在第622步，我们获得了函数 f(rm) = 4.222437908683274 * rm + -3.6304572358642386, 此时loss是: 55.471500440104656\n",
      "在第623步，我们获得了函数 f(rm) = 4.223611334231506 * rm + -3.637921534252113, 此时loss是: 55.46579185877089\n",
      "在第624步，我们获得了函数 f(rm) = 4.224784477603439 * rm + -3.6453840376832582, 此时loss是: 55.46008602261794\n",
      "在第625步，我们获得了函数 f(rm) = 4.225957338866931 * rm + -3.6528447465893117, 此时loss是: 55.454382930325735\n",
      "在第626步，我们获得了函数 f(rm) = 4.22712991808982 * rm + -3.6603036614018065, 此时loss是: 55.44868258057476\n",
      "在第627步，我们获得了函数 f(rm) = 4.228302215339928 * rm + -3.6677607825521727, 此时loss是: 55.44298497204616\n",
      "在第628步，我们获得了函数 f(rm) = 4.229474230685065 * rm + -3.6752161104717365, 此时loss是: 55.43729010342173\n",
      "在第629步，我们获得了函数 f(rm) = 4.230645964193017 * rm + -3.6826696455917194, 此时loss是: 55.43159797338388\n",
      "在第630步，我们获得了函数 f(rm) = 4.231817415931561 * rm + -3.6901213883432407, 此时loss是: 55.42590858061566\n",
      "在第631步，我们获得了函数 f(rm) = 4.232988585968453 * rm + -3.697571339157315, 此时loss是: 55.42022192380075\n",
      "在第632步，我们获得了函数 f(rm) = 4.234159474371436 * rm + -3.705019498464854, 此时loss是: 55.41453800162348\n",
      "在第633步，我们获得了函数 f(rm) = 4.235330081208234 * rm + -3.7124658666966646, 此时loss是: 55.40885681276879\n",
      "在第634步，我们获得了函数 f(rm) = 4.236500406546557 * rm + -3.7199104442834514, 此时loss是: 55.40317835592225\n",
      "在第635步，我们获得了函数 f(rm) = 4.237670450454096 * rm + -3.7273532316558144, 此时loss是: 55.3975026297701\n",
      "在第636步，我们获得了函数 f(rm) = 4.238840212998528 * rm + -3.734794229244251, 此时loss是: 55.39182963299916\n",
      "在第637步，我们获得了函数 f(rm) = 4.240009694247513 * rm + -3.7422334374791544, 此时loss是: 55.386159364296915\n",
      "在第638步，我们获得了函数 f(rm) = 4.241178894268695 * rm + -3.7496708567908144, 此时loss是: 55.38049182235149\n",
      "在第639步，我们获得了函数 f(rm) = 4.242347813129702 * rm + -3.757106487609417, 此时loss是: 55.37482700585159\n",
      "在第640步，我们获得了函数 f(rm) = 4.243516450898144 * rm + -3.764540330365046, 此时loss是: 55.369164913486614\n",
      "在第641步，我们获得了函数 f(rm) = 4.244684807641617 * rm + -3.7719723854876808, 此时loss是: 55.36350554394655\n",
      "在第642步，我们获得了函数 f(rm) = 4.245852883427698 * rm + -3.7794026534071974, 此时loss是: 55.35784889592205\n",
      "在第643步，我们获得了函数 f(rm) = 4.247020678323952 * rm + -3.7868311345533683, 此时loss是: 55.352194968104314\n",
      "在第644步，我们获得了函数 f(rm) = 4.248188192397923 * rm + -3.794257829355863, 此时loss是: 55.34654375918529\n",
      "在第645步，我们获得了函数 f(rm) = 4.249355425717143 * rm + -3.801682738244248, 此时loss是: 55.34089526785748\n",
      "在第646步，我们获得了函数 f(rm) = 4.250522378349125 * rm + -3.8091058616479856, 此时loss是: 55.33524949281402\n",
      "在第647步，我们获得了函数 f(rm) = 4.251689050361365 * rm + -3.816527199996436, 此时loss是: 55.32960643274868\n",
      "在第648步，我们获得了函数 f(rm) = 4.252855441821345 * rm + -3.823946753718855, 此时loss是: 55.323966086355895\n",
      "在第649步，我们获得了函数 f(rm) = 4.254021552796531 * rm + -3.831364523244396, 此时loss是: 55.31832845233066\n",
      "在第650步，我们获得了函数 f(rm) = 4.255187383354372 * rm + -3.8387805090021083, 此时loss是: 55.31269352936866\n",
      "在第651步，我们获得了函数 f(rm) = 4.256352933562298 * rm + -3.846194711420939, 此时loss是: 55.30706131616616\n",
      "在第652步，我们获得了函数 f(rm) = 4.257518203487728 * rm + -3.8536071309297317, 此时loss是: 55.301431811420095\n",
      "在第653步，我们获得了函数 f(rm) = 4.258683193198062 * rm + -3.8610177679572266, 此时loss是: 55.29580501382797\n",
      "在第654步，我们获得了函数 f(rm) = 4.259847902760683 * rm + -3.868426622932061, 此时loss是: 55.290180922087984\n",
      "在第655步，我们获得了函数 f(rm) = 4.261012332242959 * rm + -3.8758336962827693, 此时loss是: 55.28455953489892\n",
      "在第656步，我们获得了函数 f(rm) = 4.262176481712242 * rm + -3.8832389884377827, 此时loss是: 55.27894085096018\n",
      "在第657步，我们获得了函数 f(rm) = 4.2633403512358665 * rm + -3.890642499825429, 此时loss是: 55.27332486897183\n",
      "在第658步，我们获得了函数 f(rm) = 4.264503940881152 * rm + -3.898044230873934, 此时loss是: 55.26771158763453\n",
      "在第659步，我们获得了函数 f(rm) = 4.265667250715402 * rm + -3.905444182011419, 此时loss是: 55.26210100564955\n",
      "在第660步，我们获得了函数 f(rm) = 4.2668302808059035 * rm + -3.912842353665904, 此时loss是: 55.25649312171885\n",
      "在第661步，我们获得了函数 f(rm) = 4.267993031219926 * rm + -3.920238746265305, 此时loss是: 55.250887934544934\n",
      "在第662步，我们获得了函数 f(rm) = 4.2691555020247245 * rm + -3.9276333602374356, 此时loss是: 55.24528544283098\n",
      "在第663步，我们获得了函数 f(rm) = 4.270317693287537 * rm + -3.935026196010006, 此时loss是: 55.23968564528079\n",
      "在第664步，我们获得了函数 f(rm) = 4.271479605075586 * rm + -3.9424172540106244, 此时loss是: 55.23408854059876\n",
      "在第665步，我们获得了函数 f(rm) = 4.272641237456077 * rm + -3.949806534666795, 此时loss是: 55.22849412748993\n",
      "在第666步，我们获得了函数 f(rm) = 4.273802590496201 * rm + -3.95719403840592, 此时loss是: 55.22290240465996\n",
      "在第667步，我们获得了函数 f(rm) = 4.2749636642631295 * rm + -3.9645797656552992, 此时loss是: 55.217313370815134\n",
      "在第668步，我们获得了函数 f(rm) = 4.276124458824022 * rm + -3.9719637168421285, 此时loss是: 55.21172702466235\n",
      "在第669步，我们获得了函数 f(rm) = 4.277284974246019 * rm + -3.9793458923935017, 此时loss是: 55.20614336490914\n",
      "在第670步，我们获得了函数 f(rm) = 4.278445210596245 * rm + -3.98672629273641, 此时loss是: 55.20056239026363\n",
      "在第671步，我们获得了函数 f(rm) = 4.279605167941809 * rm + -3.9941049182977424, 此时loss是: 55.194984099434606\n",
      "在第672步，我们获得了函数 f(rm) = 4.280764846349806 * rm + -4.0014817695042835, 此时loss是: 55.18940849113146\n",
      "在第673步，我们获得了函数 f(rm) = 4.28192424588731 * rm + -4.0088568467827175, 此时loss是: 55.18383556406419\n",
      "在第674步，我们获得了函数 f(rm) = 4.283083366621383 * rm + -4.0162301505596245, 此时loss是: 55.17826531694342\n",
      "在第675步，我们获得了函数 f(rm) = 4.2842422086190695 * rm + -4.023601681261482, 此时loss是: 55.172697748480395\n",
      "在第676步，我们获得了函数 f(rm) = 4.285400771947398 * rm + -4.030971439314666, 此时loss是: 55.167132857387024\n",
      "在第677步，我们获得了函数 f(rm) = 4.28655905667338 * rm + -4.038339425145448, 此时loss是: 55.16157064237576\n",
      "在第678步，我们获得了函数 f(rm) = 4.287717062864012 * rm + -4.04570563918, 此时loss是: 55.15601110215971\n",
      "在第679步，我们获得了函数 f(rm) = 4.288874790586275 * rm + -4.05307008184439, 此时loss是: 55.15045423545261\n",
      "在第680步，我们获得了函数 f(rm) = 4.290032239907132 * rm + -4.060432753564582, 此时loss是: 55.144900040968814\n",
      "在第681步，我们获得了函数 f(rm) = 4.29118941089353 * rm + -4.067793654766439, 此时loss是: 55.13934851742326\n",
      "在第682步，我们获得了函数 f(rm) = 4.292346303612402 * rm + -4.075152785875723, 此时loss是: 55.13379966353156\n",
      "在第683步，我们获得了函数 f(rm) = 4.293502918130663 * rm + -4.082510147318091, 此时loss是: 55.12825347800989\n",
      "在第684步，我们获得了函数 f(rm) = 4.294659254515213 * rm + -4.089865739519099, 此时loss是: 55.12270995957508\n",
      "在第685步，我们获得了函数 f(rm) = 4.295815312832935 * rm + -4.0972195629042005, 此时loss是: 55.117169106944544\n",
      "在第686步，我们获得了函数 f(rm) = 4.296971093150696 * rm + -4.104571617898747, 此时loss是: 55.11163091883638\n",
      "在第687步，我们获得了函数 f(rm) = 4.298126595535349 * rm + -4.111921904927987, 此时loss是: 55.106095393969184\n",
      "在第688步，我们获得了函数 f(rm) = 4.299281820053728 * rm + -4.119270424417068, 此时loss是: 55.100562531062295\n",
      "在第689步，我们获得了函数 f(rm) = 4.300436766772651 * rm + -4.126617176791033, 此时loss是: 55.09503232883557\n",
      "在第690步，我们获得了函数 f(rm) = 4.301591435758922 * rm + -4.133962162474825, 此时loss是: 55.08950478600958\n",
      "在第691步，我们获得了函数 f(rm) = 4.302745827079329 * rm + -4.141305381893284, 此时loss是: 55.08397990130538\n",
      "在第692步，我们获得了函数 f(rm) = 4.303899940800643 * rm + -4.148646835471148, 此时loss是: 55.07845767344478\n",
      "在第693步，我们获得了函数 f(rm) = 4.305053776989617 * rm + -4.155986523633051, 此时loss是: 55.072938101150115\n",
      "在第694步，我们获得了函数 f(rm) = 4.30620733571299 * rm + -4.163324446803529, 此时loss是: 55.06742118314435\n",
      "在第695步，我们获得了函数 f(rm) = 4.307360617037488 * rm + -4.1706606054070114, 此时loss是: 55.06190691815108\n",
      "在第696步，我们获得了函数 f(rm) = 4.308513621029814 * rm + -4.177994999867829, 此时loss是: 55.0563953048945\n",
      "在第697步，我们获得了函数 f(rm) = 4.30966634775666 * rm + -4.185327630610209, 此时loss是: 55.050886342099446\n",
      "在第698步，我们获得了函数 f(rm) = 4.310818797284701 * rm + -4.192658498058277, 此时loss是: 55.04538002849133\n",
      "在第699步，我们获得了函数 f(rm) = 4.311970969680595 * rm + -4.199987602636055, 此时loss是: 55.03987636279619\n",
      "在第700步，我们获得了函数 f(rm) = 4.3131228650109845 * rm + -4.207314944767465, 此时loss是: 55.0343753437407\n",
      "在第701步，我们获得了函数 f(rm) = 4.314274483342497 * rm + -4.2146405248763275, 此时loss是: 55.02887697005209\n",
      "在第702步，我们获得了函数 f(rm) = 4.315425824741743 * rm + -4.221964343386359, 此时loss是: 55.02338124045828\n",
      "在第703步，我们获得了函数 f(rm) = 4.316576889275316 * rm + -4.2292864007211755, 此时loss是: 55.017888153687714\n",
      "在第704步，我们获得了函数 f(rm) = 4.317727677009795 * rm + -4.236606697304291, 此时loss是: 55.012397708469535\n",
      "在第705步，我们获得了函数 f(rm) = 4.318878188011744 * rm + -4.243925233559117, 此时loss是: 55.00690990353343\n",
      "在第706步，我们获得了函数 f(rm) = 4.320028422347707 * rm + -4.251242009908964, 此时loss是: 55.001424737609746\n",
      "在第707步，我们获得了函数 f(rm) = 4.321178380084215 * rm + -4.25855702677704, 此时loss是: 54.995942209429394\n",
      "在第708步，我们获得了函数 f(rm) = 4.322328061287784 * rm + -4.265870284586452, 此时loss是: 54.99046231772392\n",
      "在第709步，我们获得了函数 f(rm) = 4.323477466024912 * rm + -4.273181783760204, 此时loss是: 54.984985061225494\n",
      "在第710步，我们获得了函数 f(rm) = 4.32462659436208 * rm + -4.2804915247212, 此时loss是: 54.979510438666885\n",
      "在第711步，我们获得了函数 f(rm) = 4.325775446365756 * rm + -4.287799507892242, 此时loss是: 54.97403844878143\n",
      "在第712步，我们获得了函数 f(rm) = 4.326924022102391 * rm + -4.295105733696028, 此时loss是: 54.96856909030315\n",
      "在第713步，我们获得了函数 f(rm) = 4.328072321638418 * rm + -4.302410202555157, 此时loss是: 54.96310236196663\n",
      "在第714步，我们获得了函数 f(rm) = 4.329220345040256 * rm + -4.309712914892125, 此时loss是: 54.95763826250704\n",
      "在第715步，我们获得了函数 f(rm) = 4.330368092374308 * rm + -4.317013871129327, 此时loss是: 54.95217679066022\n",
      "在第716步，我们获得了函数 f(rm) = 4.331515563706962 * rm + -4.324313071689057, 此时loss是: 54.94671794516256\n",
      "在第717步，我们获得了函数 f(rm) = 4.332662759104586 * rm + -4.331610516993507, 此时loss是: 54.941261724751115\n",
      "在第718步，我们获得了函数 f(rm) = 4.3338096786335365 * rm + -4.338906207464766, 此时loss是: 54.935808128163494\n",
      "在第719步，我们获得了函数 f(rm) = 4.334956322360151 * rm + -4.346200143524823, 此时loss是: 54.93035715413794\n",
      "在第720步，我们获得了函数 f(rm) = 4.336102690350754 * rm + -4.353492325595565, 此时loss是: 54.92490880141329\n",
      "在第721步，我们获得了函数 f(rm) = 4.337248782671651 * rm + -4.3607827540987785, 此时loss是: 54.91946306872901\n",
      "在第722步，我们获得了函数 f(rm) = 4.338394599389133 * rm + -4.368071429456148, 此时loss是: 54.91401995482516\n",
      "在第723步，我们获得了函数 f(rm) = 4.339540140569476 * rm + -4.375358352089255, 此时loss是: 54.90857945844239\n",
      "在第724步，我们获得了函数 f(rm) = 4.340685406278937 * rm + -4.382643522419582, 此时loss是: 54.90314157832197\n",
      "在第725步，我们获得了函数 f(rm) = 4.341830396583762 * rm + -4.389926940868509, 此时loss是: 54.89770631320578\n",
      "在第726步，我们获得了函数 f(rm) = 4.342975111550176 * rm + -4.3972086078573165, 此时loss是: 54.89227366183628\n",
      "在第727步，我们获得了函数 f(rm) = 4.344119551244391 * rm + -4.40448852380718, 此时loss是: 54.88684362295658\n",
      "在第728步，我们获得了函数 f(rm) = 4.345263715732602 * rm + -4.411766689139176, 此时loss是: 54.88141619531037\n",
      "在第729步，我们获得了函数 f(rm) = 4.346407605080989 * rm + -4.41904310427428, 此时loss是: 54.87599137764191\n",
      "在第730步，我们获得了函数 f(rm) = 4.347551219355715 * rm + -4.426317769633367, 此时loss是: 54.87056916869614\n",
      "在第731步，我们获得了函数 f(rm) = 4.3486945586229275 * rm + -4.433590685637208, 此时loss是: 54.86514956721853\n",
      "在第732步，我们获得了函数 f(rm) = 4.349837622948759 * rm + -4.440861852706475, 此时loss是: 54.85973257195519\n",
      "在第733步，我们获得了函数 f(rm) = 4.350980412399324 * rm + -4.4481312712617385, 此时loss是: 54.85431818165282\n",
      "在第734步，我们获得了函数 f(rm) = 4.352122927040724 * rm + -4.455398941723467, 此时loss是: 54.848906395058755\n",
      "在第735步，我们获得了函数 f(rm) = 4.353265166939041 * rm + -4.462664864512028, 此时loss是: 54.84349721092087\n",
      "在第736步，我们获得了函数 f(rm) = 4.354407132160344 * rm + -4.46992904004769, 此时loss是: 54.8380906279877\n",
      "在第737步，我们获得了函数 f(rm) = 4.355548822770686 * rm + -4.477191468750617, 此时loss是: 54.83268664500836\n",
      "在第738步，我们获得了函数 f(rm) = 4.356690238836103 * rm + -4.484452151040876, 此时loss是: 54.82728526073256\n",
      "在第739步，我们获得了函数 f(rm) = 4.357831380422614 * rm + -4.491711087338429, 此时loss是: 54.821886473910624\n",
      "在第740步，我们获得了函数 f(rm) = 4.358972247596226 * rm + -4.49896827806314, 此时loss是: 54.816490283293454\n",
      "在第741步，我们获得了函数 f(rm) = 4.360112840422925 * rm + -4.50622372363477, 此时loss是: 54.81109668763261\n",
      "在第742步，我们获得了函数 f(rm) = 4.361253158968686 * rm + -4.51347742447298, 此时loss是: 54.80570568568017\n",
      "在第743步，我们获得了函数 f(rm) = 4.362393203299465 * rm + -4.520729380997331, 此时loss是: 54.80031727618888\n",
      "在第744步，我们获得了函数 f(rm) = 4.3635329734812025 * rm + -4.527979593627281, 此时loss是: 54.79493145791203\n",
      "在第745步，我们获得了函数 f(rm) = 4.364672469579825 * rm + -4.535228062782188, 此时loss是: 54.78954822960357\n",
      "在第746步，我们获得了函数 f(rm) = 4.365811691661242 * rm + -4.542474788881311, 此时loss是: 54.78416759001801\n",
      "在第747步，我们获得了函数 f(rm) = 4.366950639791345 * rm + -4.5497197723438045, 此时loss是: 54.778789537910455\n",
      "在第748步，我们获得了函数 f(rm) = 4.368089314036014 * rm + -4.556963013588725, 此时loss是: 54.77341407203664\n",
      "在第749步，我们获得了函数 f(rm) = 4.36922771446111 * rm + -4.564204513035029, 此时loss是: 54.76804119115287\n",
      "在第750步，我们获得了函数 f(rm) = 4.370365841132479 * rm + -4.571444271101569, 此时loss是: 54.76267089401606\n",
      "在第751步，我们获得了函数 f(rm) = 4.37150369411595 * rm + -4.5786822882071, 此时loss是: 54.75730317938371\n",
      "在第752步，我们获得了函数 f(rm) = 4.372641273477339 * rm + -4.585918564770273, 此时loss是: 54.751938046013954\n",
      "在第753步，我们获得了函数 f(rm) = 4.373778579282445 * rm + -4.593153101209641, 此时loss是: 54.746575492665464\n",
      "在第754步，我们获得了函数 f(rm) = 4.374915611597049 * rm + -4.600385897943656, 此时loss是: 54.741215518097576\n",
      "在第755步，我们获得了函数 f(rm) = 4.376052370486918 * rm + -4.607616955390668, 此时loss是: 54.73585812107016\n",
      "在第756步，我们获得了函数 f(rm) = 4.3771888560178045 * rm + -4.614846273968928, 此时loss是: 54.73050330034373\n",
      "在第757步，我们获得了函数 f(rm) = 4.378325068255442 * rm + -4.622073854096586, 此时loss是: 54.725151054679365\n",
      "在第758步，我们获得了函数 f(rm) = 4.379461007265552 * rm + -4.629299696191691, 此时loss是: 54.719801382838746\n",
      "在第759步，我们获得了函数 f(rm) = 4.380596673113836 * rm + -4.63652380067219, 此时loss是: 54.71445428358418\n",
      "在第760步，我们获得了函数 f(rm) = 4.381732065865983 * rm + -4.643746167955934, 此时loss是: 54.70910975567851\n",
      "在第761步，我们获得了函数 f(rm) = 4.382867185587665 * rm + -4.650966798460668, 此时loss是: 54.703767797885256\n",
      "在第762步，我们获得了函数 f(rm) = 4.384002032344538 * rm + -4.65818569260404, 此时loss是: 54.69842840896845\n",
      "在第763步，我们获得了函数 f(rm) = 4.385136606202243 * rm + -4.665402850803598, 此时loss是: 54.69309158769274\n",
      "在第764步，我们获得了函数 f(rm) = 4.386270907226404 * rm + -4.672618273476786, 此时loss是: 54.687757332823416\n",
      "在第765步，我们获得了函数 f(rm) = 4.38740493548263 * rm + -4.679831961040952, 此时loss是: 54.68242564312633\n",
      "在第766步，我们获得了函数 f(rm) = 4.388538691036515 * rm + -4.687043913913341, 此时loss是: 54.6770965173679\n",
      "在第767步，我们获得了函数 f(rm) = 4.389672173953635 * rm + -4.694254132511097, 此时loss是: 54.67176995431516\n",
      "在第768步，我们获得了函数 f(rm) = 4.390805384299552 * rm + -4.701462617251266, 此时loss是: 54.66644595273576\n",
      "在第769步，我们获得了函数 f(rm) = 4.391938322139812 * rm + -4.708669368550794, 此时loss是: 54.661124511397915\n",
      "在第770步，我们获得了函数 f(rm) = 4.393070987539946 * rm + -4.715874386826523, 此时loss是: 54.65580562907042\n",
      "在第771步，我们获得了函数 f(rm) = 4.394203380565466 * rm + -4.723077672495199, 此时loss是: 54.65048930452271\n",
      "在第772步，我们获得了函数 f(rm) = 4.395335501281873 * rm + -4.730279225973465, 此时loss是: 54.64517553652476\n",
      "在第773步，我们获得了函数 f(rm) = 4.396467349754649 * rm + -4.737479047677865, 此时loss是: 54.63986432384717\n",
      "在第774步，我们获得了函数 f(rm) = 4.39759892604926 * rm + -4.7446771380248425, 此时loss是: 54.634555665261125\n",
      "在第775步，我们获得了函数 f(rm) = 4.398730230231157 * rm + -4.751873497430741, 此时loss是: 54.62924955953839\n",
      "在第776步，我们获得了函数 f(rm) = 4.3998612623657785 * rm + -4.759068126311804, 此时loss是: 54.62394600545132\n",
      "在第777步，我们获得了函数 f(rm) = 4.400992022518541 * rm + -4.766261025084174, 此时loss是: 54.618645001772876\n",
      "在第778步，我们获得了函数 f(rm) = 4.402122510754851 * rm + -4.7734521941638945, 此时loss是: 54.6133465472766\n",
      "在第779步，我们获得了函数 f(rm) = 4.403252727140096 * rm + -4.780641633966909, 此时loss是: 54.60805064073662\n",
      "在第780步，我们获得了函数 f(rm) = 4.404382671739648 * rm + -4.787829344909061, 此时loss是: 54.60275728092766\n",
      "在第781步，我们获得了函数 f(rm) = 4.405512344618864 * rm + -4.795015327406092, 此时loss是: 54.59746646662504\n",
      "在第782步，我们获得了函数 f(rm) = 4.406641745843086 * rm + -4.802199581873646, 此时loss是: 54.59217819660464\n",
      "在第783步，我们获得了函数 f(rm) = 4.40777087547764 * rm + -4.809382108727267, 此时loss是: 54.586892469642976\n",
      "在第784步，我们获得了函数 f(rm) = 4.408899733587834 * rm + -4.816562908382396, 此时loss是: 54.5816092845171\n",
      "在第785步，我们获得了函数 f(rm) = 4.410028320238964 * rm + -4.823741981254379, 此时loss是: 54.576328640004675\n",
      "在第786步，我们获得了函数 f(rm) = 4.411156635496306 * rm + -4.830919327758458, 此时loss是: 54.57105053488398\n",
      "在第787步，我们获得了函数 f(rm) = 4.412284679425125 * rm + -4.838094948309777, 此时loss是: 54.565774967933834\n",
      "在第788步，我们获得了函数 f(rm) = 4.413412452090667 * rm + -4.845268843323379, 此时loss是: 54.56050193793368\n",
      "在第789步，我们获得了函数 f(rm) = 4.414539953558163 * rm + -4.852441013214209, 此时loss是: 54.55523144366351\n",
      "在第790步，我们获得了函数 f(rm) = 4.415667183892829 * rm + -4.859611458397111, 此时loss是: 54.54996348390393\n",
      "在第791步，我们获得了函数 f(rm) = 4.416794143159865 * rm + -4.866780179286828, 此时loss是: 54.54469805743616\n",
      "在第792步，我们获得了函数 f(rm) = 4.417920831424455 * rm + -4.873947176298007, 此时loss是: 54.539435163041944\n",
      "在第793步，我们获得了函数 f(rm) = 4.419047248751768 * rm + -4.881112449845191, 此时loss是: 54.53417479950363\n",
      "在第794步，我们获得了函数 f(rm) = 4.420173395206956 * rm + -4.8882760003428265, 此时loss是: 54.52891696560419\n",
      "在第795步，我们获得了函数 f(rm) = 4.421299270855157 * rm + -4.895437828205258, 此时loss是: 54.52366166012714\n",
      "在第796步，我们获得了函数 f(rm) = 4.422424875761493 * rm + -4.902597933846733, 此时loss是: 54.51840888185661\n",
      "在第797步，我们获得了函数 f(rm) = 4.423550209991069 * rm + -4.909756317681397, 此时loss是: 54.51315862957727\n",
      "在第798步，我们获得了函数 f(rm) = 4.424675273608975 * rm + -4.916912980123297, 此时loss是: 54.50791090207445\n",
      "在第799步，我们获得了函数 f(rm) = 4.425800066680286 * rm + -4.92406792158638, 此时loss是: 54.50266569813397\n",
      "在第800步，我们获得了函数 f(rm) = 4.426924589270062 * rm + -4.931221142484493, 此时loss是: 54.497423016542314\n",
      "在第801步，我们获得了函数 f(rm) = 4.428048841443345 * rm + -4.938372643231386, 此时loss是: 54.492182856086515\n",
      "在第802步，我们获得了函数 f(rm) = 4.429172823265163 * rm + -4.945522424240705, 此时loss是: 54.48694521555419\n",
      "在第803步，我们获得了函数 f(rm) = 4.4302965348005285 * rm + -4.952670485926002, 此时loss是: 54.48171009373351\n",
      "在第804步，我们获得了函数 f(rm) = 4.431419976114437 * rm + -4.959816828700725, 此时loss是: 54.476477489413305\n",
      "在第805步，我们获得了函数 f(rm) = 4.43254314727187 * rm + -4.966961452978225, 此时loss是: 54.471247401382925\n",
      "在第806步，我们获得了函数 f(rm) = 4.433666048337792 * rm + -4.974104359171752, 此时loss是: 54.466019828432316\n",
      "在第807步，我们获得了函数 f(rm) = 4.434788679377154 * rm + -4.981245547694459, 此时loss是: 54.46079476935201\n",
      "在第808步，我们获得了函数 f(rm) = 4.435911040454888 * rm + -4.988385018959398, 此时loss是: 54.455572222933135\n",
      "在第809步，我们获得了函数 f(rm) = 4.437033131635913 * rm + -4.995522773379521, 此时loss是: 54.450352187967376\n",
      "在第810步，我们获得了函数 f(rm) = 4.4381549529851325 * rm + -5.002658811367681, 此时loss是: 54.445134663246996\n",
      "在第811步，我们获得了函数 f(rm) = 4.439276504567432 * rm + -5.0097931333366335, 此时loss是: 54.43991964756487\n",
      "在第812步，我们获得了函数 f(rm) = 4.440397786447684 * rm + -5.016925739699033, 此时loss是: 54.43470713971443\n",
      "在第813步，我们获得了函数 f(rm) = 4.441518798690744 * rm + -5.0240566308674355, 此时loss是: 54.42949713848968\n",
      "在第814步，我们获得了函数 f(rm) = 4.442639541361453 * rm + -5.031185807254298, 此时loss是: 54.424289642685224\n",
      "在第815步，我们获得了函数 f(rm) = 4.443760014524635 * rm + -5.038313269271977, 此时loss是: 54.419084651096256\n",
      "在第816步，我们获得了函数 f(rm) = 4.4448802182451 * rm + -5.045439017332731, 此时loss是: 54.41388216251851\n",
      "在第817步，我们获得了函数 f(rm) = 4.446000152587639 * rm + -5.052563051848719, 此时loss是: 54.408682175748325\n",
      "在第818步，我们获得了函数 f(rm) = 4.447119817617033 * rm + -5.059685373232002, 此时loss是: 54.40348468958262\n",
      "在第819步，我们获得了函数 f(rm) = 4.4482392133980415 * rm + -5.0668059818945395, 此时loss是: 54.39828970281889\n",
      "在第820步，我们获得了函数 f(rm) = 4.449358339995413 * rm + -5.073924878248194, 此时loss是: 54.39309721425519\n",
      "在第821步，我们获得了函数 f(rm) = 4.450477197473879 * rm + -5.081042062704729, 此时loss是: 54.3879072226902\n",
      "在第822步，我们获得了函数 f(rm) = 4.451595785898155 * rm + -5.088157535675807, 此时loss是: 54.38271972692312\n",
      "在第823步，我们获得了函数 f(rm) = 4.45271410533294 * rm + -5.095271297572993, 此时loss是: 54.37753472575377\n",
      "在第824步，我们获得了函数 f(rm) = 4.453832155842919 * rm + -5.102383348807754, 此时loss是: 54.372352217982524\n",
      "在第825步，我们获得了函数 f(rm) = 4.454949937492762 * rm + -5.109493689791456, 此时loss是: 54.367172202410316\n",
      "在第826步，我们获得了函数 f(rm) = 4.4560674503471205 * rm + -5.116602320935367, 此时loss是: 54.361994677838716\n",
      "在第827步，我们获得了函数 f(rm) = 4.457184694470634 * rm + -5.123709242650655, 此时loss是: 54.356819643069834\n",
      "在第828步，我们获得了函数 f(rm) = 4.458301669927923 * rm + -5.130814455348392, 此时loss是: 54.35164709690634\n",
      "在第829步，我们获得了函数 f(rm) = 4.459418376783597 * rm + -5.137917959439548, 此时loss是: 54.346477038151484\n",
      "在第830步，我们获得了函数 f(rm) = 4.460534815102245 * rm + -5.145019755334997, 此时loss是: 54.34130946560914\n",
      "在第831步，我们获得了函数 f(rm) = 4.461650984948443 * rm + -5.152119843445511, 此时loss是: 54.33614437808371\n",
      "在第832步，我们获得了函数 f(rm) = 4.462766886386753 * rm + -5.159218224181766, 此时loss是: 54.330981774380156\n",
      "在第833步，我们获得了函数 f(rm) = 4.463882519481716 * rm + -5.166314897954337, 此时loss是: 54.32582165330408\n",
      "在第834步，我们获得了函数 f(rm) = 4.464997884297865 * rm + -5.173409865173703, 此时loss是: 54.3206640136616\n",
      "在第835步，我们获得了函数 f(rm) = 4.4661129808997115 * rm + -5.180503126250241, 此时loss是: 54.31550885425941\n",
      "在第836步，我们获得了函数 f(rm) = 4.467227809351754 * rm + -5.187594681594232, 此时loss是: 54.31035617390484\n",
      "在第837步，我们获得了函数 f(rm) = 4.468342369718475 * rm + -5.1946845316158585, 此时loss是: 54.3052059714057\n",
      "在第838步，我们获得了函数 f(rm) = 4.469456662064342 * rm + -5.201772676725201, 此时loss是: 54.300058245570455\n",
      "在第839步，我们获得了函数 f(rm) = 4.470570686453807 * rm + -5.208859117332245, 此时loss是: 54.2949129952081\n",
      "在第840步，我们获得了函数 f(rm) = 4.471684442951305 * rm + -5.215943853846875, 此时loss是: 54.289770219128215\n",
      "在第841步，我们获得了函数 f(rm) = 4.472797931621256 * rm + -5.22302688667888, 此时loss是: 54.28462991614095\n",
      "在第842步，我们获得了函数 f(rm) = 4.473911152528067 * rm + -5.230108216237945, 此时loss是: 54.27949208505702\n",
      "在第843步，我们获得了函数 f(rm) = 4.475024105736126 * rm + -5.237187842933663, 此时loss是: 54.27435672468774\n",
      "在第844步，我们获得了函数 f(rm) = 4.476136791309808 * rm + -5.244265767175524, 此时loss是: 54.26922383384494\n",
      "在第845步，我们获得了函数 f(rm) = 4.477249209313472 * rm + -5.251341989372922, 此时loss是: 54.2640934113411\n",
      "在第846步，我们获得了函数 f(rm) = 4.47836135981146 * rm + -5.2584165099351505, 此时loss是: 54.258965455989205\n",
      "在第847步，我们获得了函数 f(rm) = 4.479473242868101 * rm + -5.265489329271405, 此时loss是: 54.253839966602825\n",
      "在第848步，我们获得了函数 f(rm) = 4.480584858547706 * rm + -5.272560447790785, 此时loss是: 54.248716941996136\n",
      "在第849步，我们获得了函数 f(rm) = 4.481696206914573 * rm + -5.279629865902288, 此时loss是: 54.24359638098384\n",
      "在第850步，我们获得了函数 f(rm) = 4.482807288032983 * rm + -5.286697584014816, 此时loss是: 54.23847828238123\n",
      "在第851步，我们获得了函数 f(rm) = 4.4839181019672 * rm + -5.2937636025371715, 此时loss是: 54.23336264500418\n",
      "在第852步，我们获得了函数 f(rm) = 4.485028648781476 * rm + -5.300827921878058, 此时loss是: 54.2282494676691\n",
      "在第853步，我们获得了函数 f(rm) = 4.486138928540047 * rm + -5.307890542446082, 此时loss是: 54.223138749193\n",
      "在第854步，我们获得了函数 f(rm) = 4.48724894130713 * rm + -5.314951464649752, 此时loss是: 54.21803048839343\n",
      "在第855步，我们获得了函数 f(rm) = 4.488358687146931 * rm + -5.3220106888974765, 此时loss是: 54.21292468408856\n",
      "在第856步，我们获得了函数 f(rm) = 4.489468166123638 * rm + -5.329068215597568, 此时loss是: 54.20782133509706\n",
      "在第857步，我们获得了函数 f(rm) = 4.490577378301424 * rm + -5.336124045158238, 此时loss是: 54.20272044023821\n",
      "在第858步，我们获得了函数 f(rm) = 4.491686323744447 * rm + -5.343178177987602, 此时loss是: 54.19762199833186\n",
      "在第859步，我们获得了函数 f(rm) = 4.492795002516849 * rm + -5.350230614493678, 此时loss是: 54.192526008198435\n",
      "在第860步，我们获得了函数 f(rm) = 4.493903414682757 * rm + -5.357281355084383, 此时loss是: 54.18743246865888\n",
      "在第861步，我们获得了函数 f(rm) = 4.495011560306282 * rm + -5.364330400167538, 此时loss是: 54.18234137853474\n",
      "在第862步，我们获得了函数 f(rm) = 4.496119439451521 * rm + -5.371377750150867, 此时loss是: 54.17725273664813\n",
      "在第863步，我们获得了函数 f(rm) = 4.497227052182554 * rm + -5.378423405441993, 此时loss是: 54.172166541821724\n",
      "在第864步，我们获得了函数 f(rm) = 4.498334398563447 * rm + -5.3854673664484425, 此时loss是: 54.167082792878766\n",
      "在第865步，我们获得了函数 f(rm) = 4.499441478658249 * rm + -5.392509633577645, 此时loss是: 54.16200148864307\n",
      "在第866步，我们获得了函数 f(rm) = 4.500548292530994 * rm + -5.39955020723693, 此时loss是: 54.15692262793899\n",
      "在第867步，我们获得了函数 f(rm) = 4.501654840245703 * rm + -5.406589087833531, 此时loss是: 54.15184620959149\n",
      "在第868步，我们获得了函数 f(rm) = 4.502761121866377 * rm + -5.413626275774582, 此时loss是: 54.14677223242605\n",
      "在第869步，我们获得了函数 f(rm) = 4.503867137457006 * rm + -5.420661771467119, 此时loss是: 54.14170069526874\n",
      "在第870步，我们获得了函数 f(rm) = 4.504972887081562 * rm + -5.427695575318081, 此时loss是: 54.1366315969462\n",
      "在第871步，我们获得了函数 f(rm) = 4.506078370804004 * rm + -5.43472768773431, 此时loss是: 54.13156493628563\n",
      "在第872步，我们获得了函数 f(rm) = 4.507183588688272 * rm + -5.441758109122548, 此时loss是: 54.126500712114776\n",
      "在第873步，我们获得了函数 f(rm) = 4.5082885407982936 * rm + -5.448786839889443, 此时loss是: 54.121438923261984\n",
      "在第874步，我们获得了函数 f(rm) = 4.509393227197981 * rm + -5.455813880441539, 此时loss是: 54.116379568556134\n",
      "在第875步，我们获得了函数 f(rm) = 4.510497647951229 * rm + -5.462839231185288, 此时loss是: 54.111322646826665\n",
      "在第876步，我们获得了函数 f(rm) = 4.511601803121918 * rm + -5.469862892527041, 此时loss是: 54.1062681569036\n",
      "在第877步，我们获得了函数 f(rm) = 4.512705692773914 * rm + -5.476884864873052, 此时loss是: 54.10121609761753\n",
      "在第878步，我们获得了函数 f(rm) = 4.513809316971067 * rm + -5.483905148629479, 此时loss是: 54.09616646779956\n",
      "在第879步，我们获得了函数 f(rm) = 4.514912675777211 * rm + -5.490923744202381, 此时loss是: 54.091119266281424\n",
      "在第880步，我们获得了函数 f(rm) = 4.516015769256165 * rm + -5.497940651997718, 此时loss是: 54.08607449189535\n",
      "在第881步，我们获得了函数 f(rm) = 4.517118597471733 * rm + -5.504955872421354, 此时loss是: 54.081032143474204\n",
      "在第882步，我们获得了函数 f(rm) = 4.518221160487705 * rm + -5.511969405879055, 此时loss是: 54.07599221985135\n",
      "在第883步，我们获得了函数 f(rm) = 4.519323458367852 * rm + -5.5189812527764905, 此时loss是: 54.07095471986073\n",
      "在第884步，我们获得了函数 f(rm) = 4.520425491175933 * rm + -5.525991413519231, 此时loss是: 54.06591964233685\n",
      "在第885步，我们获得了函数 f(rm) = 4.52152725897569 * rm + -5.532999888512749, 此时loss是: 54.06088698611481\n",
      "在第886步，我们获得了函数 f(rm) = 4.52262876183085 * rm + -5.540006678162421, 此时loss是: 54.055856750030195\n",
      "在第887步，我们获得了函数 f(rm) = 4.523729999805125 * rm + -5.547011782873526, 此时loss是: 54.05082893291922\n",
      "在第888步，我们获得了函数 f(rm) = 4.524830972962212 * rm + -5.554015203051246, 此时loss是: 54.04580353361861\n",
      "在第889步，我们获得了函数 f(rm) = 4.525931681365791 * rm + -5.561016939100662, 此时loss是: 54.040780550965714\n",
      "在第890步，我们获得了函数 f(rm) = 4.527032125079529 * rm + -5.568016991426761, 此时loss是: 54.03575998379835\n",
      "在第891步，我们获得了函数 f(rm) = 4.528132304167077 * rm + -5.575015360434433, 此时loss是: 54.03074183095497\n",
      "在第892步，我们获得了函数 f(rm) = 4.529232218692069 * rm + -5.582012046528468, 此时loss是: 54.02572609127455\n",
      "在第893步，我们获得了函数 f(rm) = 4.530331868718125 * rm + -5.589007050113561, 此时loss是: 54.02071276359664\n",
      "在第894步，我们获得了函数 f(rm) = 4.531431254308851 * rm + -5.596000371594307, 此时loss是: 54.015701846761324\n",
      "在第895步，我们获得了函数 f(rm) = 4.532530375527835 * rm + -5.602992011375208, 此时loss是: 54.010693339609276\n",
      "在第896步，我们获得了函数 f(rm) = 4.533629232438652 * rm + -5.609981969860664, 此时loss是: 54.00568724098171\n",
      "在第897步，我们获得了函数 f(rm) = 4.534727825104859 * rm + -5.616970247454981, 此时loss是: 54.000683549720385\n",
      "在第898步，我们获得了函数 f(rm) = 4.535826153590002 * rm + -5.623956844562367, 此时loss是: 53.99568226466766\n",
      "在第899步，我们获得了函数 f(rm) = 4.536924217957608 * rm + -5.630941761586932, 此时loss是: 53.99068338466637\n",
      "在第900步，我们获得了函数 f(rm) = 4.53802201827119 * rm + -5.637924998932689, 此时loss是: 53.98568690856\n",
      "在第901步，我们获得了函数 f(rm) = 4.5391195545942455 * rm + -5.644906557003555, 此时loss是: 53.98069283519254\n",
      "在第902步，我们获得了函数 f(rm) = 4.540216826990257 * rm + -5.651886436203348, 此时loss是: 53.97570116340854\n",
      "在第903步，我们获得了函数 f(rm) = 4.541313835522692 * rm + -5.658864636935791, 此时loss是: 53.97071189205312\n",
      "在第904步，我们获得了函数 f(rm) = 4.542410580255002 * rm + -5.665841159604508, 此时loss是: 53.96572501997194\n",
      "在第905步，我们获得了函数 f(rm) = 4.543507061250623 * rm + -5.6728160046130265, 此时loss是: 53.960740546011216\n",
      "在第906步，我们获得了函数 f(rm) = 4.5446032785729775 * rm + -5.679789172364778, 此时loss是: 53.955758469017724\n",
      "在第907步，我们获得了函数 f(rm) = 4.545699232285471 * rm + -5.6867606632630965, 此时loss是: 53.95077878783879\n",
      "在第908步，我们获得了函数 f(rm) = 4.546794922451494 * rm + -5.6937304777112185, 此时loss是: 53.945801501322336\n",
      "在第909步，我们获得了函数 f(rm) = 4.547890349134423 * rm + -5.700698616112283, 此时loss是: 53.940826608316726\n",
      "在第910步，我们获得了函数 f(rm) = 4.548985512397619 * rm + -5.707665078869334, 此时loss是: 53.93585410767103\n",
      "在第911步，我们获得了函数 f(rm) = 4.550080412304425 * rm + -5.714629866385316, 此时loss是: 53.93088399823475\n",
      "在第912步，我们获得了函数 f(rm) = 4.551175048918172 * rm + -5.721592979063079, 此时loss是: 53.925916278858\n",
      "在第913步，我们获得了函数 f(rm) = 4.552269422302175 * rm + -5.728554417305375, 此时loss是: 53.920950948391436\n",
      "在第914步，我们获得了函数 f(rm) = 4.553363532519733 * rm + -5.735514181514859, 此时loss是: 53.91598800568624\n",
      "在第915步，我们获得了函数 f(rm) = 4.554457379634131 * rm + -5.742472272094091, 此时loss是: 53.911027449594194\n",
      "在第916步，我们获得了函数 f(rm) = 4.5555509637086375 * rm + -5.74942868944553, 此时loss是: 53.9060692789676\n",
      "在第917步，我们获得了函数 f(rm) = 4.556644284806506 * rm + -5.756383433971543, 此时loss是: 53.9011134926593\n",
      "在第918步，我们获得了函数 f(rm) = 4.557737342990975 * rm + -5.763336506074396, 此时loss是: 53.896160089522745\n",
      "在第919步，我们获得了函数 f(rm) = 4.558830138325268 * rm + -5.7702879061562635, 此时loss是: 53.891209068411875\n",
      "在第920步，我们获得了函数 f(rm) = 4.559922670872593 * rm + -5.777237634619218, 此时loss是: 53.8862604281812\n",
      "在第921步，我们获得了函数 f(rm) = 4.561014940696144 * rm + -5.784185691865238, 此时loss是: 53.88131416768581\n",
      "在第922步，我们获得了函数 f(rm) = 4.562106947859099 * rm + -5.791132078296204, 此时loss是: 53.8763702857813\n",
      "在第923步，我们获得了函数 f(rm) = 4.563198692424619 * rm + -5.798076794313902, 此时loss是: 53.871428781323836\n",
      "在第924步，我们获得了函数 f(rm) = 4.564290174455851 * rm + -5.805019840320019, 此时loss是: 53.86648965317016\n",
      "在第925步，我们获得了函数 f(rm) = 4.56538139401593 * rm + -5.8119612167161465, 此时loss是: 53.86155290017753\n",
      "在第926步，我们获得了函数 f(rm) = 4.5664723511679695 * rm + -5.81890092390378, 此时loss是: 53.85661852120374\n",
      "在第927步，我们获得了函数 f(rm) = 4.567563045975074 * rm + -5.825838962284318, 此时loss是: 53.85168651510718\n",
      "在第928步，我们获得了函数 f(rm) = 4.56865347850033 * rm + -5.832775332259062, 此时loss是: 53.84675688074676\n",
      "在第929步，我们获得了函数 f(rm) = 4.569743648806808 * rm + -5.839710034229217, 此时loss是: 53.84182961698195\n",
      "在第930步，我们获得了函数 f(rm) = 4.570833556957565 * rm + -5.846643068595892, 此时loss是: 53.83690472267272\n",
      "在第931步，我们获得了函数 f(rm) = 4.571923203015642 * rm + -5.8535744357601, 此时loss是: 53.83198219667969\n",
      "在第932步，我们获得了函数 f(rm) = 4.573012587044064 * rm + -5.860504136122756, 此时loss是: 53.82706203786393\n",
      "在第933步，我们获得了函数 f(rm) = 4.574101709105843 * rm + -5.8674321700846805, 此时loss是: 53.82214424508711\n",
      "在第934步，我们获得了函数 f(rm) = 4.575190569263975 * rm + -5.874358538046596, 此时loss是: 53.81722881721142\n",
      "在第935步，我们获得了函数 f(rm) = 4.57627916758144 * rm + -5.881283240409129, 此时loss是: 53.81231575309964\n",
      "在第936步，我们获得了函数 f(rm) = 4.577367504121203 * rm + -5.88820627757281, 此时loss是: 53.80740505161505\n",
      "在第937步，我们获得了函数 f(rm) = 4.578455578946215 * rm + -5.8951276499380745, 此时loss是: 53.80249671162148\n",
      "在第938步，我们获得了函数 f(rm) = 4.579543392119412 * rm + -5.902047357905259, 此时loss是: 53.79759073198334\n",
      "在第939步，我们获得了函数 f(rm) = 4.580630943703712 * rm + -5.908965401874606, 此时loss是: 53.79268711156555\n",
      "在第940步，我们获得了函数 f(rm) = 4.581718233762021 * rm + -5.9158817822462595, 此时loss是: 53.78778584923361\n",
      "在第941步，我们获得了函数 f(rm) = 4.582805262357228 * rm + -5.9227964994202695, 此时loss是: 53.78288694385353\n",
      "在第942步，我们获得了函数 f(rm) = 4.583892029552208 * rm + -5.929709553796589, 此时loss是: 53.77799039429191\n",
      "在第943步，我们获得了函数 f(rm) = 4.584978535409821 * rm + -5.936620945775075, 此时loss是: 53.77309619941585\n",
      "在第944步，我们获得了函数 f(rm) = 4.5860647799929115 * rm + -5.943530675755487, 此时loss是: 53.768204358093\n",
      "在第945步，我们获得了函数 f(rm) = 4.587150763364308 * rm + -5.95043874413749, 此时loss是: 53.76331486919159\n",
      "在第946步，我们获得了函数 f(rm) = 4.588236485586825 * rm + -5.957345151320653, 此时loss是: 53.75842773158036\n",
      "在第947步，我们获得了函数 f(rm) = 4.5893219467232615 * rm + -5.964249897704447, 此时loss是: 53.75354294412861\n",
      "在第948步，我们获得了函数 f(rm) = 4.590407146836402 * rm + -5.971152983688249, 此时loss是: 53.748660505706184\n",
      "在第949步，我们获得了函数 f(rm) = 4.591492085989015 * rm + -5.978054409671339, 此时loss是: 53.743780415183444\n",
      "在第950步，我们获得了函数 f(rm) = 4.592576764243853 * rm + -5.984954176052901, 此时loss是: 53.73890267143135\n",
      "在第951步，我们获得了函数 f(rm) = 4.593661181663657 * rm + -5.991852283232023, 此时loss是: 53.734027273321345\n",
      "在第952步，我们获得了函数 f(rm) = 4.5947453383111485 * rm + -5.998748731607698, 此时loss是: 53.72915421972545\n",
      "在第953步，我们获得了函数 f(rm) = 4.595829234249037 * rm + -6.005643521578821, 此时loss是: 53.72428350951623\n",
      "在第954步，我们获得了函数 f(rm) = 4.5969128695400165 * rm + -6.012536653544193, 此时loss是: 53.719415141566756\n",
      "在第955步，我们获得了函数 f(rm) = 4.597996244246764 * rm + -6.019428127902518, 此时loss是: 53.71454911475068\n",
      "在第956步，我们获得了函数 f(rm) = 4.599079358431944 * rm + -6.026317945052406, 此时loss是: 53.70968542794219\n",
      "在第957步，我们获得了函数 f(rm) = 4.600162212158204 * rm + -6.033206105392368, 此时loss是: 53.70482408001601\n",
      "在第958步，我们获得了函数 f(rm) = 4.601244805488177 * rm + -6.04009260932082, 此时loss是: 53.69996506984737\n",
      "在第959步，我们获得了函数 f(rm) = 4.602327138484482 * rm + -6.0469774572360855, 此时loss是: 53.69510839631211\n",
      "在第960步，我们获得了函数 f(rm) = 4.603409211209722 * rm + -6.0538606495363885, 此时loss是: 53.690254058286556\n",
      "在第961步，我们获得了函数 f(rm) = 4.604491023726483 * rm + -6.0607421866198585, 此时loss是: 53.685402054647604\n",
      "在第962步，我们获得了函数 f(rm) = 4.60557257609734 * rm + -6.067622068884529, 此时loss是: 53.68055238427266\n",
      "在第963步，我们获得了函数 f(rm) = 4.606653868384851 * rm + -6.074500296728338, 此时loss是: 53.6757050460397\n",
      "在第964步，我们获得了函数 f(rm) = 4.607734900651557 * rm + -6.0813768705491285, 此时loss是: 53.67086003882724\n",
      "在第965步，我们获得了函数 f(rm) = 4.608815672959987 * rm + -6.088251790744646, 此时loss是: 53.666017361514314\n",
      "在第966步，我们获得了函数 f(rm) = 4.609896185372653 * rm + -6.0951250577125435, 此时loss是: 53.66117701298048\n",
      "在第967步，我们获得了函数 f(rm) = 4.610976437952055 * rm + -6.101996671850374, 此时loss是: 53.656338992105894\n",
      "在第968步，我们获得了函数 f(rm) = 4.612056430760672 * rm + -6.108866633555599, 此时loss是: 53.65150329777121\n",
      "在第969步，我们获得了函数 f(rm) = 4.613136163860975 * rm + -6.115734943225582, 此时loss是: 53.64666992885759\n",
      "在第970步，我们获得了函数 f(rm) = 4.614215637315415 * rm + -6.122601601257592, 此时loss是: 53.64183888424682\n",
      "在第971步，我们获得了函数 f(rm) = 4.61529485118643 * rm + -6.129466608048801, 此时loss是: 53.637010162821156\n",
      "在第972步，我们获得了函数 f(rm) = 4.616373805536443 * rm + -6.136329963996289, 此时loss是: 53.632183763463395\n",
      "在第973步，我们获得了函数 f(rm) = 4.617452500427861 * rm + -6.143191669497036, 此时loss是: 53.62735968505689\n",
      "在第974步，我们获得了函数 f(rm) = 4.618530935923077 * rm + -6.15005172494793, 此时loss是: 53.62253792648555\n",
      "在第975步，我们获得了函数 f(rm) = 4.619609112084468 * rm + -6.156910130745761, 此时loss是: 53.61771848663374\n",
      "在第976步，我们获得了函数 f(rm) = 4.620687028974396 * rm + -6.163766887287226, 此时loss是: 53.61290136438649\n",
      "在第977步，我们获得了函数 f(rm) = 4.621764686655211 * rm + -6.170621994968924, 此时loss是: 53.60808655862926\n",
      "在第978步，我们获得了函数 f(rm) = 4.6228420851892436 * rm + -6.177475454187361, 此时loss是: 53.60327406824807\n",
      "在第979步，我们获得了函数 f(rm) = 4.623919224638811 * rm + -6.184327265338947, 此时loss是: 53.59846389212952\n",
      "在第980步，我们获得了函数 f(rm) = 4.624996105066218 * rm + -6.191177428819995, 此时loss是: 53.59365602916067\n",
      "在第981步，我们获得了函数 f(rm) = 4.626072726533751 * rm + -6.198025945026726, 此时loss是: 53.58885047822918\n",
      "在第982步，我们获得了函数 f(rm) = 4.627149089103682 * rm + -6.204872814355262, 此时loss是: 53.58404723822323\n",
      "在第983步，我们获得了函数 f(rm) = 4.62822519283827 * rm + -6.211718037201633, 此时loss是: 53.579246308031514\n",
      "在第984步，我们获得了函数 f(rm) = 4.629301037799758 * rm + -6.218561613961771, 此时loss是: 53.57444768654327\n",
      "在第985步，我们获得了函数 f(rm) = 4.630376624050371 * rm + -6.225403545031515, 此时loss是: 53.56965137264829\n",
      "在第986步，我们获得了函数 f(rm) = 4.631451951652325 * rm + -6.232243830806607, 此时loss是: 53.56485736523686\n",
      "在第987步，我们获得了函数 f(rm) = 4.6325270206678155 * rm + -6.239082471682695, 此时loss是: 53.560065663199836\n",
      "在第988步，我们获得了函数 f(rm) = 4.6336018311590275 * rm + -6.245919468055333, 此时loss是: 53.5552762654286\n",
      "在第989步，我们获得了函数 f(rm) = 4.634676383188127 * rm + -6.252754820319977, 此时loss是: 53.550489170815055\n",
      "在第990步，我们获得了函数 f(rm) = 4.635750676817268 * rm + -6.25958852887199, 此时loss是: 53.54570437825164\n",
      "在第991步，我们获得了函数 f(rm) = 4.636824712108588 * rm + -6.266420594106638, 此时loss是: 53.54092188663133\n",
      "在第992步，我们获得了函数 f(rm) = 4.63789848912421 * rm + -6.273251016419095, 此时loss是: 53.53614169484763\n",
      "在第993步，我们获得了函数 f(rm) = 4.6389720079262435 * rm + -6.280079796204436, 此时loss是: 53.5313638017946\n",
      "在第994步，我们获得了函数 f(rm) = 4.64004526857678 * rm + -6.286906933857646, 此时loss是: 53.526588206366775\n",
      "在第995步，我们获得了函数 f(rm) = 4.641118271137899 * rm + -6.293732429773611, 此时loss是: 53.5218149074593\n",
      "在第996步，我们获得了函数 f(rm) = 4.642191015671663 * rm + -6.300556284347123, 此时loss是: 53.51704390396775\n",
      "在第997步，我们获得了函数 f(rm) = 4.643263502240121 * rm + -6.307378497972879, 此时loss是: 53.51227519478835\n",
      "在第998步，我们获得了函数 f(rm) = 4.644335730905306 * rm + -6.314199071045482, 此时loss是: 53.50750877881777\n",
      "在第999步，我们获得了函数 f(rm) = 4.645407701729237 * rm + -6.3210180039594395, 此时loss是: 53.502744654953226\n",
      "在第1000步，我们获得了函数 f(rm) = 4.646479414773917 * rm + -6.327835297109164, 此时loss是: 53.4979828220925\n",
      "在第1001步，我们获得了函数 f(rm) = 4.647550870101336 * rm + -6.334650950888974, 此时loss是: 53.49322327913386\n",
      "在第1002步，我们获得了函数 f(rm) = 4.648622067773466 * rm + -6.341464965693091, 此时loss是: 53.488466024976134\n",
      "在第1003步，我们获得了函数 f(rm) = 4.649693007852268 * rm + -6.348277341915645, 此时loss是: 53.483711058518665\n",
      "在第1004步，我们获得了函数 f(rm) = 4.6507636903996845 * rm + -6.355088079950668, 此时loss是: 53.478958378661304\n",
      "在第1005步，我们获得了函数 f(rm) = 4.651834115477646 * rm + -6.361897180192099, 此时loss是: 53.474207984304506\n",
      "在第1006步，我们获得了函数 f(rm) = 4.652904283148065 * rm + -6.368704643033783, 此时loss是: 53.46945987434914\n",
      "在第1007步，我们获得了函数 f(rm) = 4.653974193472843 * rm + -6.375510468869468, 此时loss是: 53.46471404769673\n",
      "在第1008步，我们获得了函数 f(rm) = 4.655043846513863 * rm + -6.382314658092809, 此时loss是: 53.45997050324924\n",
      "在第1009步，我们获得了函数 f(rm) = 4.656113242332995 * rm + -6.389117211097365, 此时loss是: 53.45522923990918\n",
      "在第1010步，我们获得了函数 f(rm) = 4.657182380992094 * rm + -6.395918128276603, 此时loss是: 53.45049025657959\n",
      "在第1011步，我们获得了函数 f(rm) = 4.658251262552999 * rm + -6.402717410023892, 此时loss是: 53.44575355216405\n",
      "在第1012步，我们获得了函数 f(rm) = 4.659319887077536 * rm + -6.409515056732509, 此时loss是: 53.4410191255667\n",
      "在第1013步，我们获得了函数 f(rm) = 4.660388254627514 * rm + -6.416311068795635, 此时loss是: 53.43628697569211\n",
      "在第1014步，我们获得了函数 f(rm) = 4.66145636526473 * rm + -6.423105446606358, 此时loss是: 53.43155710144547\n",
      "在第1015步，我们获得了函数 f(rm) = 4.662524219050963 * rm + -6.429898190557668, 此时loss是: 53.426829501732435\n",
      "在第1016步，我们获得了函数 f(rm) = 4.663591816047979 * rm + -6.436689301042465, 此时loss是: 53.42210417545923\n",
      "在第1017步，我们获得了函数 f(rm) = 4.664659156317527 * rm + -6.443478778453551, 此时loss是: 53.4173811215326\n",
      "在第1018步，我们获得了函数 f(rm) = 4.665726239921346 * rm + -6.450266623183636, 此时loss是: 53.41266033885976\n",
      "在第1019步，我们获得了函数 f(rm) = 4.666793066921155 * rm + -6.457052835625334, 此时loss是: 53.407941826348555\n",
      "在第1020步，我们获得了函数 f(rm) = 4.667859637378661 * rm + -6.463837416171166, 此时loss是: 53.40322558290723\n",
      "在第1021步，我们获得了函数 f(rm) = 4.668925951355554 * rm + -6.470620365213556, 此时loss是: 53.39851160744467\n",
      "在第1022步，我们获得了函数 f(rm) = 4.669992008913512 * rm + -6.477401683144835, 此时loss是: 53.3937998988702\n",
      "在第1023步，我们获得了函数 f(rm) = 4.6710578101141955 * rm + -6.484181370357243, 此时loss是: 53.389090456093726\n",
      "在第1024步，我们获得了函数 f(rm) = 4.672123355019252 * rm + -6.49095942724292, 此时loss是: 53.384383278025666\n",
      "在第1025步，我们获得了函数 f(rm) = 4.673188643690313 * rm + -6.497735854193915, 此时loss是: 53.37967836357691\n",
      "在第1026步，我们获得了函数 f(rm) = 4.674253676188997 * rm + -6.504510651602183, 此时loss是: 53.37497571165895\n",
      "在第1027步，我们获得了函数 f(rm) = 4.675318452576904 * rm + -6.511283819859583, 此时loss是: 53.37027532118374\n",
      "在第1028步，我们获得了函数 f(rm) = 4.676382972915623 * rm + -6.51805535935788, 此时loss是: 53.365577191063814\n",
      "在第1029步，我们获得了函数 f(rm) = 4.677447237266727 * rm + -6.524825270488746, 此时loss是: 53.36088132021217\n",
      "在第1030步，我们获得了函数 f(rm) = 4.678511245691773 * rm + -6.531593553643758, 此时loss是: 53.35618770754237\n",
      "在第1031步，我们获得了函数 f(rm) = 4.679574998252305 * rm + -6.5383602092144, 此时loss是: 53.35149635196847\n",
      "在第1032步，我们获得了函数 f(rm) = 4.680638495009851 * rm + -6.54512523759206, 此时loss是: 53.34680725240508\n",
      "在第1033步，我们获得了函数 f(rm) = 4.681701736025924 * rm + -6.551888639168033, 此时loss是: 53.34212040776729\n",
      "在第1034步，我们获得了函数 f(rm) = 4.682764721362023 * rm + -6.55865041433352, 此时loss是: 53.33743581697077\n",
      "在第1035步，我们获得了函数 f(rm) = 4.683827451079632 * rm + -6.5654105634796265, 此时loss是: 53.33275347893164\n",
      "在第1036步，我们获得了函数 f(rm) = 4.684889925240221 * rm + -6.572169086997366, 此时loss是: 53.32807339256662\n",
      "在第1037步，我们获得了函数 f(rm) = 4.685952143905244 * rm + -6.578925985277658, 此时loss是: 53.32339555679288\n",
      "在第1038步，我们获得了函数 f(rm) = 4.687014107136139 * rm + -6.5856812587113245, 此时loss是: 53.31871997052814\n",
      "在第1039步，我们获得了函数 f(rm) = 4.688075814994332 * rm + -6.592434907689098, 此时loss是: 53.31404663269067\n",
      "在第1040步，我们获得了函数 f(rm) = 4.689137267541234 * rm + -6.599186932601614, 此时loss是: 53.3093755421992\n",
      "在第1041步，我们获得了函数 f(rm) = 4.690198464838239 * rm + -6.6059373338394165, 此时loss是: 53.30470669797302\n",
      "在第1042步，我们获得了函数 f(rm) = 4.6912594069467275 * rm + -6.612686111792953, 此时loss是: 53.300040098931944\n",
      "在第1043步，我们获得了函数 f(rm) = 4.692320093928067 * rm + -6.619433266852578, 此时loss是: 53.295375743996274\n",
      "在第1044步，我们获得了函数 f(rm) = 4.693380525843605 * rm + -6.626178799408554, 此时loss是: 53.29071363208687\n",
      "在第1045步，我们获得了函数 f(rm) = 4.694440702754681 * rm + -6.632922709851047, 此时loss是: 53.286053762125064\n",
      "在第1046步，我们获得了函数 f(rm) = 4.695500624722617 * rm + -6.639664998570131, 此时loss是: 53.28139613303275\n",
      "在第1047步，我们获得了函数 f(rm) = 4.696560291808717 * rm + -6.646405665955785, 此时loss是: 53.27674074373232\n",
      "在第1048步，我们获得了函数 f(rm) = 4.697619704074273 * rm + -6.6531447123978955, 此时loss是: 53.27208759314669\n",
      "在第1049步，我们获得了函数 f(rm) = 4.698678861580564 * rm + -6.659882138286253, 此时loss是: 53.267436680199296\n",
      "在第1050步，我们获得了函数 f(rm) = 4.6997377643888525 * rm + -6.666617944010557, 此时loss是: 53.26278800381408\n",
      "在第1051步，我们获得了函数 f(rm) = 4.700796412560386 * rm + -6.673352129960412, 此时loss是: 53.25814156291552\n",
      "在第1052步，我们获得了函数 f(rm) = 4.701854806156398 * rm + -6.680084696525328, 此时loss是: 53.25349735642857\n",
      "在第1053步，我们获得了函数 f(rm) = 4.702912945238104 * rm + -6.686815644094723, 此时loss是: 53.24885538327876\n",
      "在第1054步，我们获得了函数 f(rm) = 4.703970829866711 * rm + -6.693544973057919, 此时loss是: 53.244215642392106\n",
      "在第1055步，我们获得了函数 f(rm) = 4.705028460103407 * rm + -6.7002726838041475, 此时loss是: 53.23957813269514\n",
      "在第1056步，我们获得了函数 f(rm) = 4.7060858360093665 * rm + -6.7069987767225445, 此时loss是: 53.2349428531149\n",
      "在第1057步，我们获得了函数 f(rm) = 4.707142957645748 * rm + -6.713723252202152, 此时loss是: 53.230309802578965\n",
      "在第1058步，我们获得了函数 f(rm) = 4.708199825073697 * rm + -6.72044611063192, 此时loss是: 53.22567898001541\n",
      "在第1059步，我们获得了函数 f(rm) = 4.709256438354343 * rm + -6.727167352400704, 此时loss是: 53.22105038435285\n",
      "在第1060步，我们获得了函数 f(rm) = 4.710312797548801 * rm + -6.733886977897265, 此时loss是: 53.21642401452036\n",
      "在第1061步，我们获得了函数 f(rm) = 4.711368902718173 * rm + -6.740604987510274, 此时loss是: 53.211799869447624\n",
      "在第1062步，我们获得了函数 f(rm) = 4.712424753923544 * rm + -6.7473213816283035, 此时loss是: 53.207177948064746\n",
      "在第1063步，我们获得了函数 f(rm) = 4.713480351225987 * rm + -6.754036160639837, 此时loss是: 53.20255824930238\n",
      "在第1064步，我们获得了函数 f(rm) = 4.714535694686556 * rm + -6.760749324933262, 此时loss是: 53.19794077209172\n",
      "在第1065步，我们获得了函数 f(rm) = 4.715590784366294 * rm + -6.767460874896874, 此时loss是: 53.193325515364414\n",
      "在第1066步，我们获得了函数 f(rm) = 4.716645620326229 * rm + -6.774170810918874, 此时loss是: 53.18871247805271\n",
      "在第1067步，我们获得了函数 f(rm) = 4.717700202627373 * rm + -6.780879133387371, 此时loss是: 53.18410165908929\n",
      "在第1068步，我们获得了函数 f(rm) = 4.718754531330725 * rm + -6.787585842690379, 此时loss是: 53.17949305740739\n",
      "在第1069步，我们获得了函数 f(rm) = 4.719808606497265 * rm + -6.79429093921582, 此时loss是: 53.17488667194073\n",
      "在第1070步，我们获得了函数 f(rm) = 4.720862428187965 * rm + -6.800994423351522, 此时loss是: 53.170282501623596\n",
      "在第1071步，我们获得了函数 f(rm) = 4.721915996463778 * rm + -6.80769629548522, 此时loss是: 53.16568054539073\n",
      "在第1072步，我们获得了函数 f(rm) = 4.722969311385643 * rm + -6.8143965560045565, 此时loss是: 53.161080802177395\n",
      "在第1073步，我们获得了函数 f(rm) = 4.724022373014483 * rm + -6.8210952052970795, 此时loss是: 53.15648327091941\n",
      "在第1074步，我们获得了函数 f(rm) = 4.725075181411211 * rm + -6.827792243750244, 此时loss是: 53.15188795055306\n",
      "在第1075步，我们获得了函数 f(rm) = 4.72612773663672 * rm + -6.834487671751412, 此时loss是: 53.14729484001516\n",
      "在第1076步，我们获得了函数 f(rm) = 4.727180038751891 * rm + -6.841181489687854, 此时loss是: 53.142703938243024\n",
      "在第1077步，我们获得了函数 f(rm) = 4.728232087817591 * rm + -6.847873697946744, 此时loss是: 53.13811524417449\n",
      "在第1078步，我们获得了函数 f(rm) = 4.729283883894671 * rm + -6.854564296915166, 此时loss是: 53.13352875674791\n",
      "在第1079步，我们获得了函数 f(rm) = 4.730335427043967 * rm + -6.8612532869801095, 此时loss是: 53.12894447490214\n",
      "在第1080步，我们获得了函数 f(rm) = 4.731386717326302 * rm + -6.867940668528472, 此时loss是: 53.12436239757652\n",
      "在第1081步，我们获得了函数 f(rm) = 4.732437754802483 * rm + -6.874626441947055, 此时loss是: 53.119782523710974\n",
      "在第1082步，我们获得了函数 f(rm) = 4.733488539533303 * rm + -6.881310607622571, 此时loss是: 53.11520485224583\n",
      "在第1083步，我们获得了函数 f(rm) = 4.73453907157954 * rm + -6.887993165941637, 此时loss是: 53.110629382122035\n",
      "在第1084步，我们获得了函数 f(rm) = 4.735589351001958 * rm + -6.894674117290778, 此时loss是: 53.10605611228094\n",
      "在第1085步，我们获得了函数 f(rm) = 4.736639377861307 * rm + -6.901353462056425, 此时loss是: 53.10148504166452\n",
      "在第1086步，我们获得了函数 f(rm) = 4.737689152218319 * rm + -6.908031200624918, 此时loss是: 53.09691616921515\n",
      "在第1087步，我们获得了函数 f(rm) = 4.738738674133716 * rm + -6.914707333382501, 此时loss是: 53.09234949387577\n",
      "在第1088步，我们获得了函数 f(rm) = 4.739787943668202 * rm + -6.921381860715328, 此时loss是: 53.08778501458984\n",
      "在第1089步，我们获得了函数 f(rm) = 4.740836960882468 * rm + -6.9280547830094585, 此时loss是: 53.08322273030129\n",
      "在第1090步，我们获得了函数 f(rm) = 4.741885725837189 * rm + -6.934726100650861, 此时loss是: 53.078662639954565\n",
      "在第1091步，我们获得了函数 f(rm) = 4.742934238593029 * rm + -6.94139581402541, 此时loss是: 53.07410474249465\n",
      "在第1092步，我们获得了函数 f(rm) = 4.743982499210633 * rm + -6.948063923518886, 此时loss是: 53.069549036867\n",
      "在第1093步，我们获得了函数 f(rm) = 4.7450305077506325 * rm + -6.954730429516978, 此时loss是: 53.0649955220176\n",
      "在第1094步，我们获得了函数 f(rm) = 4.746078264273646 * rm + -6.961395332405283, 此时loss是: 53.06044419689292\n",
      "在第1095步，我们获得了函数 f(rm) = 4.747125768840278 * rm + -6.968058632569305, 此时loss是: 53.05589506043997\n",
      "在第1096步，我们获得了函数 f(rm) = 4.748173021511115 * rm + -6.974720330394453, 此时loss是: 53.05134811160624\n",
      "在第1097步，我们获得了函数 f(rm) = 4.749220022346733 * rm + -6.981380426266045, 此时loss是: 53.04680334933972\n",
      "在第1098步，我们获得了函数 f(rm) = 4.750266771407689 * rm + -6.988038920569308, 此时loss是: 53.04226077258893\n",
      "在第1099步，我们获得了函数 f(rm) = 4.7513132687545285 * rm + -6.994695813689374, 此时loss是: 53.03772038030291\n",
      "在第1100步，我们获得了函数 f(rm) = 4.752359514447782 * rm + -7.001351106011283, 此时loss是: 53.033182171431115\n",
      "在第1101步，我们获得了函数 f(rm) = 4.753405508547966 * rm + -7.008004797919982, 此时loss是: 53.028646144923634\n",
      "在第1102步，我们获得了函数 f(rm) = 4.754451251115581 * rm + -7.014656889800327, 此时loss是: 53.024112299730966\n",
      "在第1103步，我们获得了函数 f(rm) = 4.755496742211115 * rm + -7.02130738203708, 此时loss是: 53.01958063480416\n",
      "在第1104步，我们获得了函数 f(rm) = 4.756541981895037 * rm + -7.027956275014912, 此时loss是: 53.015051149094745\n",
      "在第1105步，我们获得了函数 f(rm) = 4.757586970227806 * rm + -7.034603569118398, 此时loss是: 53.01052384155474\n",
      "在第1106步，我们获得了函数 f(rm) = 4.758631707269867 * rm + -7.041249264732025, 此时loss是: 53.00599871113674\n",
      "在第1107步，我们获得了函数 f(rm) = 4.759676193081646 * rm + -7.047893362240186, 此时loss是: 53.001475756793795\n",
      "在第1108步，我们获得了函数 f(rm) = 4.760720427723556 * rm + -7.054535862027179, 此时loss是: 52.996954977479426\n",
      "在第1109步，我们获得了函数 f(rm) = 4.761764411255999 * rm + -7.061176764477213, 此时loss是: 52.99243637214768\n",
      "在第1110步，我们获得了函数 f(rm) = 4.76280814373936 * rm + -7.067816069974402, 此时loss是: 52.987919939753155\n",
      "在第1111步，我们获得了函数 f(rm) = 4.763851625234007 * rm + -7.07445377890277, 此时loss是: 52.983405679250914\n",
      "在第1112步，我们获得了函数 f(rm) = 4.764894855800297 * rm + -7.081089891646247, 此时loss是: 52.97889358959649\n",
      "在第1113步，我们获得了函数 f(rm) = 4.765937835498572 * rm + -7.087724408588672, 此时loss是: 52.97438366974598\n",
      "在第1114步，我们获得了函数 f(rm) = 4.766980564389157 * rm + -7.094357330113789, 此时loss是: 52.969875918655944\n",
      "在第1115步，我们获得了函数 f(rm) = 4.768023042532366 * rm + -7.100988656605254, 此时loss是: 52.96537033528346\n",
      "在第1116步，我们获得了函数 f(rm) = 4.769065269988497 * rm + -7.107618388446626, 此时loss是: 52.96086691858609\n",
      "在第1117步，我们获得了函数 f(rm) = 4.770107246817831 * rm + -7.114246526021376, 此时loss是: 52.95636566752193\n",
      "在第1118步，我们获得了函数 f(rm) = 4.771148973080639 * rm + -7.12087306971288, 此时loss是: 52.95186658104952\n",
      "在第1119步，我们获得了函数 f(rm) = 4.772190448837175 * rm + -7.127498019904424, 此时loss是: 52.94736965812797\n",
      "在第1120步，我们获得了函数 f(rm) = 4.773231674147678 * rm + -7.1341213769791985, 此时loss是: 52.94287489771684\n",
      "在第1121步，我们获得了函数 f(rm) = 4.7742726490723735 * rm + -7.140743141320305, 此时loss是: 52.938382298776226\n",
      "在第1122步，我们获得了函数 f(rm) = 4.775313373671473 * rm + -7.147363313310752, 此时loss是: 52.93389186026668\n",
      "在第1123步，我们获得了函数 f(rm) = 4.776353848005172 * rm + -7.153981893333455, 此时loss是: 52.92940358114929\n",
      "在第1124步，我们获得了函数 f(rm) = 4.777394072133653 * rm + -7.1605988817712385, 此时loss是: 52.924917460385636\n",
      "在第1125步，我们获得了函数 f(rm) = 4.778434046117083 * rm + -7.167214279006834, 此时loss是: 52.9204334969378\n",
      "在第1126步，我们获得了函数 f(rm) = 4.779473770015615 * rm + -7.173828085422882, 此时loss是: 52.915951689768335\n",
      "在第1127步，我们获得了函数 f(rm) = 4.780513243889388 * rm + -7.180440301401931, 此时loss是: 52.91147203784035\n",
      "在第1128步，我们获得了函数 f(rm) = 4.781552467798526 * rm + -7.187050927326436, 此时loss是: 52.90699454011739\n",
      "在第1129步，我们获得了函数 f(rm) = 4.782591441803137 * rm + -7.193659963578762, 此时loss是: 52.90251919556352\n",
      "在第1130步，我们获得了函数 f(rm) = 4.783630165963318 * rm + -7.20026741054118, 此时loss是: 52.89804600314334\n",
      "在第1131步，我们获得了函数 f(rm) = 4.78466864033915 * rm + -7.20687326859587, 此时loss是: 52.893574961821905\n",
      "在第1132步，我们获得了函数 f(rm) = 4.785706864990697 * rm + -7.213477538124921, 此时loss是: 52.889106070564765\n",
      "在第1133步，我们获得了函数 f(rm) = 4.786744839978012 * rm + -7.220080219510329, 此时loss是: 52.88463932833801\n",
      "在第1134步，我们获得了函数 f(rm) = 4.787782565361133 * rm + -7.226681313134, 此时loss是: 52.88017473410818\n",
      "在第1135步，我们获得了函数 f(rm) = 4.788820041200082 * rm + -7.233280819377744, 此时loss是: 52.87571228684233\n",
      "在第1136步，我们获得了函数 f(rm) = 4.789857267554867 * rm + -7.2398787386232835, 此时loss是: 52.87125198550803\n",
      "在第1137步，我们获得了函数 f(rm) = 4.790894244485483 * rm + -7.246475071252247, 此时loss是: 52.866793829073316\n",
      "在第1138步，我们获得了函数 f(rm) = 4.791930972051909 * rm + -7.253069817646172, 此时loss是: 52.86233781650674\n",
      "在第1139步，我们获得了函数 f(rm) = 4.79296745031411 * rm + -7.259662978186505, 此时loss是: 52.85788394677734\n",
      "在第1140步，我们获得了函数 f(rm) = 4.794003679332039 * rm + -7.266254553254598, 此时loss是: 52.85343221885466\n",
      "在第1141步，我们获得了函数 f(rm) = 4.795039659165629 * rm + -7.272844543231714, 此时loss是: 52.84898263170872\n",
      "在第1142步，我们获得了函数 f(rm) = 4.796075389874804 * rm + -7.279432948499023, 此时loss是: 52.84453518431008\n",
      "在第1143步，我们获得了函数 f(rm) = 4.79711087151947 * rm + -7.286019769437605, 此时loss是: 52.840089875629744\n",
      "在第1144步，我们获得了函数 f(rm) = 4.798146104159522 * rm + -7.292605006428445, 此时loss是: 52.83564670463921\n",
      "在第1145步，我们获得了函数 f(rm) = 4.799181087854837 * rm + -7.299188659852441, 此时loss是: 52.83120567031054\n",
      "在第1146步，我们获得了函数 f(rm) = 4.8002158226652805 * rm + -7.305770730090395, 此时loss是: 52.826766771616185\n",
      "在第1147步，我们获得了函数 f(rm) = 4.801250308650702 * rm + -7.3123512175230205, 此时loss是: 52.82233000752919\n",
      "在第1148步，我们获得了函数 f(rm) = 4.802284545870936 * rm + -7.318930122530937, 此时loss是: 52.817895377023035\n",
      "在第1149步，我们获得了函数 f(rm) = 4.803318534385806 * rm + -7.325507445494675, 此时loss是: 52.8134628790717\n",
      "在第1150步，我们获得了函数 f(rm) = 4.804352274255117 * rm + -7.332083186794671, 此时loss是: 52.8090325126497\n",
      "在第1151步，我们获得了函数 f(rm) = 4.805385765538661 * rm + -7.338657346811273, 此时loss是: 52.80460427673198\n",
      "在第1152步，我们获得了函数 f(rm) = 4.806419008296217 * rm + -7.345229925924734, 此时loss是: 52.800178170294004\n",
      "在第1153步，我们获得了函数 f(rm) = 4.8074520025875485 * rm + -7.351800924515219, 此时loss是: 52.795754192311755\n",
      "在第1154步，我们获得了函数 f(rm) = 4.808484748472405 * rm + -7.3583703429627985, 此时loss是: 52.7913323417617\n",
      "在第1155步，我们获得了函数 f(rm) = 4.80951724601052 * rm + -7.364938181647454, 此时loss是: 52.78691261762074\n",
      "在第1156步，我们获得了函数 f(rm) = 4.810549495261615 * rm + -7.371504440949074, 此时loss是: 52.78249501886635\n",
      "在第1157步，我们获得了函数 f(rm) = 4.811581496285396 * rm + -7.378069121247457, 此时loss是: 52.77807954447645\n",
      "在第1158步，我们获得了函数 f(rm) = 4.812613249141555 * rm + -7.384632222922309, 此时loss是: 52.773666193429456\n",
      "在第1159步，我们获得了函数 f(rm) = 4.81364475388977 * rm + -7.391193746353245, 此时loss是: 52.76925496470428\n",
      "在第1160步，我们获得了函数 f(rm) = 4.814676010589703 * rm + -7.39775369191979, 此时loss是: 52.76484585728033\n",
      "在第1161步，我们获得了函数 f(rm) = 4.815707019301002 * rm + -7.4043120600013745, 此时loss是: 52.760438870137506\n",
      "在第1162步，我们获得了函数 f(rm) = 4.8167377800833036 * rm + -7.410868850977342, 此时loss是: 52.756034002256186\n",
      "在第1163步，我们获得了函数 f(rm) = 4.8177682929962256 * rm + -7.417424065226942, 此时loss是: 52.75163125261724\n",
      "在第1164步，我们获得了函数 f(rm) = 4.818798558099376 * rm + -7.423977703129332, 此时loss是: 52.74723062020205\n",
      "在第1165步，我们获得了函数 f(rm) = 4.819828575452344 * rm + -7.430529765063582, 此时loss是: 52.74283210399245\n",
      "在第1166步，我们获得了函数 f(rm) = 4.820858345114708 * rm + -7.437080251408668, 此时loss是: 52.7384357029708\n",
      "在第1167步，我们获得了函数 f(rm) = 4.821887867146031 * rm + -7.443629162543475, 此时loss是: 52.73404141611993\n",
      "在第1168步，我们获得了函数 f(rm) = 4.82291714160586 * rm + -7.450176498846798, 此时loss是: 52.72964924242316\n",
      "在第1169步，我们获得了函数 f(rm) = 4.8239461685537295 * rm + -7.4567222606973385, 此时loss是: 52.725259180864306\n",
      "在第1170步，我们获得了函数 f(rm) = 4.82497494804916 * rm + -7.463266448473711, 此时loss是: 52.72087123042768\n",
      "在第1171步，我们获得了函数 f(rm) = 4.826003480151656 * rm + -7.469809062554435, 此时loss是: 52.71648539009805\n",
      "在第1172步，我们获得了函数 f(rm) = 4.827031764920711 * rm + -7.476350103317942, 此时loss是: 52.7121016588607\n",
      "在第1173步，我们获得了函数 f(rm) = 4.8280598024157975 * rm + -7.482889571142571, 此时loss是: 52.70772003570141\n",
      "在第1174步，我们获得了函数 f(rm) = 4.829087592696382 * rm + -7.4894274664065685, 此时loss是: 52.703340519606414\n",
      "在第1175步，我们获得了函数 f(rm) = 4.830115135821911 * rm + -7.495963789488094, 此时loss是: 52.69896310956248\n",
      "在第1176步，我们获得了函数 f(rm) = 4.831142431851819 * rm + -7.502498540765212, 此时loss是: 52.69458780455679\n",
      "在第1177步，我们获得了函数 f(rm) = 4.832169480845525 * rm + -7.5090317206159, 此时loss是: 52.69021460357711\n",
      "在第1178步，我们获得了函数 f(rm) = 4.833196282862435 * rm + -7.51556332941804, 此时loss是: 52.6858435056116\n",
      "在第1179步，我们获得了函数 f(rm) = 4.834222837961939 * rm + -7.522093367549426, 此时loss是: 52.681474509648964\n",
      "在第1180步，我们获得了函数 f(rm) = 4.835249146203416 * rm + -7.528621835387763, 此时loss是: 52.67710761467839\n",
      "在第1181步，我们获得了函数 f(rm) = 4.836275207646225 * rm + -7.535148733310661, 此时loss是: 52.672742819689546\n",
      "在第1182步，我们获得了函数 f(rm) = 4.837301022349718 * rm + -7.54167406169564, 此时loss是: 52.668380123672534\n",
      "在第1183步，我们获得了函数 f(rm) = 4.838326590373226 * rm + -7.548197820920133, 此时loss是: 52.66401952561802\n",
      "在第1184步，我们获得了函数 f(rm) = 4.839351911776071 * rm + -7.554720011361478, 此时loss是: 52.65966102451713\n",
      "在第1185步，我们获得了函数 f(rm) = 4.840376986617557 * rm + -7.561240633396924, 此时loss是: 52.65530461936144\n",
      "在第1186步，我们获得了函数 f(rm) = 4.841401814956976 * rm + -7.56775968740363, 此时loss是: 52.650950309143056\n",
      "在第1187步，我们获得了函数 f(rm) = 4.842426396853604 * rm + -7.574277173758662, 此时loss是: 52.64659809285454\n",
      "在第1188步，我们获得了函数 f(rm) = 4.843450732366704 * rm + -7.580793092838998, 此时loss是: 52.64224796948894\n",
      "在第1189步，我们获得了函数 f(rm) = 4.844474821555525 * rm + -7.587307445021523, 此时loss是: 52.63789993803984\n",
      "在第1190步，我们获得了函数 f(rm) = 4.8454986644793 * rm + -7.593820230683034, 此时loss是: 52.63355399750122\n",
      "在第1191步，我们获得了函数 f(rm) = 4.84652226119725 * rm + -7.600331450200234, 此时loss是: 52.6292101468676\n",
      "在第1192步，我们获得了函数 f(rm) = 4.84754561176858 * rm + -7.606841103949739, 此时loss是: 52.62486838513399\n",
      "在第1193步，我们获得了函数 f(rm) = 4.8485687162524815 * rm + -7.613349192308071, 此时loss是: 52.62052871129583\n",
      "在第1194步，我们获得了函数 f(rm) = 4.849591574708133 * rm + -7.619855715651664, 此时loss是: 52.61619112434913\n",
      "在第1195步，我们获得了函数 f(rm) = 4.8506141871946955 * rm + -7.626360674356862, 此时loss是: 52.611855623290296\n",
      "在第1196步，我们获得了函数 f(rm) = 4.851636553771319 * rm + -7.632864068799915, 此时loss是: 52.607522207116254\n",
      "在第1197步，我们获得了函数 f(rm) = 4.852658674497137 * rm + -7.639365899356985, 此时loss是: 52.60319087482441\n",
      "在第1198步，我们获得了函数 f(rm) = 4.853680549431271 * rm + -7.6458661664041445, 此时loss是: 52.598861625412674\n",
      "在第1199步，我们获得了函数 f(rm) = 4.854702178632826 * rm + -7.652364870317373, 此时loss是: 52.59453445787939\n",
      "在第1200步，我们获得了函数 f(rm) = 4.855723562160895 * rm + -7.658862011472562, 此时loss是: 52.59020937122344\n",
      "在第1201步，我们获得了函数 f(rm) = 4.856744700074555 * rm + -7.6653575902455096, 此时loss是: 52.58588636444412\n",
      "在第1202步，我们获得了函数 f(rm) = 4.857765592432869 * rm + -7.671851607011927, 此时loss是: 52.581565436541275\n",
      "在第1203步，我们获得了函数 f(rm) = 4.858786239294887 * rm + -7.678344062147433, 此时loss是: 52.57724658651518\n",
      "在第1204步，我们获得了函数 f(rm) = 4.859806640719643 * rm + -7.684834956027556, 此时loss是: 52.572929813366635\n",
      "在第1205步，我们获得了函数 f(rm) = 4.86082679676616 * rm + -7.691324289027736, 此时loss是: 52.568615116096886\n",
      "在第1206步，我们获得了函数 f(rm) = 4.861846707493443 * rm + -7.69781206152332, 此时loss是: 52.56430249370767\n",
      "在第1207步，我们获得了函数 f(rm) = 4.862866372960484 * rm + -7.704298273889566, 此时loss是: 52.55999194520119\n",
      "在第1208步，我们获得了函数 f(rm) = 4.863885793226263 * rm + -7.7107829265016425, 此时loss是: 52.555683469580174\n",
      "在第1209步，我们获得了函数 f(rm) = 4.864904968349743 * rm + -7.717266019734628, 此时loss是: 52.55137706584778\n",
      "在第1210步，我们获得了函数 f(rm) = 4.865923898389874 * rm + -7.723747553963508, 此时loss是: 52.54707273300766\n",
      "在第1211步，我们获得了函数 f(rm) = 4.866942583405591 * rm + -7.730227529563181, 此时loss是: 52.54277047006396\n",
      "在第1212步，我们获得了函数 f(rm) = 4.867961023455818 * rm + -7.736705946908454, 此时loss是: 52.53847027602128\n",
      "在第1213步，我们获得了函数 f(rm) = 4.86897921859946 * rm + -7.743182806374043, 此时loss是: 52.53417214988473\n",
      "在第1214步，我们获得了函数 f(rm) = 4.869997168895411 * rm + -7.749658108334576, 此时loss是: 52.52987609065988\n",
      "在第1215步，我们获得了函数 f(rm) = 4.87101487440255 * rm + -7.7561318531645895, 此时loss是: 52.52558209735277\n",
      "在第1216步，我们获得了函数 f(rm) = 4.8720323351797425 * rm + -7.7626040412385295, 此时loss是: 52.521290168969934\n",
      "在第1217步，我们获得了函数 f(rm) = 4.873049551285838 * rm + -7.769074672930754, 此时loss是: 52.51700030451838\n",
      "在第1218步，我们获得了函数 f(rm) = 4.874066522779675 * rm + -7.775543748615528, 此时loss是: 52.51271250300557\n",
      "在第1219步，我们获得了函数 f(rm) = 4.875083249720074 * rm + -7.782011268667029, 此时loss是: 52.5084267634395\n",
      "在第1220步，我们获得了函数 f(rm) = 4.876099732165844 * rm + -7.788477233459344, 此时loss是: 52.504143084828584\n",
      "在第1221步，我们获得了函数 f(rm) = 4.8771159701757805 * rm + -7.794941643366469, 此时loss是: 52.49986146618174\n",
      "在第1222步，我们获得了函数 f(rm) = 4.878131963808662 * rm + -7.801404498762311, 此时loss是: 52.495581906508335\n",
      "在第1223步，我们获得了函数 f(rm) = 4.879147713123255 * rm + -7.8078658000206875, 此时loss是: 52.49130440481828\n",
      "在第1224步，我们获得了函数 f(rm) = 4.880163218178311 * rm + -7.814325547515324, 此时loss是: 52.48702896012191\n",
      "在第1225步，我们获得了函数 f(rm) = 4.881178479032568 * rm + -7.8207837416198585, 此时loss是: 52.48275557143002\n",
      "在第1226步，我们获得了函数 f(rm) = 4.882193495744748 * rm + -7.8272403827078385, 此时loss是: 52.47848423775391\n",
      "在第1227步，我们获得了函数 f(rm) = 4.883208268373563 * rm + -7.833695471152721, 此时loss是: 52.47421495810537\n",
      "在第1228步，我们获得了函数 f(rm) = 4.884222796977706 * rm + -7.840149007327874, 此时loss是: 52.469947731496624\n",
      "在第1229步，我们获得了函数 f(rm) = 4.88523708161586 * rm + -7.846600991606574, 此时loss是: 52.4656825569404\n",
      "在第1230步，我们获得了函数 f(rm) = 4.886251122346691 * rm + -7.853051424362011, 此时loss是: 52.46141943344989\n",
      "在第1231步，我们获得了函数 f(rm) = 4.887264919228852 * rm + -7.859500305967282, 此时loss是: 52.457158360038775\n",
      "在第1232步，我们获得了函数 f(rm) = 4.888278472320982 * rm + -7.865947636795396, 此时loss是: 52.45289933572119\n",
      "在第1233步，我们获得了函数 f(rm) = 4.889291781681705 * rm + -7.872393417219272, 此时loss是: 52.44864235951177\n",
      "在第1234步，我们获得了函数 f(rm) = 4.890304847369633 * rm + -7.878837647611739, 此时loss是: 52.44438743042558\n",
      "在第1235步，我们获得了函数 f(rm) = 4.891317669443362 * rm + -7.885280328345536, 此时loss是: 52.44013454747819\n",
      "在第1236步，我们获得了函数 f(rm) = 4.892330247961474 * rm + -7.891721459793314, 此时loss是: 52.43588370968566\n",
      "在第1237步，我们获得了函数 f(rm) = 4.893342582982537 * rm + -7.898161042327633, 此时loss是: 52.431634916064475\n",
      "在第1238步，我们获得了函数 f(rm) = 4.894354674565108 * rm + -7.904599076320963, 此时loss是: 52.42738816563165\n",
      "在第1239步，我们获得了函数 f(rm) = 4.895366522767723 * rm + -7.9110355621456865, 此时loss是: 52.42314345740462\n",
      "在第1240步，我们获得了函数 f(rm) = 4.896378127648912 * rm + -7.917470500174094, 此时loss是: 52.41890079040131\n",
      "在第1241步，我们获得了函数 f(rm) = 4.897389489267185 * rm + -7.923903890778388, 此时loss是: 52.414660163640164\n",
      "在第1242步，我们获得了函数 f(rm) = 4.89840060768104 * rm + -7.930335734330681, 此时loss是: 52.410421576139996\n",
      "在第1243步，我们获得了函数 f(rm) = 4.8994114829489614 * rm + -7.936766031202996, 此时loss是: 52.40618502692021\n",
      "在第1244步，我们获得了函数 f(rm) = 4.90042211512942 * rm + -7.943194781767267, 此时loss是: 52.401950515000586\n",
      "在第1245步，我们获得了函数 f(rm) = 4.901432504280869 * rm + -7.949621986395338, 此时loss是: 52.397718039401425\n",
      "在第1246步，我们获得了函数 f(rm) = 4.902442650461753 * rm + -7.956047645458964, 此时loss是: 52.393487599143505\n",
      "在第1247步，我们获得了函数 f(rm) = 4.903452553730497 * rm + -7.96247175932981, 此时loss是: 52.38925919324801\n",
      "在第1248步，我们获得了函数 f(rm) = 4.904462214145516 * rm + -7.968894328379452, 此时loss是: 52.3850328207367\n",
      "在第1249步，我们获得了函数 f(rm) = 4.90547163176521 * rm + -7.9753153529793765, 此时loss是: 52.380808480631714\n",
      "在第1250步，我们获得了函数 f(rm) = 4.906480806647965 * rm + -7.9817348335009815, 此时loss是: 52.37658617195569\n",
      "在第1251步，我们获得了函数 f(rm) = 4.907489738852151 * rm + -7.988152770315574, 此时loss是: 52.37236589373177\n",
      "在第1252步，我们获得了函数 f(rm) = 4.9084984284361255 * rm + -7.994569163794374, 此时loss是: 52.368147644983516\n",
      "在第1253步，我们获得了函数 f(rm) = 4.909506875458233 * rm + -8.000984014308509, 此时loss是: 52.36393142473497\n",
      "在第1254步，我们获得了函数 f(rm) = 4.910515079976802 * rm + -8.007397322229021, 此时loss是: 52.359717232010674\n",
      "在第1255步，我们获得了函数 f(rm) = 4.911523042050149 * rm + -8.01380908792686, 此时loss是: 52.35550506583561\n",
      "在第1256步，我们获得了函数 f(rm) = 4.9125307617365745 * rm + -8.02021931177289, 此时loss是: 52.35129492523525\n",
      "在第1257步，我们获得了函数 f(rm) = 4.913538239094366 * rm + -8.02662799413788, 此时loss是: 52.347086809235506\n",
      "在第1258步，我们获得了函数 f(rm) = 4.914545474181796 * rm + -8.033035135392517, 此时loss是: 52.342880716862766\n",
      "在第1259步，我们获得了函数 f(rm) = 4.915552467057125 * rm + -8.039440735907393, 此时loss是: 52.33867664714391\n",
      "在第1260步，我们获得了函数 f(rm) = 4.916559217778598 * rm + -8.045844796053013, 此时loss是: 52.3344745991063\n",
      "在第1261步，我们获得了函数 f(rm) = 4.917565726404447 * rm + -8.052247316199793, 此时loss是: 52.330274571777665\n",
      "在第1262步，我们获得了函数 f(rm) = 4.918571992992887 * rm + -8.058648296718061, 此时loss是: 52.32607656418634\n",
      "在第1263步，我们获得了函数 f(rm) = 4.919578017602124 * rm + -8.065047737978055, 此时loss是: 52.32188057536103\n",
      "在第1264步，我们获得了函数 f(rm) = 4.920583800290345 * rm + -8.071445640349925, 此时loss是: 52.31768660433092\n",
      "在第1265步，我们获得了函数 f(rm) = 4.921589341115727 * rm + -8.07784200420373, 此时loss是: 52.31349465012572\n",
      "在第1266步，我们获得了函数 f(rm) = 4.92259464013643 * rm + -8.08423682990944, 此时loss是: 52.309304711775525\n",
      "在第1267步，我们获得了函数 f(rm) = 4.923599697410602 * rm + -8.090630117836936, 此时loss是: 52.30511678831097\n",
      "在第1268步，我们获得了函数 f(rm) = 4.924604512996377 * rm + -8.097021868356013, 此时loss是: 52.30093087876311\n",
      "在第1269步，我们获得了函数 f(rm) = 4.925609086951873 * rm + -8.103412081836375, 此时loss是: 52.296746982163484\n",
      "在第1270步，我们获得了函数 f(rm) = 4.9266134193351965 * rm + -8.109800758647637, 此时loss是: 52.29256509754408\n",
      "在第1271步，我们获得了函数 f(rm) = 4.927617510204438 * rm + -8.116187899159325, 此时loss是: 52.288385223937375\n",
      "在第1272步，我们获得了函数 f(rm) = 4.928621359617676 * rm + -8.122573503740876, 此时loss是: 52.2842073603763\n",
      "在第1273步，我们获得了函数 f(rm) = 4.929624967632972 * rm + -8.128957572761639, 此时loss是: 52.28003150589424\n",
      "在第1274步，我们获得了函数 f(rm) = 4.930628334308378 * rm + -8.135340106590874, 此时loss是: 52.27585765952506\n",
      "在第1275步，我们获得了函数 f(rm) = 4.9316314597019275 * rm + -8.141721105597751, 此时loss是: 52.271685820303105\n",
      "在第1276步，我们获得了函数 f(rm) = 4.932634343871643 * rm + -8.148100570151355, 此时loss是: 52.26751598726313\n",
      "在第1277步，我们获得了函数 f(rm) = 4.933636986875533 * rm + -8.154478500620677, 此时loss是: 52.263348159440405\n",
      "在第1278步，我们获得了函数 f(rm) = 4.93463938877159 * rm + -8.160854897374621, 此时loss是: 52.25918233587065\n",
      "在第1279步，我们获得了函数 f(rm) = 4.935641549617794 * rm + -8.167229760782005, 此时loss是: 52.25501851559005\n",
      "在第1280步，我们获得了函数 f(rm) = 4.936643469472111 * rm + -8.173603091211556, 此时loss是: 52.250856697635236\n",
      "在第1281步，我们获得了函数 f(rm) = 4.937645148392493 * rm + -8.17997488903191, 此时loss是: 52.246696881043356\n",
      "在第1282步，我们获得了函数 f(rm) = 4.938646586436876 * rm + -8.186345154611622, 此时loss是: 52.242539064851925\n",
      "在第1283步，我们获得了函数 f(rm) = 4.939647783663188 * rm + -8.19271388831915, 此时loss是: 52.238383248099\n",
      "在第1284步，我们获得了函数 f(rm) = 4.940648740129335 * rm + -8.199081090522867, 此时loss是: 52.23422942982308\n",
      "在第1285步，我们获得了函数 f(rm) = 4.941649455893216 * rm + -8.205446761591057, 此时loss是: 52.23007760906313\n",
      "在第1286步，我们获得了函数 f(rm) = 4.942649931012711 * rm + -8.211810901891917, 此时loss是: 52.22592778485858\n",
      "在第1287步，我们获得了函数 f(rm) = 4.94365016554569 * rm + -8.218173511793553, 此时loss是: 52.221779956249264\n",
      "在第1288步，我们获得了函数 f(rm) = 4.944650159550007 * rm + -8.224534591663984, 此时loss是: 52.21763412227558\n",
      "在第1289步，我们获得了函数 f(rm) = 4.945649913083502 * rm + -8.23089414187114, 此时loss是: 52.21349028197832\n",
      "在第1290步，我们获得了函数 f(rm) = 4.946649426204002 * rm + -8.237252162782863, 此时loss是: 52.20934843439875\n",
      "在第1291步，我们获得了函数 f(rm) = 4.947648698969319 * rm + -8.243608654766904, 此时loss是: 52.20520857857858\n",
      "在第1292步，我们获得了函数 f(rm) = 4.948647731437252 * rm + -8.24996361819093, 此时loss是: 52.20107071356003\n",
      "在第1293步，我们获得了函数 f(rm) = 4.949646523665587 * rm + -8.256317053422517, 此时loss是: 52.196934838385715\n",
      "在第1294步，我们获得了函数 f(rm) = 4.950645075712093 * rm + -8.262668960829153, 此时loss是: 52.19280095209878\n",
      "在第1295步，我们获得了函数 f(rm) = 4.951643387634529 * rm + -8.269019340778236, 此时loss是: 52.18866905374277\n",
      "在第1296步，我们获得了函数 f(rm) = 4.9526414594906365 * rm + -8.275368193637078, 此时loss是: 52.18453914236173\n",
      "在第1297步，我们获得了函数 f(rm) = 4.953639291338146 * rm + -8.281715519772902, 此时loss是: 52.180411217000156\n",
      "在第1298步，我们获得了函数 f(rm) = 4.954636883234773 * rm + -8.288061319552844, 此时loss是: 52.176285276702984\n",
      "在第1299步，我们获得了函数 f(rm) = 4.955634235238218 * rm + -8.294405593343948, 此时loss是: 52.17216132051561\n",
      "在第1300步，我们获得了函数 f(rm) = 4.9566313474061685 * rm + -8.300748341513172, 此时loss是: 52.16803934748393\n",
      "在第1301步，我们获得了函数 f(rm) = 4.9576282197963 * rm + -8.307089564427386, 此时loss是: 52.163919356654276\n",
      "在第1302步，我们获得了函数 f(rm) = 4.95862485246627 * rm + -8.313429262453374, 此时loss是: 52.159801347073405\n",
      "在第1303步，我们获得了函数 f(rm) = 4.959621245473728 * rm + -8.319767435957827, 此时loss是: 52.15568531778858\n",
      "在第1304步，我们获得了函数 f(rm) = 4.960617398876303 * rm + -8.32610408530735, 此时loss是: 52.15157126784748\n",
      "在第1305步，我们获得了函数 f(rm) = 4.961613312731614 * rm + -8.33243921086846, 此时loss是: 52.14745919629827\n",
      "在第1306步，我们获得了函数 f(rm) = 4.962608987097266 * rm + -8.338772813007587, 此时loss是: 52.14334910218961\n",
      "在第1307步，我们获得了函数 f(rm) = 4.9636044220308495 * rm + -8.34510489209107, 此时loss是: 52.13924098457052\n",
      "在第1308步，我们获得了函数 f(rm) = 4.964599617589942 * rm + -8.351435448485164, 此时loss是: 52.135134842490565\n",
      "在第1309步，我们获得了函数 f(rm) = 4.965594573832105 * rm + -8.357764482556032, 此时loss是: 52.13103067499972\n",
      "在第1310步，我们获得了函数 f(rm) = 4.9665892908148885 * rm + -8.364091994669751, 此时loss是: 52.126928481148454\n",
      "在第1311步，我们获得了函数 f(rm) = 4.967583768595827 * rm + -8.37041798519231, 此时loss是: 52.12282825998764\n",
      "在第1312步，我们获得了函数 f(rm) = 4.968578007232444 * rm + -8.376742454489607, 此时loss是: 52.118730010568655\n",
      "在第1313步，我们获得了函数 f(rm) = 4.969572006782244 * rm + -8.383065402927457, 此时loss是: 52.11463373194331\n",
      "在第1314步，我们获得了函数 f(rm) = 4.970565767302723 * rm + -8.389386830871583, 此时loss是: 52.110539423163864\n",
      "在第1315步，我们获得了函数 f(rm) = 4.97155928885136 * rm + -8.395706738687624, 此时loss是: 52.106447083283065\n",
      "在第1316步，我们获得了函数 f(rm) = 4.972552571485621 * rm + -8.402025126741126, 此时loss是: 52.10235671135409\n",
      "在第1317步，我们获得了函数 f(rm) = 4.973545615262959 * rm + -8.408341995397551, 此时loss是: 52.09826830643056\n",
      "在第1318步，我们获得了函数 f(rm) = 4.974538420240813 * rm + -8.414657345022272, 此时loss是: 52.0941818675666\n",
      "在第1319步，我们获得了函数 f(rm) = 4.975530986476605 * rm + -8.420971175980574, 此时loss是: 52.09009739381675\n",
      "在第1320步，我们获得了函数 f(rm) = 4.976523314027749 * rm + -8.427283488637652, 此时loss是: 52.086014884235986\n",
      "在第1321步，我们获得了函数 f(rm) = 4.9775154029516395 * rm + -8.433594283358618, 此时loss是: 52.08193433787977\n",
      "在第1322步，我们获得了函数 f(rm) = 4.978507253305661 * rm + -8.439903560508492, 此时loss是: 52.07785575380405\n",
      "在第1323步，我们获得了函数 f(rm) = 4.979498865147183 * rm + -8.446211320452209, 此时loss是: 52.073779131065166\n",
      "在第1324步，我们获得了函数 f(rm) = 4.9804902385335605 * rm + -8.452517563554613, 此时loss是: 52.06970446871993\n",
      "在第1325步，我们获得了函数 f(rm) = 4.981481373522136 * rm + -8.458822290180464, 此时loss是: 52.06563176582563\n",
      "在第1326步，我们获得了函数 f(rm) = 4.982472270170238 * rm + -8.465125500694434, 此时loss是: 52.06156102143998\n",
      "在第1327步，我们获得了函数 f(rm) = 4.983462928535179 * rm + -8.471427195461104, 此时loss是: 52.05749223462116\n",
      "在第1328步，我们获得了函数 f(rm) = 4.984453348674261 * rm + -8.477727374844967, 此时loss是: 52.05342540442782\n",
      "在第1329步，我们获得了函数 f(rm) = 4.985443530644771 * rm + -8.484026039210434, 此时loss是: 52.04936052991903\n",
      "在第1330步，我们获得了函数 f(rm) = 4.98643347450398 * rm + -8.490323188921824, 此时loss是: 52.04529761015433\n",
      "在第1331步，我们获得了函数 f(rm) = 4.987423180309149 * rm + -8.496618824343368, 此时loss是: 52.0412366441937\n",
      "在第1332步，我们获得了函数 f(rm) = 4.988412648117523 * rm + -8.502912945839213, 此时loss是: 52.037177631097585\n",
      "在第1333步，我们获得了函数 f(rm) = 4.9894018779863325 * rm + -8.509205553773414, 此时loss是: 52.0331205699269\n",
      "在第1334步，我们获得了函数 f(rm) = 4.990390869972797 * rm + -8.515496648509941, 此时loss是: 52.02906545974297\n",
      "在第1335步，我们获得了函数 f(rm) = 4.99137962413412 * rm + -8.521786230412678, 此时loss是: 52.025012299607596\n",
      "在第1336步，我们获得了函数 f(rm) = 4.992368140527491 * rm + -8.528074299845416, 此时loss是: 52.02096108858302\n",
      "在第1337步，我们获得了函数 f(rm) = 4.993356419210087 * rm + -8.534360857171865, 此时loss是: 52.01691182573195\n",
      "在第1338步，我们获得了函数 f(rm) = 4.9943444602390725 * rm + -8.540645902755644, 此时loss是: 52.012864510117524\n",
      "在第1339步，我们获得了函数 f(rm) = 4.995332263671595 * rm + -8.546929436960285, 此时loss是: 52.00881914080334\n",
      "在第1340步，我们获得了函数 f(rm) = 4.99631982956479 * rm + -8.553211460149232, 此时loss是: 52.00477571685348\n",
      "在第1341步，我们获得了函数 f(rm) = 4.997307157975779 * rm + -8.559491972685842, 此时loss是: 52.00073423733241\n",
      "在第1342步，我们获得了函数 f(rm) = 4.99829424896167 * rm + -8.565770974933388, 此时loss是: 51.996694701305096\n",
      "在第1343步，我们获得了函数 f(rm) = 4.999281102579558 * rm + -8.57204846725505, 此时loss是: 51.99265710783693\n",
      "在第1344步，我们获得了函数 f(rm) = 5.000267718886522 * rm + -8.578324450013923, 此时loss是: 51.98862145599376\n",
      "在第1345步，我们获得了函数 f(rm) = 5.001254097939629 * rm + -8.584598923573017, 此时loss是: 51.984587744841896\n",
      "在第1346步，我们获得了函数 f(rm) = 5.002240239795934 * rm + -8.59087188829525, 此时loss是: 51.980555973448055\n",
      "在第1347步，我们获得了函数 f(rm) = 5.003226144512473 * rm + -8.59714334454346, 此时loss是: 51.97652614087948\n",
      "在第1348步，我们获得了函数 f(rm) = 5.0042118121462735 * rm + -8.603413292680388, 此时loss是: 51.97249824620377\n",
      "在第1349步，我们获得了函数 f(rm) = 5.005197242754347 * rm + -8.609681733068696, 此时loss是: 51.96847228848907\n",
      "在第1350步，我们获得了函数 f(rm) = 5.006182436393693 * rm + -8.615948666070954, 此时loss是: 51.96444826680388\n",
      "在第1351步，我们获得了函数 f(rm) = 5.007167393121293 * rm + -8.622214092049648, 此时loss是: 51.96042618021717\n",
      "在第1352步，我们获得了函数 f(rm) = 5.008152112994119 * rm + -8.628478011367173, 此时loss是: 51.95640602779843\n",
      "在第1353步，我们获得了函数 f(rm) = 5.009136596069129 * rm + -8.634740424385843, 此时loss是: 51.95238780861753\n",
      "在第1354步，我们获得了函数 f(rm) = 5.010120842403265 * rm + -8.641001331467878, 此时loss是: 51.94837152174478\n",
      "在第1355步，我们获得了函数 f(rm) = 5.011104852053458 * rm + -8.647260732975415, 此时loss是: 51.94435716625098\n",
      "在第1356步，我们获得了函数 f(rm) = 5.012088625076622 * rm + -8.653518629270502, 此时loss是: 51.94034474120733\n",
      "在第1357步，我们获得了函数 f(rm) = 5.01307216152966 * rm + -8.659775020715104, 此时loss是: 51.936334245685536\n",
      "在第1358步，我们获得了函数 f(rm) = 5.014055461469462 * rm + -8.666029907671092, 此时loss是: 51.93232567875769\n",
      "在第1359步，我们获得了函数 f(rm) = 5.015038524952901 * rm + -8.672283290500257, 此时loss是: 51.92831903949639\n",
      "在第1360步，我们获得了函数 f(rm) = 5.016021352036839 * rm + -8.678535169564297, 此时loss是: 51.924314326974596\n",
      "在第1361步，我们获得了函数 f(rm) = 5.017003942778124 * rm + -8.684785545224827, 此时loss是: 51.92031154026579\n",
      "在第1362步，我们获得了函数 f(rm) = 5.017986297233589 * rm + -8.691034417843374, 此时loss是: 51.91631067844389\n",
      "在第1363步，我们获得了函数 f(rm) = 5.018968415460055 * rm + -8.697281787781378, 此时loss是: 51.912311740583235\n",
      "在第1364步，我们获得了函数 f(rm) = 5.019950297514327 * rm + -8.703527655400192, 此时loss是: 51.9083147257586\n",
      "在第1365步，我们获得了函数 f(rm) = 5.020931943453199 * rm + -8.709772021061081, 此时loss是: 51.904319633045226\n",
      "在第1366步，我们获得了函数 f(rm) = 5.02191335333345 * rm + -8.716014885125226, 此时loss是: 51.90032646151883\n",
      "在第1367步，我们获得了函数 f(rm) = 5.022894527211847 * rm + -8.722256247953718, 此时loss是: 51.89633521025548\n",
      "在第1368步，我们获得了函数 f(rm) = 5.023875465145139 * rm + -8.728496109907562, 此时loss是: 51.8923458783318\n",
      "在第1369步，我们获得了函数 f(rm) = 5.024856167190066 * rm + -8.734734471347679, 此时loss是: 51.88835846482479\n",
      "在第1370步，我们获得了函数 f(rm) = 5.025836633403353 * rm + -8.7409713326349, 此时loss是: 51.884372968811896\n",
      "在第1371步，我们获得了函数 f(rm) = 5.0268168638417094 * rm + -8.747206694129968, 此时loss是: 51.88038938937103\n",
      "在第1372步，我们获得了函数 f(rm) = 5.0277968585618344 * rm + -8.753440556193544, 此时loss是: 51.876407725580535\n",
      "在第1373步，我们获得了函数 f(rm) = 5.0287766176204105 * rm + -8.7596729191862, 此时loss是: 51.87242797651919\n",
      "在第1374步，我们获得了函数 f(rm) = 5.029756141074108 * rm + -8.765903783468417, 此时loss是: 51.86845014126626\n",
      "在第1375步，我们获得了函数 f(rm) = 5.030735428979583 * rm + -8.772133149400597, 此时loss是: 51.86447421890138\n",
      "在第1376步，我们获得了函数 f(rm) = 5.031714481393479 * rm + -8.778361017343052, 此时loss是: 51.860500208504696\n",
      "在第1377步，我们获得了函数 f(rm) = 5.032693298372425 * rm + -8.784587387656005, 此时loss是: 51.856528109156756\n",
      "在第1378步，我们获得了函数 f(rm) = 5.033671879973037 * rm + -8.790812260699596, 此时loss是: 51.85255791993857\n",
      "在第1379步，我们获得了函数 f(rm) = 5.034650226251915 * rm + -8.797035636833876, 此时loss是: 51.84858963993157\n",
      "在第1380步，我们获得了函数 f(rm) = 5.03562833726565 * rm + -8.803257516418808, 此时loss是: 51.84462326821765\n",
      "在第1381步，我们获得了函数 f(rm) = 5.036606213070815 * rm + -8.809477899814274, 此时loss是: 51.84065880387914\n",
      "在第1382步，我们获得了函数 f(rm) = 5.037583853723972 * rm + -8.815696787380064, 此时loss是: 51.836696245998795\n",
      "在第1383步，我们获得了函数 f(rm) = 5.038561259281668 * rm + -8.821914179475884, 此时loss是: 51.832735593659834\n",
      "在第1384步，我们获得了函数 f(rm) = 5.039538429800436 * rm + -8.828130076461354, 此时loss是: 51.828776845945924\n",
      "在第1385步，我们获得了函数 f(rm) = 5.040515365336799 * rm + -8.834344478696005, 此时loss是: 51.82482000194112\n",
      "在第1386步，我们获得了函数 f(rm) = 5.041492065947262 * rm + -8.840557386539285, 此时loss是: 51.82086506072997\n",
      "在第1387步，我们获得了函数 f(rm) = 5.042468531688318 * rm + -8.846768800350551, 此时loss是: 51.81691202139747\n",
      "在第1388步，我们获得了函数 f(rm) = 5.0434447626164465 * rm + -8.85297872048908, 此时loss是: 51.81296088302899\n",
      "在第1389步，我们获得了函数 f(rm) = 5.044420758788115 * rm + -8.859187147314056, 此时loss是: 51.80901164471041\n",
      "在第1390步，我们获得了函数 f(rm) = 5.045396520259774 * rm + -8.865394081184581, 此时loss是: 51.80506430552801\n",
      "在第1391步，我们获得了函数 f(rm) = 5.046372047087864 * rm + -8.871599522459668, 此时loss是: 51.80111886456853\n",
      "在第1392步，我们获得了函数 f(rm) = 5.047347339328809 * rm + -8.877803471498245, 此时loss是: 51.79717532091913\n",
      "在第1393步，我们获得了函数 f(rm) = 5.048322397039022 * rm + -8.884005928659153, 此时loss是: 51.79323367366742\n",
      "在第1394步，我们获得了函数 f(rm) = 5.049297220274899 * rm + -8.890206894301148, 此时loss是: 51.789293921901454\n",
      "在第1395步，我们获得了函数 f(rm) = 5.050271809092826 * rm + -8.896406368782898, 此时loss是: 51.785356064709724\n",
      "在第1396步，我们获得了函数 f(rm) = 5.051246163549175 * rm + -8.902604352462989, 此时loss是: 51.781420101181126\n",
      "在第1397步，我们获得了函数 f(rm) = 5.052220283700302 * rm + -8.908800845699913, 此时loss是: 51.77748603040506\n",
      "在第1398步，我们获得了函数 f(rm) = 5.053194169602551 * rm + -8.914995848852083, 此时loss是: 51.7735538514713\n",
      "在第1399步，我们获得了函数 f(rm) = 5.0541678213122525 * rm + -8.921189362277822, 此时loss是: 51.769623563470084\n",
      "在第1400步，我们获得了函数 f(rm) = 5.0551412388857235 * rm + -8.927381386335368, 此时loss是: 51.76569516549211\n",
      "在第1401步，我们获得了函数 f(rm) = 5.056114422379268 * rm + -8.933571921382873, 此时loss是: 51.761768656628455\n",
      "在第1402步，我们获得了函数 f(rm) = 5.057087371849174 * rm + -8.939760967778403, 此时loss是: 51.75784403597072\n",
      "在第1403步，我们获得了函数 f(rm) = 5.058060087351719 * rm + -8.945948525879936, 此时loss是: 51.753921302610834\n",
      "在第1404步，我们获得了函数 f(rm) = 5.059032568943165 * rm + -8.952134596045367, 此时loss是: 51.75000045564127\n",
      "在第1405步，我们获得了函数 f(rm) = 5.060004816679761 * rm + -8.958319178632502, 此时loss是: 51.746081494154865\n",
      "在第1406步，我们获得了函数 f(rm) = 5.0609768306177445 * rm + -8.964502273999063, 此时loss是: 51.74216441724491\n",
      "在第1407步，我们获得了函数 f(rm) = 5.061948610813335 * rm + -8.970683882502685, 此时loss是: 51.73824922400514\n",
      "在第1408步，我们获得了函数 f(rm) = 5.0629201573227425 * rm + -8.97686400450092, 此时loss是: 51.73433591352974\n",
      "在第1409步，我们获得了函数 f(rm) = 5.063891470202162 * rm + -8.983042640351227, 此时loss是: 51.7304244849133\n",
      "在第1410步，我们获得了函数 f(rm) = 5.064862549507774 * rm + -8.989219790410987, 此时loss是: 51.72651493725085\n",
      "在第1411步，我们获得了函数 f(rm) = 5.065833395295748 * rm + -8.995395455037489, 此时loss是: 51.722607269637884\n",
      "在第1412步，我们获得了函数 f(rm) = 5.066804007622237 * rm + -9.00156963458794, 此时loss是: 51.7187014811703\n",
      "在第1413步，我们获得了函数 f(rm) = 5.067774386543384 * rm + -9.007742329419457, 此时loss是: 51.714797570944434\n",
      "在第1414步，我们获得了函数 f(rm) = 5.068744532115315 * rm + -9.013913539889078, 此时loss是: 51.710895538057095\n",
      "在第1415步，我们获得了函数 f(rm) = 5.069714444394144 * rm + -9.020083266353748, 此时loss是: 51.70699538160546\n",
      "在第1416步，我们获得了函数 f(rm) = 5.0706841234359725 * rm + -9.026251509170327, 此时loss是: 51.7030971006872\n",
      "在第1417步，我们获得了函数 f(rm) = 5.0716535692968865 * rm + -9.032418268695595, 此时loss是: 51.699200694400375\n",
      "在第1418步，我们获得了函数 f(rm) = 5.072622782032959 * rm + -9.038583545286242, 此时loss是: 51.695306161843526\n",
      "在第1419步，我们获得了函数 f(rm) = 5.073591761700252 * rm + -9.04474733929887, 此时loss是: 51.69141350211558\n",
      "在第1420步，我们获得了函数 f(rm) = 5.074560508354811 * rm + -9.050909651090002, 此时loss是: 51.68752271431593\n",
      "在第1421步，我们获得了函数 f(rm) = 5.0755290220526685 * rm + -9.057070481016067, 此时loss是: 51.683633797544374\n",
      "在第1422步，我们获得了函数 f(rm) = 5.0764973028498455 * rm + -9.063229829433416, 此时loss是: 51.67974675090119\n",
      "在第1423步，我们获得了函数 f(rm) = 5.077465350802347 * rm + -9.069387696698309, 此时loss是: 51.67586157348702\n",
      "在第1424步，我们获得了函数 f(rm) = 5.078433165966167 * rm + -9.075544083166921, 此时loss是: 51.671978264403\n",
      "在第1425步，我们获得了函数 f(rm) = 5.0794007483972825 * rm + -9.081698989195344, 此时loss是: 51.6680968227507\n",
      "在第1426步，我们获得了函数 f(rm) = 5.080368098151661 * rm + -9.087852415139581, 此时loss是: 51.66421724763203\n",
      "在第1427步，我们获得了函数 f(rm) = 5.081335215285255 * rm + -9.094004361355553, 此时loss是: 51.66033953814945\n",
      "在第1428步，我们获得了函数 f(rm) = 5.082302099854002 * rm + -9.100154828199095, 此时loss是: 51.656463693405804\n",
      "在第1429步，我们获得了函数 f(rm) = 5.083268751913828 * rm + -9.106303816025951, 此时loss是: 51.652589712504316\n",
      "在第1430步，我们获得了函数 f(rm) = 5.084235171520646 * rm + -9.112451325191786, 此时loss是: 51.64871759454875\n",
      "在第1431步，我们获得了函数 f(rm) = 5.0852013587303535 * rm + -9.118597356052177, 此时loss是: 51.6448473386432\n",
      "在第1432步，我们获得了函数 f(rm) = 5.0861673135988354 * rm + -9.124741908962616, 此时loss是: 51.64097894389224\n",
      "在第1433步，我们获得了函数 f(rm) = 5.087133036181964 * rm + -9.130884984278508, 此时loss是: 51.63711240940087\n",
      "在第1434步，我们获得了函数 f(rm) = 5.088098526535597 * rm + -9.137026582355173, 此时loss是: 51.6332477342745\n",
      "在第1435步，我们获得了函数 f(rm) = 5.089063784715578 * rm + -9.143166703547847, 此时loss是: 51.629384917619014\n",
      "在第1436步，我们获得了函数 f(rm) = 5.090028810777741 * rm + -9.149305348211678, 此时loss是: 51.625523958540676\n",
      "在第1437步，我们获得了函数 f(rm) = 5.090993604777902 * rm + -9.155442516701733, 此时loss是: 51.62166485614622\n",
      "在第1438步，我们获得了函数 f(rm) = 5.091958166771866 * rm + -9.161578209372989, 此时loss是: 51.617807609542766\n",
      "在第1439步，我们获得了函数 f(rm) = 5.092922496815423 * rm + -9.167712426580339, 此时loss是: 51.6139522178379\n",
      "在第1440步，我们获得了函数 f(rm) = 5.093886594964353 * rm + -9.173845168678593, 此时loss是: 51.610098680139636\n",
      "在第1441步，我们获得了函数 f(rm) = 5.094850461274418 * rm + -9.179976436022471, 此时loss是: 51.606246995556404\n",
      "在第1442步，我们获得了函数 f(rm) = 5.095814095801369 * rm + -9.186106228966613, 此时loss是: 51.60239716319707\n",
      "在第1443步，我们获得了函数 f(rm) = 5.096777498600946 * rm + -9.19223454786557, 此时loss是: 51.5985491821709\n",
      "在第1444步，我们获得了函数 f(rm) = 5.09774066972887 * rm + -9.198361393073812, 此时loss是: 51.59470305158762\n",
      "在第1445步，我们获得了函数 f(rm) = 5.098703609240852 * rm + -9.204486764945717, 此时loss是: 51.590858770557396\n",
      "在第1446步，我们获得了函数 f(rm) = 5.0996663171925904 * rm + -9.210610663835583, 此时loss是: 51.58701633819079\n",
      "在第1447步，我们获得了函数 f(rm) = 5.1006287936397685 * rm + -9.216733090097621, 此时loss是: 51.58317575359881\n",
      "在第1448步，我们获得了函数 f(rm) = 5.101591038638057 * rm + -9.222854044085958, 此时loss是: 51.57933701589288\n",
      "在第1449步，我们获得了函数 f(rm) = 5.102553052243112 * rm + -9.228973526154634, 此时loss是: 51.57550012418486\n",
      "在第1450步，我们获得了函数 f(rm) = 5.103514834510579 * rm + -9.235091536657604, 此时loss是: 51.571665077587035\n",
      "在第1451步，我们获得了函数 f(rm) = 5.104476385496087 * rm + -9.241208075948741, 此时loss是: 51.56783187521209\n",
      "在第1452步，我们获得了函数 f(rm) = 5.105437705255252 * rm + -9.247323144381829, 此时loss是: 51.56400051617322\n",
      "在第1453步，我们获得了函数 f(rm) = 5.106398793843679 * rm + -9.25343674231057, 此时loss是: 51.560170999583946\n",
      "在第1454步，我们获得了函数 f(rm) = 5.1073596513169575 * rm + -9.259548870088578, 此时loss是: 51.55634332455826\n",
      "在第1455步，我们获得了函数 f(rm) = 5.108320277730665 * rm + -9.265659528069383, 此时loss是: 51.55251749021058\n",
      "在第1456步，我们获得了函数 f(rm) = 5.109280673140364 * rm + -9.271768716606433, 此时loss是: 51.54869349565577\n",
      "在第1457步，我们获得了函数 f(rm) = 5.110240837601604 * rm + -9.277876436053086, 此时loss是: 51.544871340009074\n",
      "在第1458步，我们获得了函数 f(rm) = 5.111200771169924 * rm + -9.283982686762618, 此时loss是: 51.541051022386206\n",
      "在第1459步，我们获得了函数 f(rm) = 5.112160473900845 * rm + -9.29008746908822, 此时loss是: 51.53723254190324\n",
      "在第1460步，我们获得了函数 f(rm) = 5.113119945849879 * rm + -9.296190783382999, 此时loss是: 51.53341589767679\n",
      "在第1461步，我们获得了函数 f(rm) = 5.11407918707252 * rm + -9.302292629999974, 此时loss是: 51.52960108882378\n",
      "在第1462步，我们获得了函数 f(rm) = 5.115038197624254 * rm + -9.30839300929208, 此时loss是: 51.52578811446161\n",
      "在第1463步，我们获得了函数 f(rm) = 5.11599697756055 * rm + -9.31449192161217, 此时loss是: 51.52197697370809\n",
      "在第1464步，我们获得了函数 f(rm) = 5.116955526936864 * rm + -9.320589367313008, 此时loss是: 51.518167665681474\n",
      "在第1465步，我们获得了函数 f(rm) = 5.11791384580864 * rm + -9.326685346747277, 此时loss是: 51.51436018950044\n",
      "在第1466步，我们获得了函数 f(rm) = 5.118871934231307 * rm + -9.332779860267573, 此时loss是: 51.51055454428404\n",
      "在第1467步，我们获得了函数 f(rm) = 5.1198297922602825 * rm + -9.338872908226408, 此时loss是: 51.50675072915183\n",
      "在第1468步，我们获得了函数 f(rm) = 5.12078741995097 * rm + -9.344964490976208, 此时loss是: 51.50294874322372\n",
      "在第1469步，我们获得了函数 f(rm) = 5.121744817358759 * rm + -9.351054608869315, 此时loss是: 51.49914858562008\n",
      "在第1470步，我们获得了函数 f(rm) = 5.1227019845390265 * rm + -9.357143262257988, 此时loss是: 51.49535025546168\n",
      "在第1471步，我们获得了函数 f(rm) = 5.123658921547135 * rm + -9.363230451494399, 此时loss是: 51.49155375186975\n",
      "在第1472步，我们获得了函数 f(rm) = 5.124615628438436 * rm + -9.369316176930635, 此时loss是: 51.4877590739659\n",
      "在第1473步，我们获得了函数 f(rm) = 5.125572105268264 * rm + -9.3754004389187, 此时loss是: 51.483966220872176\n",
      "在第1474步，我们获得了函数 f(rm) = 5.1265283520919445 * rm + -9.381483237810512, 此时loss是: 51.48017519171107\n",
      "在第1475步，我们获得了函数 f(rm) = 5.127484368964787 * rm + -9.387564573957906, 此时loss是: 51.47638598560546\n",
      "在第1476步，我们获得了函数 f(rm) = 5.128440155942088 * rm + -9.393644447712632, 此时loss是: 51.472598601678676\n",
      "在第1477步，我们获得了函数 f(rm) = 5.129395713079131 * rm + -9.399722859426353, 此时loss是: 51.46881303905445\n",
      "在第1478步，我们获得了函数 f(rm) = 5.130351040431186 * rm + -9.40579980945065, 此时loss是: 51.46502929685694\n",
      "在第1479步，我们获得了函数 f(rm) = 5.13130613805351 * rm + -9.411875298137021, 此时loss是: 51.461247374210735\n",
      "在第1480步，我们获得了函数 f(rm) = 5.132261006001348 * rm + -9.417949325836874, 此时loss是: 51.457467270240834\n",
      "在第1481步，我们获得了函数 f(rm) = 5.133215644329928 * rm + -9.424021892901537, 此时loss是: 51.45368898407265\n",
      "在第1482步，我们获得了函数 f(rm) = 5.1341700530944685 * rm + -9.430092999682254, 此时loss是: 51.44991251483203\n",
      "在第1483步，我们获得了函数 f(rm) = 5.135124232350173 * rm + -9.43616264653018, 此时loss是: 51.44613786164525\n",
      "在第1484步，我们获得了函数 f(rm) = 5.136078182152232 * rm + -9.442230833796389, 此时loss是: 51.44236502363899\n",
      "在第1485步，我们获得了函数 f(rm) = 5.137031902555823 * rm + -9.44829756183187, 此时loss是: 51.43859399994034\n",
      "在第1486步，我们获得了函数 f(rm) = 5.13798539361611 * rm + -9.45436283098753, 此时loss是: 51.434824789676824\n",
      "在第1487步，我们获得了函数 f(rm) = 5.138938655388243 * rm + -9.460426641614184, 此时loss是: 51.43105739197642\n",
      "在第1488步，我们获得了函数 f(rm) = 5.13989168792736 * rm + -9.466488994062573, 此时loss是: 51.42729180596744\n",
      "在第1489步，我们获得了函数 f(rm) = 5.140844491288585 * rm + -9.472549888683345, 此时loss是: 51.423528030778705\n",
      "在第1490步，我们获得了函数 f(rm) = 5.1417970655270295 * rm + -9.478609325827069, 此时loss是: 51.419766065539406\n",
      "在第1491步，我们获得了函数 f(rm) = 5.142749410697791 * rm + -9.484667305844226, 此时loss是: 51.416005909379145\n",
      "在第1492步，我们获得了函数 f(rm) = 5.143701526855952 * rm + -9.490723829085217, 此时loss是: 51.412247561427975\n",
      "在第1493步，我们获得了函数 f(rm) = 5.144653414056586 * rm + -9.496778895900356, 此时loss是: 51.40849102081634\n",
      "在第1494步，我们获得了函数 f(rm) = 5.14560507235475 * rm + -9.50283250663987, 此时loss是: 51.40473628667514\n",
      "在第1495步，我们获得了函数 f(rm) = 5.14655650180549 * rm + -9.508884661653907, 此时loss是: 51.40098335813565\n",
      "在第1496步，我们获得了函数 f(rm) = 5.147507702463836 * rm + -9.514935361292528, 此时loss是: 51.39723223432958\n",
      "在第1497步，我们获得了函数 f(rm) = 5.1484586743848055 * rm + -9.520984605905712, 此时loss是: 51.39348291438904\n",
      "在第1498步，我们获得了函数 f(rm) = 5.1494094176234055 * rm + -9.52703239584335, 此时loss是: 51.38973539744662\n",
      "在第1499步，我们获得了函数 f(rm) = 5.150359932234626 * rm + -9.533078731455252, 此时loss是: 51.385989682635255\n",
      "在第1500步，我们获得了函数 f(rm) = 5.151310218273448 * rm + -9.539123613091144, 此时loss是: 51.38224576908831\n",
      "在第1501步，我们获得了函数 f(rm) = 5.152260275794834 * rm + -9.545167041100665, 此时loss是: 51.378503655939596\n",
      "在第1502步，我们获得了函数 f(rm) = 5.153210104853738 * rm + -9.551209015833374, 此时loss是: 51.37476334232333\n",
      "在第1503步，我们获得了函数 f(rm) = 5.154159705505097 * rm + -9.557249537638743, 此时loss是: 51.37102482737415\n",
      "在第1504步，我们获得了函数 f(rm) = 5.155109077803839 * rm + -9.56328860686616, 此时loss是: 51.36728811022708\n",
      "在第1505步，我们获得了函数 f(rm) = 5.156058221804875 * rm + -9.569326223864929, 此时loss是: 51.363553190017576\n",
      "在第1506步，我们获得了函数 f(rm) = 5.157007137563104 * rm + -9.575362388984272, 此时loss是: 51.35982006588155\n",
      "在第1507步，我们获得了函数 f(rm) = 5.157955825133414 * rm + -9.581397102573327, 此时loss是: 51.35608873695526\n",
      "在第1508步，我们获得了函数 f(rm) = 5.158904284570676 * rm + -9.587430364981143, 此时loss是: 51.35235920237541\n",
      "在第1509步，我们获得了函数 f(rm) = 5.15985251592975 * rm + -9.593462176556693, 此时loss是: 51.34863146127915\n",
      "在第1510步，我们获得了函数 f(rm) = 5.160800519265483 * rm + -9.59949253764886, 此时loss是: 51.344905512804004\n",
      "在第1511步，我们获得了函数 f(rm) = 5.161748294632708 * rm + -9.605521448606446, 此时loss是: 51.34118135608794\n",
      "在第1512步，我们获得了函数 f(rm) = 5.1626958420862445 * rm + -9.611548909778167, 此时loss是: 51.33745899026928\n",
      "在第1513步，我们获得了函数 f(rm) = 5.1636431616809 * rm + -9.617574921512656, 此时loss是: 51.33373841448687\n",
      "在第1514步，我们获得了函数 f(rm) = 5.164590253471469 * rm + -9.623599484158461, 此时loss是: 51.33001962787985\n",
      "在第1515步，我们获得了函数 f(rm) = 5.165537117512732 * rm + -9.629622598064051, 此时loss是: 51.32630262958785\n",
      "在第1516步，我们获得了函数 f(rm) = 5.166483753859454 * rm + -9.635644263577806, 此时loss是: 51.3225874187509\n",
      "在第1517步，我们获得了函数 f(rm) = 5.167430162566392 * rm + -9.641664481048025, 此时loss是: 51.31887399450943\n",
      "在第1518步，我们获得了函数 f(rm) = 5.168376343688285 * rm + -9.647683250822922, 此时loss是: 51.315162356004286\n",
      "在第1519步，我们获得了函数 f(rm) = 5.169322297279863 * rm + -9.653700573250628, 此时loss是: 51.31145250237673\n",
      "在第1520步，我们获得了函数 f(rm) = 5.170268023395839 * rm + -9.65971644867919, 此时loss是: 51.30774443276845\n",
      "在第1521步，我们获得了函数 f(rm) = 5.171213522090916 * rm + -9.665730877456568, 此时loss是: 51.30403814632153\n",
      "在第1522步，我们获得了函数 f(rm) = 5.172158793419781 * rm + -9.671743859930645, 此时loss是: 51.300333642178465\n",
      "在第1523步，我们获得了函数 f(rm) = 5.173103837437109 * rm + -9.677755396449216, 此时loss是: 51.296630919482176\n",
      "在第1524步，我们获得了函数 f(rm) = 5.1740486541975645 * rm + -9.683765487359992, 此时loss是: 51.292929977375984\n",
      "在第1525步，我们获得了函数 f(rm) = 5.174993243755794 * rm + -9.689774133010603, 此时loss是: 51.28923081500363\n",
      "在第1526步，我们获得了函数 f(rm) = 5.175937606166435 * rm + -9.695781333748593, 此时loss是: 51.285533431509265\n",
      "在第1527步，我们获得了函数 f(rm) = 5.176881741484109 * rm + -9.701787089921424, 此时loss是: 51.28183782603745\n",
      "在第1528步，我们获得了函数 f(rm) = 5.177825649763426 * rm + -9.707791401876474, 此时loss是: 51.27814399773315\n",
      "在第1529步，我们获得了函数 f(rm) = 5.178769331058982 * rm + -9.713794269961037, 此时loss是: 51.27445194574175\n",
      "在第1530步，我们获得了函数 f(rm) = 5.1797127854253615 * rm + -9.719795694522324, 此时loss是: 51.270761669209065\n",
      "在第1531步，我们获得了函数 f(rm) = 5.180656012917134 * rm + -9.725795675907461, 此时loss是: 51.26707316728127\n",
      "在第1532步，我们获得了函数 f(rm) = 5.1815990135888566 * rm + -9.731794214463495, 此时loss是: 51.263386439105005\n",
      "在第1533步，我们获得了函数 f(rm) = 5.182541787495072 * rm + -9.737791310537384, 此时loss是: 51.259701483827286\n",
      "在第1534步，我们获得了函数 f(rm) = 5.183484334690314 * rm + -9.743786964476005, 此时loss是: 51.25601830059554\n",
      "在第1535步，我们获得了函数 f(rm) = 5.184426655229097 * rm + -9.74978117662615, 此时loss是: 51.25233688855763\n",
      "在第1536步，我们获得了函数 f(rm) = 5.185368749165928 * rm + -9.755773947334534, 此时loss是: 51.24865724686181\n",
      "在第1537步，我们获得了函数 f(rm) = 5.186310616555296 * rm + -9.761765276947779, 此时loss是: 51.244979374656744\n",
      "在第1538步，我们获得了函数 f(rm) = 5.187252257451682 * rm + -9.76775516581243, 此时loss是: 51.2413032710915\n",
      "在第1539步，我们获得了函数 f(rm) = 5.188193671909551 * rm + -9.773743614274947, 此时loss是: 51.23762893531558\n",
      "在第1540步，我们获得了函数 f(rm) = 5.189134859983353 * rm + -9.779730622681708, 此时loss是: 51.233956366478864\n",
      "在第1541步，我们获得了函数 f(rm) = 5.190075821727529 * rm + -9.785716191379006, 此时loss是: 51.230285563731655\n",
      "在第1542步，我们获得了函数 f(rm) = 5.191016557196504 * rm + -9.79170032071305, 此时loss是: 51.22661652622465\n",
      "在第1543步，我们获得了函数 f(rm) = 5.191957066444692 * rm + -9.797683011029967, 此时loss是: 51.222949253109014\n",
      "在第1544步，我们获得了函数 f(rm) = 5.1928973495264925 * rm + -9.803664262675802, 此时loss是: 51.219283743536224\n",
      "在第1545步，我们获得了函数 f(rm) = 5.193837406496291 * rm + -9.809644075996514, 此时loss是: 51.21561999665826\n",
      "在第1546步，我们获得了函数 f(rm) = 5.194777237408463 * rm + -9.815622451337982, 此时loss是: 51.21195801162742\n",
      "在第1547步，我们获得了函数 f(rm) = 5.195716842317367 * rm + -9.821599389045998, 此时loss是: 51.20829778759649\n",
      "在第1548步，我们获得了函数 f(rm) = 5.196656221277353 * rm + -9.827574889466275, 此时loss是: 51.2046393237186\n",
      "在第1549步，我们获得了函数 f(rm) = 5.197595374342753 * rm + -9.83354895294444, 此时loss是: 51.200982619147354\n",
      "在第1550步，我们获得了函数 f(rm) = 5.19853430156789 * rm + -9.839521579826037, 此时loss是: 51.19732767303669\n",
      "在第1551步，我们获得了函数 f(rm) = 5.199473003007072 * rm + -9.845492770456529, 此时loss是: 51.19367448454101\n",
      "在第1552步，我们获得了函数 f(rm) = 5.200411478714594 * rm + -9.851462525181294, 此时loss是: 51.19002305281508\n",
      "在第1553步，我们获得了函数 f(rm) = 5.201349728744738 * rm + -9.857430844345627, 此时loss是: 51.18637337701411\n",
      "在第1554步，我们获得了函数 f(rm) = 5.202287753151774 * rm + -9.863397728294741, 此时loss是: 51.18272545629369\n",
      "在第1555步，我们获得了函数 f(rm) = 5.2032255519899575 * rm + -9.869363177373765, 此时loss是: 51.17907928980982\n",
      "在第1556步，我们获得了函数 f(rm) = 5.2041631253135305 * rm + -9.875327191927747, 此时loss是: 51.17543487671893\n",
      "在第1557步，我们获得了函数 f(rm) = 5.205100473176725 * rm + -9.881289772301649, 此时loss是: 51.17179221617782\n",
      "在第1558步，我们获得了函数 f(rm) = 5.206037595633758 * rm + -9.887250918840351, 此时loss是: 51.16815130734372\n",
      "在第1559步，我们获得了函数 f(rm) = 5.206974492738831 * rm + -9.893210631888653, 此时loss是: 51.164512149374254\n",
      "在第1560步，我们获得了函数 f(rm) = 5.207911164546138 * rm + -9.899168911791268, 此时loss是: 51.160874741427456\n",
      "在第1561步，我们获得了函数 f(rm) = 5.208847611109854 * rm + -9.905125758892828, 此时loss是: 51.15723908266177\n",
      "在第1562步，我们获得了函数 f(rm) = 5.2097838324841454 * rm + -9.91108117353788, 此时loss是: 51.15360517223602\n",
      "在第1563步，我们获得了函数 f(rm) = 5.210719828723164 * rm + -9.917035156070893, 此时loss是: 51.14997300930947\n",
      "在第1564步，我们获得了函数 f(rm) = 5.211655599881048 * rm + -9.922987706836249, 此时loss是: 51.14634259304175\n",
      "在第1565步，我们获得了函数 f(rm) = 5.212591146011924 * rm + -9.928938826178248, 此时loss是: 51.14271392259295\n",
      "在第1566步，我们获得了函数 f(rm) = 5.2135264671699035 * rm + -9.93488851444111, 此时loss是: 51.139086997123506\n",
      "在第1567步，我们获得了函数 f(rm) = 5.214461563409087 * rm + -9.940836771968966, 此时loss是: 51.13546181579427\n",
      "在第1568步，我们获得了函数 f(rm) = 5.215396434783562 * rm + -9.94678359910587, 此时loss是: 51.13183837776653\n",
      "在第1569步，我们获得了函数 f(rm) = 5.216331081347401 * rm + -9.952728996195793, 此时loss是: 51.12821668220195\n",
      "在第1570步，我们获得了函数 f(rm) = 5.217265503154666 * rm + -9.958672963582618, 此时loss是: 51.12459672826259\n",
      "在第1571步，我们获得了函数 f(rm) = 5.218199700259403 * rm + -9.964615501610153, 此时loss是: 51.12097851511094\n",
      "在第1572步，我们获得了函数 f(rm) = 5.219133672715648 * rm + -9.970556610622117, 此时loss是: 51.117362041909864\n",
      "在第1573步，我们获得了函数 f(rm) = 5.220067420577422 * rm + -9.976496290962148, 此时loss是: 51.113747307822656\n",
      "在第1574步，我们获得了函数 f(rm) = 5.2210009438987335 * rm + -9.982434542973802, 此时loss是: 51.110134312013\n",
      "在第1575步，我们获得了函数 f(rm) = 5.22193424273358 * rm + -9.988371367000555, 此时loss是: 51.106523053644985\n",
      "在第1576步，我们获得了函数 f(rm) = 5.222867317135942 * rm + -9.994306763385795, 此时loss是: 51.10291353188307\n",
      "在第1577步，我们获得了函数 f(rm) = 5.223800167159792 * rm + -10.00024073247283, 此时loss是: 51.09930574589218\n",
      "在第1578步，我们获得了函数 f(rm) = 5.224732792859084 * rm + -10.006173274604889, 此时loss是: 51.0956996948376\n",
      "在第1579步，我们获得了函数 f(rm) = 5.225665194287763 * rm + -10.012104390125112, 此时loss是: 51.09209537788499\n",
      "在第1580步，我们获得了函数 f(rm) = 5.22659737149976 * rm + -10.01803407937656, 此时loss是: 51.088492794200484\n",
      "在第1581步，我们获得了函数 f(rm) = 5.227529324548993 * rm + -10.023962342702212, 此时loss是: 51.084891942950534\n",
      "在第1582步，我们获得了函数 f(rm) = 5.228461053489367 * rm + -10.029889180444963, 此时loss是: 51.08129282330207\n",
      "在第1583步，我们获得了函数 f(rm) = 5.229392558374773 * rm + -10.035814592947625, 此时loss是: 51.07769543442239\n",
      "在第1584步，我们获得了函数 f(rm) = 5.230323839259091 * rm + -10.04173858055293, 此时loss是: 51.07409977547916\n",
      "在第1585步，我们获得了函数 f(rm) = 5.231254896196186 * rm + -10.047661143603527, 此时loss是: 51.070505845640504\n",
      "在第1586步，我们获得了函数 f(rm) = 5.232185729239913 * rm + -10.053582282441981, 此时loss是: 51.06691364407492\n",
      "在第1587步，我们获得了函数 f(rm) = 5.233116338444111 * rm + -10.059501997410777, 此时loss是: 51.06332316995128\n",
      "在第1588步，我们获得了函数 f(rm) = 5.2340467238626065 * rm + -10.065420288852314, 此时loss是: 51.059734422438886\n",
      "在第1589步，我们获得了函数 f(rm) = 5.234976885549215 * rm + -10.071337157108912, 此时loss是: 51.05614740070745\n",
      "在第1590步，我们获得了函数 f(rm) = 5.235906823557737 * rm + -10.077252602522806, 此时loss是: 51.05256210392706\n",
      "在第1591步，我们获得了函数 f(rm) = 5.236836537941961 * rm + -10.083166625436153, 此时loss是: 51.048978531268205\n",
      "在第1592步，我们获得了函数 f(rm) = 5.237766028755662 * rm + -10.089079226191021, 此时loss是: 51.045396681901785\n",
      "在第1593步，我们获得了函数 f(rm) = 5.238695296052604 * rm + -10.094990405129403, 此时loss是: 51.04181655499909\n",
      "在第1594步，我们获得了函数 f(rm) = 5.239624339886535 * rm + -10.100900162593206, 此时loss是: 51.0382381497318\n",
      "在第1595步，我们获得了函数 f(rm) = 5.240553160311193 * rm + -10.106808498924254, 此时loss是: 51.034661465272\n",
      "在第1596步，我们获得了函数 f(rm) = 5.241481757380301 * rm + -10.11271541446429, 此时loss是: 51.03108650079221\n",
      "在第1597步，我们获得了函数 f(rm) = 5.24241013114757 * rm + -10.118620909554977, 此时loss是: 51.02751325546528\n",
      "在第1598步，我们获得了函数 f(rm) = 5.243338281666697 * rm + -10.124524984537892, 此时loss是: 51.02394172846451\n",
      "在第1599步，我们获得了函数 f(rm) = 5.244266208991368 * rm + -10.130427639754531, 此时loss是: 51.02037191896357\n",
      "在第1600步，我们获得了函数 f(rm) = 5.245193913175254 * rm + -10.13632887554631, 此时loss是: 51.01680382613654\n",
      "在第1601步，我们获得了函数 f(rm) = 5.246121394272016 * rm + -10.142228692254559, 此时loss是: 51.0132374491579\n",
      "在第1602步，我们获得了函数 f(rm) = 5.2470486523353 * rm + -10.14812709022053, 此时loss是: 51.00967278720251\n",
      "在第1603步，我们获得了函数 f(rm) = 5.247975687418737 * rm + -10.154024069785391, 此时loss是: 51.006109839445664\n",
      "在第1604步，我们获得了函数 f(rm) = 5.248902499575951 * rm + -10.159919631290228, 此时loss是: 51.002548605062984\n",
      "在第1605步，我们获得了函数 f(rm) = 5.249829088860546 * rm + -10.165813775076044, 此时loss是: 50.99898908323056\n",
      "在第1606步，我们获得了函数 f(rm) = 5.250755455326119 * rm + -10.171706501483763, 此时loss是: 50.99543127312486\n",
      "在第1607步，我们获得了函数 f(rm) = 5.25168159902625 * rm + -10.177597810854223, 此时loss是: 50.99187517392271\n",
      "在第1608步，我们获得了函数 f(rm) = 5.252607520014511 * rm + -10.183487703528185, 此时loss是: 50.98832078480137\n",
      "在第1609步，我们获得了函数 f(rm) = 5.2535332183444545 * rm + -10.189376179846322, 此时loss是: 50.984768104938475\n",
      "在第1610步，我们获得了函数 f(rm) = 5.254458694069625 * rm + -10.195263240149231, 此时loss是: 50.9812171335121\n",
      "在第1611步，我们获得了函数 f(rm) = 5.255383947243555 * rm + -10.201148884777423, 此时loss是: 50.97766786970064\n",
      "在第1612步，我们获得了函数 f(rm) = 5.256308977919758 * rm + -10.207033114071328, 此时loss是: 50.974120312682956\n",
      "在第1613步，我们获得了函数 f(rm) = 5.257233786151741 * rm + -10.212915928371295, 此时loss是: 50.97057446163825\n",
      "在第1614步，我们获得了函数 f(rm) = 5.258158371992995 * rm + -10.218797328017592, 此时loss是: 50.96703031574615\n",
      "在第1615步，我们获得了函数 f(rm) = 5.259082735497 * rm + -10.224677313350403, 此时loss是: 50.96348787418668\n",
      "在第1616步，我们获得了函数 f(rm) = 5.2600068767172194 * rm + -10.230555884709831, 此时loss是: 50.959947136140244\n",
      "在第1617步，我们获得了函数 f(rm) = 5.260930795707108 * rm + -10.236433042435898, 此时loss是: 50.956408100787634\n",
      "在第1618步，我们获得了函数 f(rm) = 5.261854492520106 * rm + -10.242308786868543, 此时loss是: 50.95287076731006\n",
      "在第1619步，我们获得了函数 f(rm) = 5.26277796720964 * rm + -10.248183118347624, 此时loss是: 50.94933513488911\n",
      "在第1620步，我们获得了函数 f(rm) = 5.263701219829127 * rm + -10.254056037212917, 此时loss是: 50.945801202706775\n",
      "在第1621步，我们获得了函数 f(rm) = 5.2646242504319645 * rm + -10.259927543804118, 此时loss是: 50.94226896994542\n",
      "在第1622步，我们获得了函数 f(rm) = 5.265547059071545 * rm + -10.265797638460837, 此时loss是: 50.93873843578782\n",
      "在第1623步，我们获得了函数 f(rm) = 5.266469645801242 * rm + -10.271666321522607, 此时loss是: 50.93520959941715\n",
      "在第1624步，我们获得了函数 f(rm) = 5.2673920106744205 * rm + -10.277533593328878, 此时loss是: 50.931682460016944\n",
      "在第1625步，我们获得了函数 f(rm) = 5.26831415374443 * rm + -10.283399454219017, 此时loss是: 50.92815701677117\n",
      "在第1626步，我们获得了函数 f(rm) = 5.269236075064608 * rm + -10.28926390453231, 此时loss是: 50.92463326886418\n",
      "在第1627步，我们获得了函数 f(rm) = 5.270157774688279 * rm + -10.295126944607961, 此时loss是: 50.92111121548068\n",
      "在第1628步，我们获得了函数 f(rm) = 5.2710792526687555 * rm + -10.300988574785094, 此时loss是: 50.917590855805805\n",
      "在第1629步，我们获得了函数 f(rm) = 5.272000509059336 * rm + -10.30684879540275, 此时loss是: 50.91407218902509\n",
      "在第1630步，我们获得了函数 f(rm) = 5.2729215439133075 * rm + -10.31270760679989, 此时loss是: 50.910555214324425\n",
      "在第1631步，我们获得了函数 f(rm) = 5.273842357283943 * rm + -10.31856500931539, 此时loss是: 50.90703993089012\n",
      "在第1632步，我们获得了函数 f(rm) = 5.2747629492245025 * rm + -10.32442100328805, 此时loss是: 50.903526337908865\n",
      "在第1633步，我们获得了函数 f(rm) = 5.275683319788234 * rm + -10.330275589056583, 此时loss是: 50.90001443456773\n",
      "在第1634步，我们获得了函数 f(rm) = 5.276603469028373 * rm + -10.336128766959625, 此时loss是: 50.89650422005423\n",
      "在第1635步，我们获得了函数 f(rm) = 5.277523396998141 * rm + -10.341980537335727, 此时loss是: 50.892995693556195\n",
      "在第1636步，我们获得了函数 f(rm) = 5.278443103750749 * rm + -10.34783090052336, 此时loss是: 50.889488854261884\n",
      "在第1637步，我们获得了函数 f(rm) = 5.279362589339391 * rm + -10.353679856860916, 此时loss是: 50.88598370135995\n",
      "在第1638步，我们获得了函数 f(rm) = 5.280281853817253 * rm + -10.359527406686702, 此时loss是: 50.88248023403943\n",
      "在第1639步，我们获得了函数 f(rm) = 5.281200897237505 * rm + -10.365373550338944, 此时loss是: 50.87897845148976\n",
      "在第1640步，我们获得了函数 f(rm) = 5.282119719653306 * rm + -10.37121828815579, 此时loss是: 50.87547835290074\n",
      "在第1641步，我们获得了函数 f(rm) = 5.2830383211178 * rm + -10.377061620475303, 此时loss是: 50.87197993746258\n",
      "在第1642步，我们获得了函数 f(rm) = 5.2839567016841205 * rm + -10.382903547635467, 此时loss是: 50.86848320436587\n",
      "在第1643步，我们获得了函数 f(rm) = 5.284874861405388 * rm + -10.388744069974182, 此时loss是: 50.8649881528016\n",
      "在第1644步，我们获得了函数 f(rm) = 5.2857928003347086 * rm + -10.394583187829271, 此时loss是: 50.86149478196115\n",
      "在第1645步，我们获得了函数 f(rm) = 5.286710518525178 * rm + -10.400420901538471, 此时loss是: 50.858003091036274\n",
      "在第1646步，我们获得了函数 f(rm) = 5.287628016029876 * rm + -10.406257211439442, 此时loss是: 50.854513079219124\n",
      "在第1647步，我们获得了函数 f(rm) = 5.288545292901872 * rm + -10.41209211786976, 此时loss是: 50.85102474570223\n",
      "在第1648步，我们获得了函数 f(rm) = 5.289462349194222 * rm + -10.417925621166921, 此时loss是: 50.847538089678565\n",
      "在第1649步，我们获得了函数 f(rm) = 5.290379184959971 * rm + -10.42375772166834, 此时loss是: 50.84405311034137\n",
      "在第1650步，我们获得了函数 f(rm) = 5.291295800252146 * rm + -10.429588419711349, 此时loss是: 50.84056980688442\n",
      "在第1651步，我们获得了函数 f(rm) = 5.292212195123769 * rm + -10.435417715633202, 此时loss是: 50.837088178501766\n",
      "在第1652步，我们获得了函数 f(rm) = 5.2931283696278415 * rm + -10.44124560977107, 此时loss是: 50.83360822438787\n",
      "在第1653步，我们获得了函数 f(rm) = 5.294044323817357 * rm + -10.447072102462043, 此时loss是: 50.83012994373766\n",
      "在第1654步，我们获得了函数 f(rm) = 5.294960057745296 * rm + -10.45289719404313, 此时loss是: 50.826653335746336\n",
      "在第1655步，我们获得了函数 f(rm) = 5.295875571464625 * rm + -10.458720884851259, 此时loss是: 50.82317839960956\n",
      "在第1656步，我们获得了函数 f(rm) = 5.296790865028297 * rm + -10.464543175223277, 此时loss是: 50.81970513452336\n",
      "在第1657步，我们获得了函数 f(rm) = 5.297705938489254 * rm + -10.47036406549595, 此时loss是: 50.81623353968414\n",
      "在第1658步，我们获得了函数 f(rm) = 5.298620791900424 * rm + -10.476183556005964, 此时loss是: 50.81276361428871\n",
      "在第1659步，我们获得了函数 f(rm) = 5.299535425314724 * rm + -10.482001647089922, 此时loss是: 50.80929535753426\n",
      "在第1660步，我们获得了函数 f(rm) = 5.300449838785057 * rm + -10.487818339084347, 此时loss是: 50.80582876861835\n",
      "在第1661步，我们获得了函数 f(rm) = 5.3013640323643125 * rm + -10.493633632325682, 此时loss是: 50.80236384673894\n",
      "在第1662步，我们获得了函数 f(rm) = 5.302278006105368 * rm + -10.499447527150288, 此时loss是: 50.79890059109438\n",
      "在第1663步，我们获得了函数 f(rm) = 5.30319176006109 * rm + -10.505260023894445, 此时loss是: 50.795439000883405\n",
      "在第1664步，我们获得了函数 f(rm) = 5.304105294284329 * rm + -10.511071122894354, 此时loss是: 50.79197907530512\n",
      "在第1665步，我们获得了函数 f(rm) = 5.305018608827926 * rm + -10.516880824486131, 此时loss是: 50.78852081355903\n",
      "在第1666步，我们获得了函数 f(rm) = 5.305931703744708 * rm + -10.522689129005817, 此时loss是: 50.78506421484503\n",
      "在第1667步，我们获得了函数 f(rm) = 5.306844579087488 * rm + -10.528496036789367, 此时loss是: 50.78160927836336\n",
      "在第1668步，我们获得了函数 f(rm) = 5.307757234909068 * rm + -10.534301548172659, 此时loss是: 50.778156003314706\n",
      "在第1669步，我们获得了函数 f(rm) = 5.308669671262236 * rm + -10.540105663491486, 此时loss是: 50.7747043889001\n",
      "在第1670步，我们获得了函数 f(rm) = 5.30958188819977 * rm + -10.545908383081565, 此时loss是: 50.77125443432096\n",
      "在第1671步，我们获得了函数 f(rm) = 5.310493885774432 * rm + -10.55170970727853, 此时loss是: 50.76780613877907\n",
      "在第1672步，我们获得了函数 f(rm) = 5.311405664038972 * rm + -10.557509636417933, 此时loss是: 50.76435950147667\n",
      "在第1673步，我们获得了函数 f(rm) = 5.312317223046129 * rm + -10.563308170835247, 此时loss是: 50.76091452161629\n",
      "在第1674步，我们获得了函数 f(rm) = 5.3132285628486295 * rm + -10.569105310865865, 此时loss是: 50.75747119840093\n",
      "在第1675步，我们获得了函数 f(rm) = 5.314139683499183 * rm + -10.574901056845098, 此时loss是: 50.754029531033886\n",
      "在第1676步，我们获得了函数 f(rm) = 5.315050585050493 * rm + -10.580695409108175, 此时loss是: 50.75058951871892\n",
      "在第1677步，我们获得了函数 f(rm) = 5.3159612675552435 * rm + -10.586488367990249, 此时loss是: 50.747151160660124\n",
      "在第1678步，我们获得了函数 f(rm) = 5.3168717310661116 * rm + -10.592279933826386, 此时loss是: 50.74371445606198\n",
      "在第1679步，我们获得了函数 f(rm) = 5.317781975635758 * rm + -10.598070106951578, 此时loss是: 50.74027940412938\n",
      "在第1680步，我们获得了函数 f(rm) = 5.318692001316832 * rm + -10.603858887700733, 此时loss是: 50.736846004067566\n",
      "在第1681步，我们获得了函数 f(rm) = 5.319601808161972 * rm + -10.609646276408677, 此时loss是: 50.733414255082174\n",
      "在第1682步，我们获得了函数 f(rm) = 5.320511396223799 * rm + -10.615432273410159, 此时loss是: 50.72998415637923\n",
      "在第1683步，我们获得了函数 f(rm) = 5.321420765554927 * rm + -10.621216879039844, 此时loss是: 50.726555707165126\n",
      "在第1684步，我们获得了函数 f(rm) = 5.322329916207953 * rm + -10.62700009363232, 此时loss是: 50.723128906646664\n",
      "在第1685步，我们获得了函数 f(rm) = 5.323238848235463 * rm + -10.632781917522093, 此时loss是: 50.719703754031\n",
      "在第1686步，我们获得了函数 f(rm) = 5.324147561690032 * rm + -10.638562351043587, 此时loss是: 50.716280248525656\n",
      "在第1687步，我们获得了函数 f(rm) = 5.325056056624219 * rm + -10.644341394531148, 此时loss是: 50.712858389338585\n",
      "在第1688步，我们获得了函数 f(rm) = 5.325964333090573 * rm + -10.65011904831904, 此时loss是: 50.709438175678095\n",
      "在第1689步，我们获得了函数 f(rm) = 5.326872391141629 * rm + -10.655895312741448, 此时loss是: 50.70601960675286\n",
      "在第1690步，我们获得了函数 f(rm) = 5.3277802308299105 * rm + -10.661670188132476, 此时loss是: 50.70260268177195\n",
      "在第1691步，我们获得了函数 f(rm) = 5.328687852207927 * rm + -10.667443674826146, 此时loss是: 50.699187399944826\n",
      "在第1692步，我们获得了函数 f(rm) = 5.329595255328176 * rm + -10.673215773156402, 此时loss是: 50.69577376048131\n",
      "在第1693步，我们获得了函数 f(rm) = 5.3305024402431425 * rm + -10.678986483457107, 此时loss是: 50.692361762591595\n",
      "在第1694步，我们获得了函数 f(rm) = 5.3314094070052995 * rm + -10.684755806062045, 此时loss是: 50.688951405486314\n",
      "在第1695步，我们获得了函数 f(rm) = 5.332316155667106 * rm + -10.690523741304915, 此时loss是: 50.6855426883764\n",
      "在第1696步，我们获得了函数 f(rm) = 5.33322268628101 * rm + -10.696290289519341, 此时loss是: 50.682135610473196\n",
      "在第1697步，我们获得了函数 f(rm) = 5.334128998899444 * rm + -10.702055451038866, 此时loss是: 50.678730170988445\n",
      "在第1698步，我们获得了函数 f(rm) = 5.335035093574832 * rm + -10.707819226196952, 此时loss是: 50.67532636913427\n",
      "在第1699步，我们获得了函数 f(rm) = 5.335940970359583 * rm + -10.713581615326977, 此时loss是: 50.67192420412311\n",
      "在第1700步，我们获得了函数 f(rm) = 5.336846629306092 * rm + -10.719342618762244, 此时loss是: 50.668523675167876\n",
      "在第1701步，我们获得了函数 f(rm) = 5.3377520704667445 * rm + -10.725102236835976, 此时loss是: 50.66512478148178\n",
      "在第1702步，我们获得了函数 f(rm) = 5.338657293893911 * rm + -10.730860469881312, 此时loss是: 50.661727522278454\n",
      "在第1703步，我们获得了函数 f(rm) = 5.339562299639952 * rm + -10.736617318231314, 此时loss是: 50.6583318967719\n",
      "在第1704步，我们获得了函数 f(rm) = 5.340467087757212 * rm + -10.742372782218961, 此时loss是: 50.654937904176485\n",
      "在第1705步，我们获得了函数 f(rm) = 5.3413716582980255 * rm + -10.748126862177156, 此时loss是: 50.651545543706966\n",
      "在第1706步，我们获得了函数 f(rm) = 5.342276011314714 * rm + -10.753879558438719, 此时loss是: 50.64815481457849\n",
      "在第1707步，我们获得了函数 f(rm) = 5.343180146859584 * rm + -10.75963087133639, 此时loss是: 50.64476571600657\n",
      "在第1708步，我们获得了函数 f(rm) = 5.344084064984934 * rm + -10.765380801202829, 此时loss是: 50.641378247207044\n",
      "在第1709步，我们获得了函数 f(rm) = 5.3449877657430465 * rm + -10.77112934837062, 此时loss是: 50.637992407396226\n",
      "在第1710步，我们获得了函数 f(rm) = 5.3458912491861925 * rm + -10.77687651317226, 此时loss是: 50.634608195790754\n",
      "在第1711步，我们获得了函数 f(rm) = 5.346794515366629 * rm + -10.782622295940172, 此时loss是: 50.63122561160762\n",
      "在第1712步，我们获得了函数 f(rm) = 5.347697564336603 * rm + -10.788366697006696, 此时loss是: 50.627844654064226\n",
      "在第1713步，我们获得了函数 f(rm) = 5.348600396148347 * rm + -10.794109716704092, 此时loss是: 50.62446532237836\n",
      "在第1714步，我们获得了函数 f(rm) = 5.3495030108540815 * rm + -10.799851355364542, 此时loss是: 50.621087615768154\n",
      "在第1715步，我们获得了函数 f(rm) = 5.350405408506014 * rm + -10.805591613320146, 此时loss是: 50.617711533452145\n",
      "在第1716步，我们获得了函数 f(rm) = 5.351307589156342 * rm + -10.811330490902927, 此时loss是: 50.61433707464921\n",
      "在第1717步，我们获得了函数 f(rm) = 5.3522095528572455 * rm + -10.817067988444824, 此时loss是: 50.61096423857864\n",
      "在第1718步，我们获得了函数 f(rm) = 5.353111299660897 * rm + -10.822804106277701, 此时loss是: 50.60759302446007\n",
      "在第1719步，我们获得了函数 f(rm) = 5.354012829619453 * rm + -10.82853884473334, 此时loss是: 50.60422343151353\n",
      "在第1720步，我们获得了函数 f(rm) = 5.354914142785059 * rm + -10.834272204143439, 此时loss是: 50.60085545895944\n",
      "在第1721步，我们获得了函数 f(rm) = 5.355815239209848 * rm + -10.840004184839623, 此时loss是: 50.597489106018536\n",
      "在第1722步，我们获得了函数 f(rm) = 5.356716118945941 * rm + -10.845734787153436, 此时loss是: 50.59412437191201\n",
      "在第1723步，我们获得了函数 f(rm) = 5.357616782045444 * rm + -10.851464011416338, 此时loss是: 50.59076125586136\n",
      "在第1724步，我们获得了函数 f(rm) = 5.358517228560452 * rm + -10.857191857959714, 此时loss是: 50.58739975708847\n",
      "在第1725步，我们获得了函数 f(rm) = 5.359417458543049 * rm + -10.862918327114865, 此时loss是: 50.584039874815666\n",
      "在第1726步，我们获得了函数 f(rm) = 5.360317472045305 * rm + -10.868643419213017, 此时loss是: 50.58068160826553\n",
      "在第1727步，我们获得了函数 f(rm) = 5.361217269119276 * rm + -10.874367134585313, 此时loss是: 50.57732495666114\n",
      "在第1728步，我们获得了函数 f(rm) = 5.362116849817008 * rm + -10.880089473562817, 此时loss是: 50.57396991922585\n",
      "在第1729步，我们获得了函数 f(rm) = 5.363016214190534 * rm + -10.885810436476515, 此时loss是: 50.570616495183444\n",
      "在第1730步，我们获得了函数 f(rm) = 5.3639153622918725 * rm + -10.89153002365731, 此时loss是: 50.567264683758054\n",
      "在第1731步，我们获得了函数 f(rm) = 5.364814294173033 * rm + -10.897248235436031, 此时loss是: 50.563914484174205\n",
      "在第1732步，我们获得了函数 f(rm) = 5.365713009886008 * rm + -10.90296507214342, 此时loss是: 50.56056589565678\n",
      "在第1733步，我们获得了函数 f(rm) = 5.366611509482781 * rm + -10.908680534110147, 此时loss是: 50.557218917431044\n",
      "在第1734步，我们获得了函数 f(rm) = 5.367509793015323 * rm + -10.914394621666796, 此时loss是: 50.55387354872261\n",
      "在第1735步，我们获得了函数 f(rm) = 5.368407860535591 * rm + -10.920107335143877, 此时loss是: 50.550529788757515\n",
      "在第1736步，我们获得了函数 f(rm) = 5.369305712095528 * rm + -10.925818674871817, 此时loss是: 50.5471876367621\n",
      "在第1737步，我们获得了函数 f(rm) = 5.370203347747069 * rm + -10.931528641180964, 此时loss是: 50.54384709196312\n",
      "在第1738步，我们获得了函数 f(rm) = 5.371100767542133 * rm + -10.93723723440159, 此时loss是: 50.54050815358772\n",
      "在第1739步，我们获得了函数 f(rm) = 5.371997971532628 * rm + -10.94294445486388, 此时loss是: 50.53717082086338\n",
      "在第1740步，我们获得了函数 f(rm) = 5.372894959770448 * rm + -10.94865030289795, 此时loss是: 50.53383509301795\n",
      "在第1741步，我们获得了函数 f(rm) = 5.373791732307476 * rm + -10.954354778833824, 此时loss是: 50.530500969279686\n",
      "在第1742步，我们获得了函数 f(rm) = 5.374688289195582 * rm + -10.96005788300146, 此时loss是: 50.527168448877184\n",
      "在第1743步，我们获得了函数 f(rm) = 5.375584630486625 * rm + -10.965759615730725, 此时loss是: 50.52383753103941\n",
      "在第1744步，我们获得了函数 f(rm) = 5.376480756232447 * rm + -10.971459977351417, 此时loss是: 50.52050821499573\n",
      "在第1745步，我们获得了函数 f(rm) = 5.377376666484883 * rm + -10.977158968193246, 此时loss是: 50.517180499975865\n",
      "在第1746步，我们获得了函数 f(rm) = 5.378272361295753 * rm + -10.982856588585847, 此时loss是: 50.51385438520989\n",
      "在第1747步，我们获得了函数 f(rm) = 5.379167840716864 * rm + -10.988552838858775, 此时loss是: 50.51052986992828\n",
      "在第1748步，我们获得了函数 f(rm) = 5.380063104800012 * rm + -10.994247719341507, 此时loss是: 50.50720695336182\n",
      "在第1749步，我们获得了函数 f(rm) = 5.380958153596979 * rm + -10.999941230363438, 此时loss是: 50.50388563474177\n",
      "在第1750步，我们获得了函数 f(rm) = 5.381852987159537 * rm + -11.005633372253886, 此时loss是: 50.50056591329967\n",
      "在第1751步，我们获得了函数 f(rm) = 5.382747605539441 * rm + -11.011324145342089, 此时loss是: 50.49724778826745\n",
      "在第1752步，我们获得了函数 f(rm) = 5.383642008788439 * rm + -11.017013549957205, 此时loss是: 50.493931258877446\n",
      "在第1753步，我们获得了函数 f(rm) = 5.384536196958264 * rm + -11.022701586428317, 此时loss是: 50.49061632436229\n",
      "在第1754步，我们获得了函数 f(rm) = 5.385430170100634 * rm + -11.028388255084423, 此时loss是: 50.48730298395507\n",
      "在第1755步，我们获得了函数 f(rm) = 5.38632392826726 * rm + -11.034073556254445, 此时loss是: 50.48399123688918\n",
      "在第1756步，我们获得了函数 f(rm) = 5.387217471509837 * rm + -11.039757490267226, 此时loss是: 50.48068108239842\n",
      "在第1757步，我们获得了函数 f(rm) = 5.388110799880047 * rm + -11.045440057451529, 此时loss是: 50.477372519716916\n",
      "在第1758步，我们获得了函数 f(rm) = 5.389003913429562 * rm + -11.051121258136037, 此时loss是: 50.47406554807921\n",
      "在第1759步，我们获得了函数 f(rm) = 5.38989681221004 * rm + -11.056801092649359, 此时loss是: 50.470760166720176\n",
      "在第1760步，我们获得了函数 f(rm) = 5.390789496273127 * rm + -11.062479561320018, 此时loss是: 50.46745637487509\n",
      "在第1761步，我们获得了函数 f(rm) = 5.3916819656704575 * rm + -11.068156664476462, 此时loss是: 50.464154171779555\n",
      "在第1762步，我们获得了函数 f(rm) = 5.3925742204536515 * rm + -11.073832402447058, 此时loss是: 50.46085355666956\n",
      "在第1763步，我们获得了函数 f(rm) = 5.393466260674318 * rm + -11.079506775560098, 此时loss是: 50.45755452878149\n",
      "在第1764步，我们获得了函数 f(rm) = 5.394358086384053 * rm + -11.085179784143792, 此时loss是: 50.45425708735204\n",
      "在第1765步，我们获得了函数 f(rm) = 5.3952496976344415 * rm + -11.09085142852627, 此时loss是: 50.45096123161833\n",
      "在第1766步，我们获得了函数 f(rm) = 5.396141094477055 * rm + -11.096521709035585, 此时loss是: 50.4476669608178\n",
      "在第1767步，我们获得了函数 f(rm) = 5.397032276963451 * rm + -11.102190625999711, 此时loss是: 50.44437427418829\n",
      "在第1768步，我们获得了函数 f(rm) = 5.397923245145177 * rm + -11.107858179746543, 此时loss是: 50.44108317096798\n",
      "在第1769步，我们获得了函数 f(rm) = 5.398813999073768 * rm + -11.113524370603896, 此时loss是: 50.43779365039545\n",
      "在第1770步，我们获得了函数 f(rm) = 5.399704538800746 * rm + -11.119189198899509, 此时loss是: 50.43450571170961\n",
      "在第1771步，我们获得了函数 f(rm) = 5.400594864377619 * rm + -11.124852664961038, 此时loss是: 50.43121935414976\n",
      "在第1772步，我们获得了函数 f(rm) = 5.401484975855886 * rm + -11.130514769116063, 此时loss是: 50.42793457695554\n",
      "在第1773步，我们获得了函数 f(rm) = 5.40237487328703 * rm + -11.136175511692086, 此时loss是: 50.42465137936699\n",
      "在第1774步，我们获得了函数 f(rm) = 5.403264556722526 * rm + -11.141834893016528, 此时loss是: 50.42136976062451\n",
      "在第1775步，我们获得了函数 f(rm) = 5.404154026213831 * rm + -11.147492913416732, 此时loss是: 50.41808971996881\n",
      "在第1776步，我们获得了函数 f(rm) = 5.405043281812395 * rm + -11.153149573219965, 此时loss是: 50.41481125664106\n",
      "在第1777步，我们获得了函数 f(rm) = 5.405932323569652 * rm + -11.158804872753409, 此时loss是: 50.41153436988271\n",
      "在第1778步，我们获得了函数 f(rm) = 5.4068211515370255 * rm + -11.164458812344174, 此时loss是: 50.408259058935634\n",
      "在第1779步，我们获得了函数 f(rm) = 5.407709765765925 * rm + -11.170111392319289, 此时loss是: 50.404985323042006\n",
      "在第1780步，我们获得了函数 f(rm) = 5.40859816630775 * rm + -11.1757626130057, 此时loss是: 50.401713161444455\n",
      "在第1781步，我们获得了函数 f(rm) = 5.409486353213886 * rm + -11.181412474730282, 此时loss是: 50.39844257338588\n",
      "在第1782步，我们获得了函数 f(rm) = 5.410374326535706 * rm + -11.187060977819826, 此时loss是: 50.39517355810962\n",
      "在第1783步，我们获得了函数 f(rm) = 5.411262086324571 * rm + -11.192708122601045, 此时loss是: 50.39190611485933\n",
      "在第1784步，我们获得了函数 f(rm) = 5.412149632631831 * rm + -11.198353909400577, 此时loss是: 50.38864024287903\n",
      "在第1785步，我们获得了函数 f(rm) = 5.41303696550882 * rm + -11.203998338544977, 此时loss是: 50.385375941413116\n",
      "在第1786步，我们获得了函数 f(rm) = 5.413924085006864 * rm + -11.209641410360726, 此时loss是: 50.382113209706375\n",
      "在第1787步，我们获得了函数 f(rm) = 5.414810991177275 * rm + -11.21528312517422, 此时loss是: 50.37885204700393\n",
      "在第1788步，我们获得了函数 f(rm) = 5.415697684071351 * rm + -11.220923483311784, 此时loss是: 50.37559245255125\n",
      "在第1789步，我们获得了函数 f(rm) = 5.416584163740381 * rm + -11.22656248509966, 此时loss是: 50.3723344255942\n",
      "在第1790步，我们获得了函数 f(rm) = 5.417470430235637 * rm + -11.23220013086401, 此时loss是: 50.36907796537896\n",
      "在第1791步，我们获得了函数 f(rm) = 5.418356483608383 * rm + -11.237836420930924, 此时loss是: 50.36582307115214\n",
      "在第1792步，我们获得了函数 f(rm) = 5.419242323909869 * rm + -11.243471355626406, 此时loss是: 50.36256974216066\n",
      "在第1793步，我们获得了函数 f(rm) = 5.420127951191332 * rm + -11.249104935276387, 此时loss是: 50.359317977651834\n",
      "在第1794步，我们获得了函数 f(rm) = 5.421013365503998 * rm + -11.254737160206718, 此时loss是: 50.35606777687331\n",
      "在第1795步，我们获得了函数 f(rm) = 5.421898566899081 * rm + -11.26036803074317, 此时loss是: 50.3528191390731\n",
      "在第1796步，我们获得了函数 f(rm) = 5.42278355542778 * rm + -11.26599754721144, 此时loss是: 50.349572063499615\n",
      "在第1797步，我们获得了函数 f(rm) = 5.423668331141285 * rm + -11.27162570993714, 此时loss是: 50.34632654940158\n",
      "在第1798步，我们获得了函数 f(rm) = 5.424552894090771 * rm + -11.277252519245812, 此时loss是: 50.34308259602812\n",
      "在第1799步，我们获得了函数 f(rm) = 5.425437244327402 * rm + -11.282877975462911, 此时loss是: 50.33984020262868\n",
      "在第1800步，我们获得了函数 f(rm) = 5.42632138190233 * rm + -11.28850207891382, 此时loss是: 50.33659936845311\n",
      "在第1801步，我们获得了函数 f(rm) = 5.427205306866695 * rm + -11.294124829923843, 此时loss是: 50.33336009275158\n",
      "在第1802步，我们获得了函数 f(rm) = 5.428089019271622 * rm + -11.299746228818202, 此时loss是: 50.330122374774675\n",
      "在第1803步，我们获得了函数 f(rm) = 5.4289725191682265 * rm + -11.305366275922045, 此时loss是: 50.32688621377328\n",
      "在第1804步，我们获得了函数 f(rm) = 5.429855806607612 * rm + -11.31098497156044, 此时loss是: 50.32365160899866\n",
      "在第1805步，我们获得了函数 f(rm) = 5.430738881640866 * rm + -11.316602316058377, 此时loss是: 50.32041855970246\n",
      "在第1806步，我们获得了函数 f(rm) = 5.431621744319069 * rm + -11.322218309740768, 此时loss是: 50.31718706513667\n",
      "在第1807步，我们获得了函数 f(rm) = 5.432504394693286 * rm + -11.327832952932445, 此时loss是: 50.31395712455363\n",
      "在第1808步，我们获得了函数 f(rm) = 5.433386832814568 * rm + -11.333446245958164, 此时loss是: 50.31072873720606\n",
      "在第1809步，我们获得了函数 f(rm) = 5.434269058733959 * rm + -11.339058189142603, 此时loss是: 50.30750190234705\n",
      "在第1810步，我们获得了函数 f(rm) = 5.435151072502485 * rm + -11.344668782810363, 此时loss是: 50.30427661922999\n",
      "在第1811步，我们获得了函数 f(rm) = 5.436032874171164 * rm + -11.350278027285963, 此时loss是: 50.30105288710868\n",
      "在第1812步，我们获得了函数 f(rm) = 5.436914463790999 * rm + -11.355885922893847, 此时loss是: 50.29783070523728\n",
      "在第1813步，我们获得了函数 f(rm) = 5.437795841412984 * rm + -11.36149246995838, 此时loss是: 50.294610072870285\n",
      "在第1814步，我们获得了函数 f(rm) = 5.4386770070880965 * rm + -11.367097668803849, 此时loss是: 50.29139098926257\n",
      "在第1815步，我们获得了函数 f(rm) = 5.439557960867305 * rm + -11.372701519754465, 此时loss是: 50.28817345366934\n",
      "在第1816步，我们获得了函数 f(rm) = 5.440438702801563 * rm + -11.378304023134357, 此时loss是: 50.28495746534619\n",
      "在第1817步，我们获得了函数 f(rm) = 5.441319232941814 * rm + -11.38390517926758, 此时loss是: 50.28174302354904\n",
      "在第1818步，我们获得了函数 f(rm) = 5.442199551338991 * rm + -11.38950498847811, 此时loss是: 50.278530127534204\n",
      "在第1819步，我们获得了函数 f(rm) = 5.4430796580440095 * rm + -11.395103451089843, 此时loss是: 50.27531877655834\n",
      "在第1820步，我们获得了函数 f(rm) = 5.443959553107777 * rm + -11.4007005674266, 此时loss是: 50.272108969878445\n",
      "在第1821步，我们获得了函数 f(rm) = 5.444839236581186 * rm + -11.406296337812122, 此时loss是: 50.26890070675189\n",
      "在第1822步，我们获得了函数 f(rm) = 5.445718708515119 * rm + -11.411890762570073, 此时loss是: 50.26569398643641\n",
      "在第1823步，我们获得了函数 f(rm) = 5.446597968960446 * rm + -11.417483842024039, 此时loss是: 50.26248880819008\n",
      "在第1824步，我们获得了函数 f(rm) = 5.447477017968023 * rm + -11.423075576497528, 此时loss是: 50.25928517127133\n",
      "在第1825步，我们获得了函数 f(rm) = 5.448355855588695 * rm + -11.428665966313972, 此时loss是: 50.256083074938985\n",
      "在第1826步，我们获得了函数 f(rm) = 5.449234481873296 * rm + -11.434255011796722, 此时loss是: 50.25288251845216\n",
      "在第1827步，我们获得了函数 f(rm) = 5.450112896872644 * rm + -11.439842713269053, 此时loss是: 50.24968350107039\n",
      "在第1828步，我们获得了函数 f(rm) = 5.45099110063755 * rm + -11.445429071054164, 此时loss是: 50.24648602205354\n",
      "在第1829步，我们获得了函数 f(rm) = 5.451869093218809 * rm + -11.451014085475173, 此时loss是: 50.243290080661815\n",
      "在第1830步，我们获得了函数 f(rm) = 5.452746874667204 * rm + -11.456597756855121, 此时loss是: 50.240095676155796\n",
      "在第1831步，我们获得了函数 f(rm) = 5.453624445033507 * rm + -11.462180085516975, 此时loss是: 50.23690280779643\n",
      "在第1832步，我们获得了函数 f(rm) = 5.454501804368478 * rm + -11.467761071783618, 此时loss是: 50.23371147484499\n",
      "在第1833步，我们获得了函数 f(rm) = 5.455378952722864 * rm + -11.473340715977862, 此时loss是: 50.23052167656312\n",
      "在第1834步，我们获得了函数 f(rm) = 5.456255890147399 * rm + -11.478919018422436, 此时loss是: 50.22733341221282\n",
      "在第1835步，我们获得了函数 f(rm) = 5.457132616692808 * rm + -11.484495979439995, 此时loss是: 50.224146681056446\n",
      "在第1836步，我们获得了函数 f(rm) = 5.458009132409798 * rm + -11.490071599353113, 此时loss是: 50.2209614823567\n",
      "在第1837步，我们获得了函数 f(rm) = 5.458885437349071 * rm + -11.495645878484291, 此时loss是: 50.217777815376635\n",
      "在第1838步，我们获得了函数 f(rm) = 5.459761531561311 * rm + -11.501218817155948, 此时loss是: 50.214595679379705\n",
      "在第1839步，我们获得了函数 f(rm) = 5.460637415097192 * rm + -11.506790415690428, 此时loss是: 50.211415073629624\n",
      "在第1840步，我们获得了函数 f(rm) = 5.461513088007378 * rm + -11.512360674409997, 此时loss是: 50.208235997390574\n",
      "在第1841步，我们获得了函数 f(rm) = 5.462388550342516 * rm + -11.517929593636843, 此时loss是: 50.205058449926995\n",
      "在第1842步，我们获得了函数 f(rm) = 5.463263802153246 * rm + -11.523497173693077, 此时loss是: 50.20188243050372\n",
      "在第1843步，我们获得了函数 f(rm) = 5.464138843490191 * rm + -11.529063414900731, 此时loss是: 50.19870793838596\n",
      "在第1844步，我们获得了函数 f(rm) = 5.465013674403965 * rm + -11.534628317581763, 此时loss是: 50.19553497283923\n",
      "在第1845步，我们获得了函数 f(rm) = 5.465888294945168 * rm + -11.540191882058052, 此时loss是: 50.19236353312944\n",
      "在第1846步，我们获得了函数 f(rm) = 5.466762705164391 * rm + -11.545754108651396, 此时loss是: 50.18919361852282\n",
      "在第1847步，我们获得了函数 f(rm) = 5.467636905112209 * rm + -11.551314997683521, 此时loss是: 50.186025228285985\n",
      "在第1848步，我们获得了函数 f(rm) = 5.468510894839186 * rm + -11.556874549476074, 此时loss是: 50.18285836168587\n",
      "在第1849步，我们获得了函数 f(rm) = 5.469384674395876 * rm + -11.562432764350623, 此时loss是: 50.17969301798977\n",
      "在第1850步，我们获得了函数 f(rm) = 5.470258243832818 * rm + -11.56798964262866, 此时loss是: 50.17652919646537\n",
      "在第1851步，我们获得了函数 f(rm) = 5.47113160320054 * rm + -11.5735451846316, 此时loss是: 50.173366896380664\n",
      "在第1852步，我们获得了函数 f(rm) = 5.472004752549558 * rm + -11.579099390680778, 此时loss是: 50.17020611700401\n",
      "在第1853步，我们获得了函数 f(rm) = 5.472877691930376 * rm + -11.584652261097455, 此时loss是: 50.16704685760412\n",
      "在第1854步，我们获得了函数 f(rm) = 5.4737504213934844 * rm + -11.590203796202815, 此时loss是: 50.163889117450054\n",
      "在第1855步，我们获得了函数 f(rm) = 5.474622940989364 * rm + -11.59575399631796, 此时loss是: 50.160732895811236\n",
      "在第1856步，我们获得了函数 f(rm) = 5.47549525076848 * rm + -11.601302861763921, 此时loss是: 50.15757819195743\n",
      "在第1857步，我们获得了函数 f(rm) = 5.47636735078129 * rm + -11.606850392861649, 此时loss是: 50.15442500515874\n",
      "在第1858步，我们获得了函数 f(rm) = 5.4772392410782365 * rm + -11.612396589932015, 此时loss是: 50.15127333468564\n",
      "在第1859步，我们获得了函数 f(rm) = 5.478110921709749 * rm + -11.617941453295819, 此时loss是: 50.14812317980897\n",
      "在第1860步，我们获得了函数 f(rm) = 5.478982392726247 * rm + -11.623484983273778, 此时loss是: 50.144974539799875\n",
      "在第1861步，我们获得了函数 f(rm) = 5.479853654178137 * rm + -11.629027180186535, 此时loss是: 50.14182741392989\n",
      "在第1862步，我们获得了函数 f(rm) = 5.480724706115813 * rm + -11.634568044354655, 此时loss是: 50.13868180147087\n",
      "在第1863步，我们获得了函数 f(rm) = 5.481595548589659 * rm + -11.640107576098627, 此时loss是: 50.13553770169506\n",
      "在第1864步，我们获得了函数 f(rm) = 5.482466181650043 * rm + -11.645645775738862, 此时loss是: 50.132395113875\n",
      "在第1865步，我们获得了函数 f(rm) = 5.483336605347325 * rm + -11.651182643595693, 此时loss是: 50.129254037283644\n",
      "在第1866步，我们获得了函数 f(rm) = 5.48420681973185 * rm + -11.656718179989376, 此时loss是: 50.12611447119422\n",
      "在第1867步，我们获得了函数 f(rm) = 5.485076824853952 * rm + -11.662252385240093, 此时loss是: 50.1229764148804\n",
      "在第1868步，我们获得了函数 f(rm) = 5.485946620763953 * rm + -11.667785259667946, 此时loss是: 50.119839867616115\n",
      "在第1869步，我们获得了函数 f(rm) = 5.486816207512162 * rm + -11.673316803592963, 此时loss是: 50.116704828675694\n",
      "在第1870步，我们获得了函数 f(rm) = 5.487685585148879 * rm + -11.678847017335091, 此时loss是: 50.11357129733379\n",
      "在第1871步，我们获得了函数 f(rm) = 5.488554753724387 * rm + -11.684375901214201, 此时loss是: 50.11043927286545\n",
      "在第1872步，我们获得了函数 f(rm) = 5.48942371328896 * rm + -11.689903455550091, 此时loss是: 50.10730875454602\n",
      "在第1873步，我们获得了函数 f(rm) = 5.490292463892859 * rm + -11.695429680662478, 此时loss是: 50.104179741651215\n",
      "在第1874步，我们获得了函数 f(rm) = 5.491161005586335 * rm + -11.700954576871004, 此时loss是: 50.10105223345709\n",
      "在第1875步，我们获得了函数 f(rm) = 5.4920293384196235 * rm + -11.70647814449523, 此时loss是: 50.09792622924007\n",
      "在第1876步，我们获得了函数 f(rm) = 5.49289746244295 * rm + -11.71200038385465, 此时loss是: 50.0948017282769\n",
      "在第1877步，我们获得了函数 f(rm) = 5.493765377706528 * rm + -11.717521295268668, 此时loss是: 50.09167872984469\n",
      "在第1878步，我们获得了函数 f(rm) = 5.494633084260557 * rm + -11.723040879056622, 此时loss是: 50.088557233220875\n",
      "在第1879步，我们获得了函数 f(rm) = 5.495500582155228 * rm + -11.72855913553777, 此时loss是: 50.08543723768329\n",
      "在第1880步，我们获得了函数 f(rm) = 5.496367871440716 * rm + -11.73407606503129, 此时loss是: 50.08231874251005\n",
      "在第1881步，我们获得了函数 f(rm) = 5.497234952167186 * rm + -11.739591667856288, 此时loss是: 50.07920174697966\n",
      "在第1882步，我们获得了函数 f(rm) = 5.498101824384792 * rm + -11.745105944331788, 此时loss是: 50.076086250370984\n",
      "在第1883步，我们获得了函数 f(rm) = 5.498968488143673 * rm + -11.750618894776743, 此时loss是: 50.07297225196318\n",
      "在第1884步，我们获得了函数 f(rm) = 5.4998349434939575 * rm + -11.756130519510025, 此时loss是: 50.0698597510358\n",
      "在第1885步，我们获得了函数 f(rm) = 5.5007011904857634 * rm + -11.761640818850433, 此时loss是: 50.06674874686871\n",
      "在第1886步，我们获得了函数 f(rm) = 5.501567229169194 * rm + -11.767149793116685, 此时loss是: 50.06363923874216\n",
      "在第1887步，我们获得了函数 f(rm) = 5.502433059594343 * rm + -11.772657442627425, 此时loss是: 50.0605312259367\n",
      "在第1888步，我们获得了函数 f(rm) = 5.503298681811289 * rm + -11.778163767701221, 此时loss是: 50.05742470773328\n",
      "在第1889步，我们获得了函数 f(rm) = 5.5041640958701015 * rm + -11.783668768656561, 此时loss是: 50.054319683413134\n",
      "在第1890步，我们获得了函数 f(rm) = 5.505029301820836 * rm + -11.789172445811861, 此时loss是: 50.051216152257894\n",
      "在第1891步，我们获得了函数 f(rm) = 5.505894299713537 * rm + -11.794674799485458, 此时loss是: 50.04811411354951\n",
      "在第1892步，我们获得了函数 f(rm) = 5.506759089598237 * rm + -11.800175829995611, 此时loss是: 50.04501356657028\n",
      "在第1893步，我们获得了函数 f(rm) = 5.507623671524956 * rm + -11.805675537660507, 此时loss是: 50.04191451060286\n",
      "在第1894步，我们获得了函数 f(rm) = 5.508488045543702 * rm + -11.811173922798252, 此时loss是: 50.03881694493022\n",
      "在第1895步，我们获得了函数 f(rm) = 5.5093522117044715 * rm + -11.816670985726876, 此时loss是: 50.035720868835746\n",
      "在第1896步，我们获得了函数 f(rm) = 5.510216170057248 * rm + -11.822166726764335, 此时loss是: 50.03262628160306\n",
      "在第1897步，我们获得了函数 f(rm) = 5.511079920652004 * rm + -11.827661146228508, 此时loss是: 50.02953318251623\n",
      "在第1898步，我们获得了函数 f(rm) = 5.511943463538699 * rm + -11.833154244437194, 此时loss是: 50.026441570859625\n",
      "在第1899步，我们获得了函数 f(rm) = 5.512806798767281 * rm + -11.83864602170812, 此时loss是: 50.023351445917925\n",
      "在第1900步，我们获得了函数 f(rm) = 5.513669926387687 * rm + -11.844136478358935, 此时loss是: 50.02026280697623\n",
      "在第1901步，我们获得了函数 f(rm) = 5.514532846449839 * rm + -11.849625614707211, 此时loss是: 50.01717565331992\n",
      "在第1902步，我们获得了函数 f(rm) = 5.515395559003652 * rm + -11.855113431070444, 此时loss是: 50.01408998423474\n",
      "在第1903步，我们获得了函数 f(rm) = 5.5162580640990235 * rm + -11.860599927766055, 此时loss是: 50.011005799006774\n",
      "在第1904步，我们获得了函数 f(rm) = 5.517120361785842 * rm + -11.866085105111386, 此时loss是: 50.007923096922475\n",
      "在第1905步，我们获得了函数 f(rm) = 5.517982452113986 * rm + -11.871568963423705, 此时loss是: 50.00484187726863\n",
      "在第1906步，我们获得了函数 f(rm) = 5.518844335133316 * rm + -11.877051503020203, 此时loss是: 50.00176213933231\n",
      "在第1907步，我们获得了函数 f(rm) = 5.519706010893685 * rm + -11.882532724217993, 此时loss是: 49.99868388240101\n",
      "在第1908步，我们获得了函数 f(rm) = 5.520567479444934 * rm + -11.888012627334115, 此时loss是: 49.995607105762524\n",
      "在第1909步，我们获得了函数 f(rm) = 5.521428740836891 * rm + -11.89349121268553, 此时loss是: 49.99253180870501\n",
      "在第1910步，我们获得了函数 f(rm) = 5.522289795119371 * rm + -11.898968480589126, 此时loss是: 49.989457990516954\n",
      "在第1911步，我们获得了函数 f(rm) = 5.523150642342178 * rm + -11.90444443136171, 此时loss是: 49.98638565048717\n",
      "在第1912步，我们获得了函数 f(rm) = 5.524011282555105 * rm + -11.909919065320016, 此时loss是: 49.98331478790486\n",
      "在第1913步，我们获得了函数 f(rm) = 5.524871715807933 * rm + -11.915392382780704, 此时loss是: 49.980245402059516\n",
      "在第1914步，我们获得了函数 f(rm) = 5.525731942150428 * rm + -11.920864384060351, 此时loss是: 49.977177492241\n",
      "在第1915步，我们获得了函数 f(rm) = 5.526591961632348 * rm + -11.926335069475465, 此时loss是: 49.97411105773951\n",
      "在第1916步，我们获得了函数 f(rm) = 5.527451774303436 * rm + -11.931804439342475, 此时loss是: 49.9710460978456\n",
      "在第1917步，我们获得了函数 f(rm) = 5.528311380213425 * rm + -11.937272493977732, 此时loss是: 49.96798261185014\n",
      "在第1918步，我们获得了函数 f(rm) = 5.529170779412034 * rm + -11.942739233697512, 此时loss是: 49.96492059904435\n",
      "在第1919步，我们获得了函数 f(rm) = 5.530029971948974 * rm + -11.94820465881802, 此时loss是: 49.961860058719786\n",
      "在第1920步，我们获得了函数 f(rm) = 5.530888957873938 * rm + -11.953668769655376, 此时loss是: 49.95880099016837\n",
      "在第1921步，我们获得了函数 f(rm) = 5.531747737236614 * rm + -11.959131566525631, 此时loss是: 49.95574339268232\n",
      "在第1922步，我们获得了函数 f(rm) = 5.532606310086672 * rm + -11.964593049744757, 此时loss是: 49.95268726555425\n",
      "在第1923步，我们获得了函数 f(rm) = 5.533464676473773 * rm + -11.970053219628651, 此时loss是: 49.94963260807706\n",
      "在第1924步，我们获得了函数 f(rm) = 5.534322836447567 * rm + -11.975512076493134, 此时loss是: 49.94657941954404\n",
      "在第1925步，我们获得了函数 f(rm) = 5.535180790057688 * rm + -11.98096962065395, 此时loss是: 49.94352769924877\n",
      "在第1926步，我们获得了函数 f(rm) = 5.536038537353764 * rm + -11.986425852426768, 此时loss是: 49.9404774464852\n",
      "在第1927步，我们获得了函数 f(rm) = 5.536896078385405 * rm + -11.991880772127182, 此时loss是: 49.93742866054762\n",
      "在第1928步，我们获得了函数 f(rm) = 5.537753413202213 * rm + -11.997334380070708, 此时loss是: 49.93438134073064\n",
      "在第1929步，我们获得了函数 f(rm) = 5.538610541853778 * rm + -12.002786676572786, 此时loss是: 49.93133548632924\n",
      "在第1930步，我们获得了函数 f(rm) = 5.539467464389675 * rm + -12.008237661948783, 此时loss是: 49.92829109663871\n",
      "在第1931步，我们获得了函数 f(rm) = 5.5403241808594705 * rm + -12.01368733651399, 此时loss是: 49.925248170954696\n",
      "在第1932步，我们获得了函数 f(rm) = 5.541180691312717 * rm + -12.019135700583616, 此时loss是: 49.92220670857317\n",
      "在第1933步，我们获得了函数 f(rm) = 5.542036995798957 * rm + -12.024582754472803, 此时loss是: 49.919166708790456\n",
      "在第1934步，我们获得了函数 f(rm) = 5.542893094367718 * rm + -12.03002849849661, 此时loss是: 49.91612817090319\n",
      "在第1935步，我们获得了函数 f(rm) = 5.543748987068518 * rm + -12.035472932970027, 此时loss是: 49.9130910942084\n",
      "在第1936步，我们获得了函数 f(rm) = 5.544604673950864 * rm + -12.04091605820796, 此时loss是: 49.9100554780034\n",
      "在第1937步，我们获得了函数 f(rm) = 5.545460155064248 * rm + -12.046357874525247, 此时loss是: 49.90702132158585\n",
      "在第1938步，我们获得了函数 f(rm) = 5.546315430458153 * rm + -12.051798382236647, 此时loss是: 49.90398862425377\n",
      "在第1939步，我们获得了函数 f(rm) = 5.547170500182046 * rm + -12.057237581656842, 此时loss是: 49.9009573853055\n",
      "在第1940步，我们获得了函数 f(rm) = 5.5480253642853885 * rm + -12.062675473100441, 此时loss是: 49.897927604039744\n",
      "在第1941步，我们获得了函数 f(rm) = 5.548880022817625 * rm + -12.068112056881976, 此时loss是: 49.894899279755485\n",
      "在第1942步，我们获得了函数 f(rm) = 5.549734475828189 * rm + -12.073547333315902, 此时loss是: 49.8918724117521\n",
      "在第1943步，我们获得了函数 f(rm) = 5.550588723366504 * rm + -12.0789813027166, 此时loss是: 49.888846999329296\n",
      "在第1944步，我们获得了函数 f(rm) = 5.551442765481979 * rm + -12.084413965398378, 此时loss是: 49.88582304178707\n",
      "在第1945步，我们获得了函数 f(rm) = 5.552296602224014 * rm + -12.089845321675464, 此时loss是: 49.88280053842581\n",
      "在第1946步，我们获得了函数 f(rm) = 5.553150233641995 * rm + -12.09527537186201, 此时loss是: 49.87977948854623\n",
      "在第1947步，我们获得了函数 f(rm) = 5.554003659785296 * rm + -12.100704116272098, 此时loss是: 49.87675989144935\n",
      "在第1948步，我们获得了函数 f(rm) = 5.554856880703282 * rm + -12.10613155521973, 此时loss是: 49.87374174643655\n",
      "在第1949步，我们获得了函数 f(rm) = 5.555709896445302 * rm + -12.11155768901883, 此时loss是: 49.87072505280956\n",
      "在第1950步，我们获得了函数 f(rm) = 5.556562707060695 * rm + -12.116982517983255, 此时loss是: 49.8677098098704\n",
      "在第1951步，我们获得了函数 f(rm) = 5.557415312598789 * rm + -12.122406042426778, 此时loss是: 49.86469601692147\n",
      "在第1952步，我们获得了函数 f(rm) = 5.5582677131089 * rm + -12.127828262663103, 此时loss是: 49.86168367326549\n",
      "在第1953步，我们获得了函数 f(rm) = 5.55911990864033 * rm + -12.133249179005853, 此时loss是: 49.858672778205516\n",
      "在第1954步，我们获得了函数 f(rm) = 5.559971899242372 * rm + -12.13866879176858, 此时loss是: 49.855663331044916\n",
      "在第1955步，我们获得了函数 f(rm) = 5.560823684964306 * rm + -12.144087101264759, 此时loss是: 49.85265533108744\n",
      "在第1956步，我们获得了函数 f(rm) = 5.561675265855398 * rm + -12.149504107807788, 此时loss是: 49.84964877763714\n",
      "在第1957步，我们获得了函数 f(rm) = 5.562526641964906 * rm + -12.154919811710993, 此时loss是: 49.84664366999841\n",
      "在第1958步，我们获得了函数 f(rm) = 5.563377813342075 * rm + -12.160334213287621, 此时loss是: 49.84364000747596\n",
      "在第1959步，我们获得了函数 f(rm) = 5.564228780036135 * rm + -12.165747312850845, 此时loss是: 49.84063778937488\n",
      "在第1960步，我们获得了函数 f(rm) = 5.565079542096308 * rm + -12.171159110713766, 此时loss是: 49.83763701500056\n",
      "在第1961步，我们获得了函数 f(rm) = 5.565930099571803 * rm + -12.176569607189405, 此时loss是: 49.83463768365872\n",
      "在第1962步，我们获得了函数 f(rm) = 5.566780452511817 * rm + -12.18197880259071, 此时loss是: 49.83163979465544\n",
      "在第1963步，我们获得了函数 f(rm) = 5.567630600965534 * rm + -12.18738669723055, 此时loss是: 49.82864334729711\n",
      "在第1964步，我们获得了函数 f(rm) = 5.568480544982128 * rm + -12.192793291421728, 此时loss是: 49.825648340890446\n",
      "在第1965步，我们获得了函数 f(rm) = 5.569330284610761 * rm + -12.19819858547696, 此时loss是: 49.82265477474254\n",
      "在第1966步，我们获得了函数 f(rm) = 5.570179819900581 * rm + -12.203602579708898, 此时loss是: 49.81966264816079\n",
      "在第1967步，我们获得了函数 f(rm) = 5.571029150900728 * rm + -12.209005274430108, 此时loss是: 49.81667196045291\n",
      "在第1968步，我们获得了函数 f(rm) = 5.571878277660327 * rm + -12.21440666995309, 此时loss是: 49.813682710926976\n",
      "在第1969步，我们获得了函数 f(rm) = 5.572727200228491 * rm + -12.219806766590265, 此时loss是: 49.81069489889138\n",
      "在第1970步，我们获得了函数 f(rm) = 5.573575918654324 * rm + -12.225205564653978, 此时loss是: 49.807708523654846\n",
      "在第1971步，我们获得了函数 f(rm) = 5.574424432986915 * rm + -12.2306030644565, 此时loss是: 49.80472358452646\n",
      "在第1972步，我们获得了函数 f(rm) = 5.575272743275345 * rm + -12.235999266310026, 此时loss是: 49.8017400808156\n",
      "在第1973步，我们获得了函数 f(rm) = 5.57612084956868 * rm + -12.241394170526679, 此时loss是: 49.798758011831985\n",
      "在第1974步，我们获得了函数 f(rm) = 5.576968751915974 * rm + -12.246787777418502, 此时loss是: 49.795777376885695\n",
      "在第1975步，我们获得了函数 f(rm) = 5.577816450366272 * rm + -12.252180087297468, 此时loss是: 49.7927981752871\n",
      "在第1976步，我们获得了函数 f(rm) = 5.578663944968604 * rm + -12.257571100475472, 此时loss是: 49.789820406346934\n",
      "在第1977步，我们获得了函数 f(rm) = 5.579511235771991 * rm + -12.262960817264334, 此时loss是: 49.78684406937625\n",
      "在第1978步，我们获得了函数 f(rm) = 5.580358322825441 * rm + -12.2683492379758, 此时loss是: 49.78386916368642\n",
      "在第1979步，我们获得了函数 f(rm) = 5.581205206177949 * rm + -12.27373636292154, 此时loss是: 49.78089568858918\n",
      "在第1980步，我们获得了函数 f(rm) = 5.582051885878501 * rm + -12.27912219241315, 此时loss是: 49.77792364339657\n",
      "在第1981步，我们获得了函数 f(rm) = 5.582898361976069 * rm + -12.284506726762151, 此时loss是: 49.774953027420956\n",
      "在第1982步，我们获得了函数 f(rm) = 5.583744634519613 * rm + -12.28988996627999, 此时loss是: 49.771983839975064\n",
      "在第1983步，我们获得了函数 f(rm) = 5.584590703558083 * rm + -12.295271911278036, 此时loss是: 49.76901608037192\n",
      "在第1984步，我们获得了函数 f(rm) = 5.585436569140417 * rm + -12.300652562067585, 此时loss是: 49.7660497479249\n",
      "在第1985步，我们获得了函数 f(rm) = 5.586282231315539 * rm + -12.30603191895986, 此时loss是: 49.763084841947716\n",
      "在第1986步，我们获得了函数 f(rm) = 5.587127690132364 * rm + -12.311409982266007, 此时loss是: 49.76012136175437\n",
      "在第1987步，我们获得了函数 f(rm) = 5.587972945639794 * rm + -12.316786752297096, 此时loss是: 49.75715930665923\n",
      "在第1988步，我们获得了函数 f(rm) = 5.588817997886718 * rm + -12.322162229364126, 此时loss是: 49.754198675977\n",
      "在第1989步，我们获得了函数 f(rm) = 5.589662846922015 * rm + -12.327536413778018, 此时loss是: 49.75123946902268\n",
      "在第1990步，我们获得了函数 f(rm) = 5.590507492794553 * rm + -12.332909305849618, 此时loss是: 49.74828168511164\n",
      "在第1991步，我们获得了函数 f(rm) = 5.5913519355531855 * rm + -12.3382809058897, 此时loss是: 49.745325323559534\n",
      "在第1992步，我们获得了函数 f(rm) = 5.592196175246757 * rm + -12.343651214208963, 此时loss是: 49.74237038368239\n",
      "在第1993步，我们获得了函数 f(rm) = 5.593040211924098 * rm + -12.349020231118027, 此时loss是: 49.73941686479653\n",
      "在第1994步，我们获得了函数 f(rm) = 5.593884045634028 * rm + -12.354387956927443, 此时loss是: 49.736464766218624\n",
      "在第1995步，我们获得了函数 f(rm) = 5.594727676425357 * rm + -12.359754391947682, 此时loss是: 49.73351408726565\n",
      "在第1996步，我们获得了函数 f(rm) = 5.595571104346879 * rm + -12.365119536489145, 此时loss是: 49.730564827254945\n",
      "在第1997步，我们获得了函数 f(rm) = 5.596414329447381 * rm + -12.370483390862155, 此时loss是: 49.72761698550417\n",
      "在第1998步，我们获得了函数 f(rm) = 5.5972573517756326 * rm + -12.375845955376963, 此时loss是: 49.72467056133127\n",
      "在第1999步，我们获得了函数 f(rm) = 5.5981001713803975 * rm + -12.381207230343744, 此时loss是: 49.721725554054565\n"
     ]
    }
   ],
   "source": [
    "def partial_k(k, b, x, y):\n",
    "    return 2 * np.mean((k * x + b - y) * x)\n",
    "\n",
    "def partial_b(k, b, x, y):\n",
    "    return 2 * np.mean(k * x + b - y)\n",
    "\n",
    "k, b = random.random(), random.random()\n",
    "min_loss = float('inf')\n",
    "best_k, bes_b = None, None\n",
    "learning_rate = 1e-2\n",
    "\n",
    "for step in range(2000):\n",
    "    k, b = k + (-1 * partial_k(k, b, X_rm, Y) * learning_rate), b + (-1 * partial_b(k, b, X_rm, Y) * learning_rate)\n",
    "    y_hats = k * X_rm + b\n",
    "    current_loss = loss(y_hats, Y)\n",
    "    \n",
    "    if current_loss < min_loss:\n",
    "        min_loss = current_loss\n",
    "        best_k, best_b = k, b\n",
    "        print('在第{}步，我们获得了函数 f(rm) = {} * rm + {}, 此时loss是: {}'.format(step, k, b, current_loss))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 我们可以看到，基本上每一步都在更新"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "49.721725554054565"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "min_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.collections.PathCollection at 0x7f267f68ca90>"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.scatter(X_rm, Y)\n",
    "plt.scatter(X_rm, [best_k * rm + best_b for rm in X_rm])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "def partial_k(k, b, x, y):\n",
    "    return 2 * np.mean((k * x + b - y) * x)\n",
    "\n",
    "def partial_b(k, b, x, y):\n",
    "    return 2 * np.mean(k * x + b - y)\n",
    "\n",
    "k, b = random.random(), random.random()\n",
    "min_loss = float('inf')\n",
    "best_k, bes_b = None, None\n",
    "learning_rate = 1e-2\n",
    "\n",
    "k_b_history = []\n",
    "\n",
    "for step in range(2000):\n",
    "    k, b = k + (-1 * partial_k(k, b, X_rm, Y) * learning_rate), b + (-1 * partial_b(k, b, X_rm, Y) * learning_rate)\n",
    "    y_hats = k * X_rm + b\n",
    "    current_loss = loss(y_hats, Y)\n",
    "    \n",
    "    if current_loss < min_loss:\n",
    "        min_loss = current_loss\n",
    "        best_k, best_b = k, b\n",
    "        k_b_history.append((best_k, best_b))\n",
    "#         print('在第{}步，我们获得了函数 f(rm) = {} * rm + {}, 此时loss是: {}'.format(step, k, b, current_loss))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.scatter(X_rm, Y)\n",
    "test = [0,10,100,500,-1]\n",
    "for i in test:\n",
    "    plt.scatter(X_rm, k_b_history[i][0]*X_rm+k_b_history[i][1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 透视了整个获得k和b的过程"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "def model(x,k,b):\n",
    "    return k*x+b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 17 µs, sys: 2 µs, total: 19 µs\n",
      "Wall time: 25 µs\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "21.182996615629033"
      ]
     },
     "execution_count": 73,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%time\n",
    "model(6,best_k, best_b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 896 µs, sys: 0 ns, total: 896 µs\n",
      "Wall time: 905 µs\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "20.933333333333334"
      ]
     },
     "execution_count": 74,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%time\n",
    "find_price_by_similar(rm_to_price,6)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 用梯度下降得到的算法，速度比KNN有大大的提升"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Review\n",
    "+ 1.correlation,salient feature\n",
    "+ 2.Machine Learing\n",
    "+ 3.KNN\n",
    "+ 4.Random Iteration get best k and b (蒙特卡洛模拟)\n",
    "+ 5.*loss\n",
    "+ 6.Gradient Descent\n",
    "+ 7.Performance of Gradient Descent\n",
    "\n",
    "## Next\n",
    "+ 1.More complicated function\n",
    "+ 2.Activation Function\n",
    "+ 3.Neural Network\n",
    "+ 4.Deep Learning\n",
    "+ 5.Back propogation\n",
    "+ 6.Auto-Back propogation\n",
    "+ 7.Topological Sorting\n",
    "+ 8.Auto-Compute Gradient\n",
    "+ 9.All the elements of a neural Network framework"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
